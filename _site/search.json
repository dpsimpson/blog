[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "A footnote that got out of control."
  },
  {
    "objectID": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html",
    "href": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html",
    "title": "MCMC with the wrong acceptance probability",
    "section": "",
    "text": "Just the other day1 I was chatting with a friend2 about MCMC and he asked me a fundamental, but seldom asked, question: What happens my acceptance probability is a bit off?.\nThis question comes up a bunch. In this context, they were switching from double to single precision3 and were a little worried that some of their operations would be a bit more inexact than they were used to. Would this tank MCMC? Would everything still be fine?"
  },
  {
    "objectID": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html#what-is-markov-chain-monte-carlo",
    "href": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html#what-is-markov-chain-monte-carlo",
    "title": "MCMC with the wrong acceptance probability",
    "section": "What is Markov chain Monte Carlo",
    "text": "What is Markov chain Monte Carlo\nMarkov chain Monte Carlo (MCMC) is, usually, guess-and-check for people who want to be fancy.\nIt is a class of algorithms that allow you to construct a4 Markov chain that has a given stationary distribution5 \\(\\pi\\). In Bayesian applications, we usually want to choose \\(\\pi = p(\\theta \\mid y)\\), but there are other applications of MCMC.\nMost6 MCMC algorithms live in the Metropolis-Hastings family of algorithms. These methods require only one component: a proposal distribution \\(q(\\theta' \\mid \\theta)\\). Given basically any7 proposal distribution, we can go from our current state \\(\\theta_k\\) to the new state \\(\\theta_{k+1}\\) using the following three steps:\n\nPropose a potential new state \\(\\theta' \\sim q(\\theta' \\mid \\theta_k)\\)\nSample a Bernoulli random variable \\(r_{k+1}\\) with \\[\n\\Pr(r_{k+1} = 1 \\mid \\theta_k) = \\alpha_{k+1} =  \\min\\left\\{1, \\frac{\\pi(\\theta')}{\\pi(\\theta_k)}\\frac{q(\\theta_k \\mid \\theta')}{q(\\theta' \\mid \\theta_k)}\\right\\}\n\\]\nSet \\(\\theta_{k+1}\\) according to the formula \\[\n\\theta_{k+1} = \\begin{cases} \\theta', & r_{k+1}=1 \\\\ \\theta_k, &r_{k+1} = 0.\\end{cases}\n\\]\n\nThe acceptance probability8 is chosen9 to balance10 out the proposal \\(q(\\cdot \\mid \\cdot)\\) with the target distribution \\(\\pi\\).\nYou can interpret the two ratios in the acceptance probability separately. The first one prefers proposals from high-density regions over proposals from low-density regions. The second ratio balances this by down-weighting proposed states that were easy to propose from the current location. When the proposal is symmetric, ie \\(q(\\theta'\\mid \\theta)= q(\\theta \\mid \\theta')\\), the second ratio is always 1. However, in better algorithms like MALA11, the proposal is not symmetric. If we look at the MALA proposal \\[\nq(\\theta'\\mid \\theta) \\sim N\\left(\\theta + \\frac{1}{2}\\Sigma\\nabla \\log \\pi(\\theta), \\Sigma\\right)\n\\] it’s pretty easy to see that we are biasing our samples towards the mode of the distribution. If we did not have the second ratio in the acceptance probability we would severely under-sample the tails of the distribution."
  },
  {
    "objectID": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html#mcmc-with-approximate-acceptance-probabilities",
    "href": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html#mcmc-with-approximate-acceptance-probabilities",
    "title": "MCMC with the wrong acceptance probability",
    "section": "MCMC with approximate acceptance probabilities",
    "text": "MCMC with approximate acceptance probabilities\nWith this definition in hand, it’s now possible to re-cast the question my friend asked as &gt; What happens to my MCMC algorithm if, instead of \\(\\alpha_{k+1}\\) I accidentally compute \\(\\tilde \\alpha_{k+1}\\) and use that instead to simulate \\(r_{k+1}\\)?\nSo let’s go about answering that!"
  },
  {
    "objectID": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html#a-bit-of-a-literature-review",
    "href": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html#a-bit-of-a-literature-review",
    "title": "MCMC with the wrong acceptance probability",
    "section": "A bit of a literature review",
    "text": "A bit of a literature review\nUnsurprisingly, this type of question has popped up over and over again in the literature:\n\nThis exact question was asked by Gareth Roberts and Jeff Rosenthal first12 with Peter Schwartz and a second, more13 14 realistic, time with Laird Breyer. They found that as long as the chain’s convergence is sufficiently nice15 then the perturbed chain will converge nicely and have16 a central limit theorem.\nAbout 10 years ago, an absolute orgy17 18 of research happened around the question What happens if the acceptance probability is random but unbiased?. These exact approximate19 or pseudo-marginal methods. These have some success in situations20 where the likelihood has a parameter dependent normalising constant that can’t be computed exactly, but can be estimated unbiasedly. The problem with this class of methods is that the extra noise tends to make the Markov chain perform pretty badly21. This limits its practical use to models where we really can’t do anything else22. That said, there is some interesting literature on random sub-sampling of data where it doesn’t really work and where it does work.\nA third branch of literature is on truly approximate algorithms. These try to understand what happens if you’re just wrong with \\(\\alpha_{k+1}\\) and you don’t do anything to correct it. There are a lot of papers on this, and I’m not going to do anything approaching a thorough review. I have work23 24 to do. So I will just list two older papers that were influential for me. The first was by Geoff Nichols, Colin Fox, and Alexis Muir Watt, which looks at what happens when you don’t correct your pseudo-marginal method correctly. It’s a really neat theory paper that is a great presentation25 of the concepts. The second paper is by Pierre Alquier, Nial Friel, Richard Everitt, and Aidan Boland, which looks at general approximate Markov chains. They show empirically that these methods work extremely well relative to pseudo-marginal methods for practical settings. There are also some nice results on perturbations of Markov chains in general, for instance this paper by Daniel Rudolf and Nikolaus Schweizer.\n\n\nTrying to understand noisy Markov chains\nSo how do I think of noisy Markov chains. Despite all appearances26 I am not really a theory person. So while I know that there’s a massive literature on the stability of Markov chains, it doesn’t really influence how I think about it.\nInstead, I think about it in terms of that Nicholls, Fox, and Muir Watt paper paper. Or, specifically, a talk I saw Colin give at some point that was really clear.\nThe important thing to recognise is that it is not important how well you compute \\(\\alpha_{k+1}\\). What is important is if you get the same outcome. Imagine we have two random variables \\(r_{k+1} \\sim \\text{Bernoulli}(\\alpha_{k+1})\\) and \\(\\tilde r_{k+1} \\sim \\text{Bernoulli}(\\tilde \\alpha_{k+1})\\). If our realisation of \\(r_{k+1}\\) is the same as our realisation of \\(\\tilde r_{k+1}\\), then we get the same \\(x_{k+1}\\). Or, to put it another way, when \\(r_{k+1} = \\tilde r_{k+1}\\), no one can tell27 that it’s an approximate Markov chain.\nThis means that one way to understand inexact MCMC is to think of the Markov chain \\[\n(\\tilde{\\theta}_k, s_k), \\qquad k=0, 1, \\ldots, \\infty,\n\\] where28 \\[\ns_k = \\begin{cases} 0, \\quad & r_{k} = \\tilde r_k \\\\\n1, &r_k \\neq \\tilde r_k\\end{cases}\n\\] indicates whether or not we made the wrong decision. It’s important to note that while \\(\\tilde \\theta_k\\) is marginally a Markov chain, \\(s_k\\) is not. You can actually think of \\(s_k\\) as the observation of a hidden Markov model if you want to. I won’t stop you. Nothing will. There is no morality, there is no law. It is The Purge.\nAlthough we can never actually observe \\(s_k\\), thinking about it is really useful. In particular, we note that until \\(s_k =1\\) for the first time, the samples of \\(\\tilde \\theta_k\\) are identical to a correct Metropolis-Hastings algorithm. After this point, the approximate chain and the (imaginary) exact chain will be different. But we can iterate this argument.\nTo do this, we can define the length \\(N_j\\) of the Markov chain that would be the same as the exact MCMC algorithm started at \\(\\theta_{N_{k-1}}\\) by \\(N_0=0\\) and \\[\nN_k = \\inf_{i &gt; N_k}\\{i - N_{k-1}: s_i = 1\\}.\n\\]\nIf we run our algorithm for \\(N\\) steps, we can then think of the output as being the same as running \\(J = \\sum_{k=1}^N s_k\\) Markov chains of different lengths. The \\(j\\)th chain starts at \\(\\theta_{N_{j-1}}\\) and is length \\(N_{j}-1\\). It is worth remembering that these chains are not started from independent points. In particular, if \\(N_j\\) is small, then the starting position of the \\(j\\)th and the \\(j+1\\)th chain will be heavily correlated.\nTo think about this we need to think about what happens after \\(N_k\\) steps of a Markov chain. We are going to need the notation \\(\\theta_k = P^k \\theta_0\\) denotes \\(k\\) steps of the exact algorithm.\nThe topic of convergence of Markov chains is a complex business, but we are going to assume that our exact Markov chain is29 geometrically ergodic, which means that \\[\n\\|P^k \\theta_0 - \\pi\\| \\leq M(\\theta_0) \\rho^{k}\n\\] for some function30 \\(M(x_0)\\) and \\(0 &lt; \\rho &lt; 1\\).\nGeometric ergodicity is a great condition because, among other things, it ensures that sample means from the Markov chain satisfy a central limit theorem. It’s also bloody impossible to prove. But usually indicators like R-hat do a decent job at suggesting that there might be problems. Also if you are spending a lot of time rejecting proposals in certain parts of the space, there’s a solid chance that you’re not geometrically ergodic.\nNow let’s assume that we are interested in computing \\(\\mathbb{E}_\\pi(h(\\theta))\\) for some nice31 function \\(h\\). Then the nice thing about Markov chains is that, give or take32 \\[\n\\left|\\frac{1}{N_j-1}\\sum_{k=N_{j-1}}^{N_j-1}h(\\theta_k) - \\mathbb{E}_\\pi(h(\\theta))\\right| \\leq C \\frac{M(\\theta_{N_{j-1}})}{N_j-1}\\frac{1 - \\rho^{N_{j}-1}}{1- \\rho}.\n\\] where \\(C\\) might depend on \\(h\\) if \\(h\\) is unbounded.\nThis suggests that the error is bounded by, roughly, \\[\n\\left|\\frac{1}{N}\\sum_{k=1}^{N}h(\\theta_k) - \\mathbb{E}_\\pi(h(\\theta))\\right| \\leq \\frac{C}{N} \\sum_{j = 1}^J M(\\theta_{N_{j-1}})\\frac{1 - \\rho^{N_{j}-1}}{1- \\rho}.\n\\]\nThis suggests a few things:\n\nIf \\(J\\) is small relative to \\(N\\), we are going to get very similar estimates to just running \\(J\\) parallel Markov chains and combining them without removing any warm up iterations. In particular, if almost all \\(N_j\\) are big, it will be a lot like combining \\(J\\) warmed up independent chains.\nEffective sample size and Monte Carlo standard error estimates will potentially be very wrong. This is because instead of computing them based on multiple dependent chains, we are pretending that all of our samples came from a single ergodic Markov chain. Is this a problem? I really don’t know. Again, if the \\(N_j\\)s are usually large, we will be fine.\nBecause \\(M(\\theta)\\) can be pretty large when \\(\\theta\\) is large, we might have some problems. It’s easy to imagine cases where we get stuck out in a tail and we just fire off a lot of events when \\(\\theta_{N_j}\\) is really big. This will be a problem. But also, if we are stuck out in a tail, we are rightly fucked anyway and all of the MCMC diagnostics should be screaming at you. We can take heart that \\(\\mathbb{E}_\\pi(M(\\theta))\\) is usually finite33 and not, you know, massive.\n\n\n\nWhat do the \\(N_j\\) look like?\nSo the take away from the last section was that if the random variables \\(N_j\\) are usually pretty big, then everything will work ok. Intuitively this makes sense. If the \\(N_j\\)s were always small, it would be very difficult to ever get close to any sort of stationary distribution.\nThe paper by Nicholls, Fox, and Muir Watt paper talks about potential sizes for \\(N_j\\). The general construction that they use is a coupling, which is a bivariate Markov chain \\((\\theta_k, \\tilde \\theta_k)\\) that start from the same position and are updated as follows:\n\nPropose \\(\\theta' \\sim q(\\theta' \\mid \\tilde \\theta_{k})\\)\nGenerate a uniform random number \\(u_{k+1}\\)\nUpdate \\(\\theta\\) as \\[\n\\theta_{k+1} = \\begin{cases} \\theta', \\qquad & u_{k+1} \\leq \\alpha_{k+1} \\\\\n\\theta_{k}, & u_{k+1} &gt; \\alpha_{k+1}.\\end{cases}\n\\]\nUpdate \\(\\tilde \\theta\\) as \\[\n\\tilde \\theta_{k+1} = \\begin{cases} \\theta', \\qquad & u_{k+1} \\leq \\tilde \\alpha_{k+1} \\\\\n\\tilde \\theta_{k}, & u_{k+1} &gt; \\tilde \\alpha_{k+1}.\\end{cases}\n\\]\n\nThis Markov chain is coupled in three ways ways. The chain starts at the same values \\(\\theta_0 = \\tilde \\theta_0\\), the proposed \\(\\theta'\\) is the same for both chains, and the randomness34 used to do the accept/reject step is the same. Together, this things mean that \\(\\theta_k = \\tilde \\theta_k\\) for all \\(k &lt; N_1\\).\nFor this coupling construction, we can get the exact distribution of the \\(s_k\\). To do this, we remember that we will only make different decisions in the two chains (or uncouple) if \\(u\\) is on different sides of the two acceptance probabilities. The probability of happening is \\[\\begin{align*}\n\\Pr(s_k = 1) &= \\Pr( u \\in [\\min\\{ \\alpha_{k}, \\tilde \\alpha_k\\}, \\max\\{ \\alpha_{k}, \\tilde \\alpha_k\\}]) \\\\\n&= |\\alpha_k - \\tilde \\alpha_k|.\n\\end{align*}\\]\nI guess you could write down the distribution of the \\(N_j\\) in terms of this. In particular, you get \\[\n\\Pr(N_1 = n) = |\\alpha_n - \\tilde \\alpha_n|\\prod_{k=1}^{n-1} (1- |\\alpha_k - \\tilde \\alpha_k|)\n\\], but honestly it would be an absolute nightmare.\nWhen people get stuck in probability questions, the natural thing to do is to make the problem so abstract that you can make the answer up. In that spirit, let’s ask a slightly different: what is the distribution of the maximal decoupling time between the exact and the approximate chain. This is the distribution of the longest possible coupling of the two chains over all35 possible random sequences \\((\\theta_k, \\tilde \\theta_k)\\) such that the distribution of \\((\\theta_1, \\theta_2, \\ldots)\\) is the same as our exact Markov chain and the distribution of \\((\\tilde\\theta_1,\\tilde \\theta_2, \\ldots)\\) is the same as our approximate Markov chain.\nThis maximal value of \\(N_1\\) is called the maximal agreement coupling time or, more whimsically, the MEXIT time. It turns out that getting the distribution of \\(N_1\\) is … difficult, but we36 can construct a random variable \\(\\tau\\) that is independent of \\(\\tilde \\theta_k\\) such that \\(\\tau \\leq N_1\\) almost surely and \\[\n\\Pr(\\tau = t\\mid \\tau \\geq t) = 1 - \\operatorname*{ess\\,inf}_{B, \\theta_{&lt;t}} \\left\\{\\frac{P(\\theta_t \\in B \\mid \\theta_{&lt;t})}{\\tilde P(\\theta_t \\in B \\mid \\theta_{&lt;t})}\\right\\},\n\\] where \\(P(\\theta_t \\mid \\theta_{&lt;t})\\) is the transition distribution for the exact Markov37 chain and \\(\\tilde P(\\theta_t \\mid \\theta_{&lt;t})\\) is the transition distribution for the approximate Markov chain.\nFor a Metropolis-Hastings algorithm, the transition distribution has the form \\[\nP(B, \\theta)= \\begin{cases} \\alpha(\\theta)Q(B \\mid \\theta),\\qquad & \\theta \\not \\in B \\\\\n\\alpha(\\theta)Q(B\\mid \\theta) + (1-\\alpha(\\theta)), &\\theta \\in B\n\\end{cases}\n\\] where \\(Q(B\\mid \\theta)\\) is the probability associated with the proposal density \\(q(\\cdot \\mid \\theta)\\) and I have been very explicit about the dependence of the acceptance probability on \\(\\theta\\). (The \\((1-\\alpha(\\theta))\\) term takes into account the probability of starting at \\(\\theta\\) and not accepting the proposed state.)\nThat definition of \\(\\tau\\) looks pretty nasty, but it’s not too bad: in particular, the infinitum only cares of \\(\\theta_{t-1}\\in B\\). This means that the condition simplifies to \\[\n\\Pr(\\tau = t\\mid \\tau \\geq t) = 1 - \\min\\left\\{\\operatorname*{ess\\,inf}_{B, \\theta_{t-1}} \\frac{\\alpha_t(\\theta_{t-1}) Q(B \\mid \\theta_{t-1})}{\\tilde\\alpha_t(\\theta_{t-1}) Q(B \\mid \\theta_{t-1})}, \\operatorname*{ess\\,inf}_{B, \\theta_{t-1}} \\frac{\\alpha_t(\\theta_{t-1}) Q(B \\mid \\theta_{t-1}) + (1-\\alpha_t(\\theta_{t-1}))}{\\tilde\\alpha_t(\\theta_{t-1}) Q(B \\mid \\theta_{t-1}) + (1- \\tilde \\alpha_t(\\theta_{t-1}))}\\right\\}.\n\\]\nThis simplifies further if we assume that the proposal distribution \\(Q(\\cdot \\mid \\theta_k)\\) is absolutely continuous and has a strictly positive density. Then, it truly does not matter what \\(B\\) is. For the first term, it just cancels, while the second term is monotone38 in \\(Q(B \\mid \\theta_{t-1})\\), so we can take this term to be either zero or one and get39 \\[\n\\Pr(\\tau = t\\mid \\tau \\geq t) = 1 - \\min\\left\\{\\operatorname*{ess\\,inf}_{ \\theta_{t-1}} \\frac{\\alpha_t(\\theta_{t-1}) }{\\tilde\\alpha_t(\\theta_{t-1})}, \\operatorname*{ess\\,inf}_{\\theta_{t-1}} \\frac{1-\\alpha_t(\\theta_{t-1})}{ 1- \\tilde \\alpha_t(\\theta_{t-1})},1\\right\\}.\n\\]\nThis is, as the Greeks would say, not too bad.\nIf, for instance, we know the relative error \\[\n\\tilde\\alpha(\\theta) = (1 + \\delta(\\theta))\\alpha(\\theta),\n\\] then \\[\n\\frac{\\alpha(\\theta)}{\\tilde \\alpha(\\theta)} = \\frac{1}{1 + \\delta(\\theta)},\n\\] and if we know40 \\(\\delta(\\theta) \\leq \\bar \\delta\\), we get \\[\n\\frac{\\alpha(\\theta)}{\\tilde \\alpha(\\theta)} \\geq \\frac{1}{1 + \\bar\\delta}.\n\\] Similarly, if \\[\n1-\\tilde \\alpha(\\theta) = (1-\\alpha(\\theta))(1+\\epsilon(\\theta)),\n\\] and \\(\\epsilon(\\theta) \\leq \\bar \\epsilon\\), then we get \\[\n\\frac{1-\\alpha(\\theta)}{1-\\tilde \\alpha(\\theta)} = \\frac{1}{1+\\epsilon(x)} \\geq \\frac{1}{1+\\bar\\epsilon}.\n\\]\nThe nice thing is that we can choose our upper bounds so that \\(\\rho = (1+ \\bar \\delta)^{-1} = (1+ \\bar\\epsilon)^{-1}\\) and get the upper bound \\[\n\\Pr(\\tau = t\\mid \\tau \\geq t) \\leq 1 - \\rho.\n\\] It follows that \\[\n\\Pr(\\tau = t) \\leq \\rho^{t-1}(1-\\rho).\n\\]\nNow this is a bit nasty. It’s an upper bound on the probability of a lower bound on the maximal decoupling time. Probability, eh.\nProbably the most useful thing we can get from this is an upper bound on \\(\\mathbb{E}(\\tau)\\), which is41 \\[\n\\mathbb{E}(\\tau) \\leq \\frac{1}{1-\\rho} = 1 + \\bar \\delta^{-1}.\n\\]\nThis confirms our intuition that if the relative error is large, we will have, on average, quite small \\(N_j\\). It’s not quite enough to show the opposite (good floating point error begets big \\(N_j\\)), but that’s probably true as well.\nAnd that is where we end this saga. There is definitely more that could be said, but I decided to spend exactly one day writing this post and that time is now over."
  },
  {
    "objectID": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html#footnotes",
    "href": "posts/2022-11-23-wrong-mcmc/wrong-mcmc.html#footnotes",
    "title": "MCMC with the wrong acceptance probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsually this is a lie, but it was actually a thing that happened last week↩︎\nDon’t judge me (or my friends) based on this. I promise we also talk about other shit.↩︎\nHi GPUs!↩︎\nusually reversible, although a lot of cool but not ready for prime time work is being done on non-reversible chains.↩︎\nA stationary distribution, if it exists, is the distribution that is preserved by the Markov chain. If \\(\\pi\\) is the stationary distribution and \\(x_1 \\sim \\pi\\), then if we construct \\(x_2, x_3,\\ldots\\) by running the Markov chain then for every \\(k\\), the marginal distribution is \\(x_k \\sim \\pi\\).↩︎\nBut critically not all! The dynamic HMC algorithm used in Stan, for instance, is not a Metropolis-Hastings algorithm. Instead of doing an accept/reject step it samples from the proposed trajectory. Betancourt’s long intro to Hamiltonian Monte Carlo covers this very well.↩︎\nThe conditions for this to work are very light. But that’s because the definition of “working” only thinks about what happens after infinitely many steps. To get a practically useful Metropolis-Hastings algorithm, you’ve got to work very hard on choosing your proposal density.↩︎\nsometimes called the Hastings correction↩︎\nThis is not the only choice that will work, but in some sense it is the most efficient one.↩︎\nTechnically, it is chosen by requiring that the Markov proposal \\(P(\\theta,\\theta')\\) satisfies the detailed balance condition \\(\\pi P(\\theta,\\theta') = P(\\theta', \\theta)\\pi\\), but everything about that equation is beyond the scope of this particular post.↩︎\nMetropolis-adjusted Langevin Algorithm↩︎\nUnder the assumption that the total floating point error was bounded by a constant \\(\\delta\\)↩︎\nThis time the assumption was that the rounding error for the acceptance probability at state \\(\\theta_k\\) was bounded by \\(\\delta \\|\\theta_k\\|\\). This is a lot closer to how floating point arithmetic actually works. The trade off is that it requires a tighter condition on the drift function \\(V\\).↩︎\nIEEE floating point arithmetic represents a real number using \\(B\\) bits. Typically \\(B = 64\\) (double precision) or \\(B = 32\\) (single precision). You can read a great intro to this on Nick Higham’s blog. But in general, the best we can represent a real number \\(\\theta\\) by is by a floating point number \\(\\tilde \\theta\\) that satisfies \\[\n|\\theta - \\tilde \\theta| \\leq 2^{-N+1}|\\theta|,\n\\] where \\(N=23\\) in single precision and \\(N=32\\) in double precision. Of course, the acceptance probability is a non-linear combination of floating point numbers, so the actual error is going to be more complicated than that. I strongly recommend you read Nick Higham’s book on the subject.↩︎\n\\(V\\)-geometrically ergodic with some light conditions on \\(V\\)↩︎\nGeometric ergodicity implies the existence of a CLT! Which is nice, because all of our intuition about how to use the output from MCMC depends on a CLT.↩︎\nLike all good orgies, this one was mostly populated by men↩︎\nYes, I know. My (limited) contribution this literature was some small contributions to a paper lead by Anne-Marie Lyne. But if years of compulsory catholicism taught me anything (other than “If you’re drinking with a nun or an aging homosexual, don’t try to keep up”) it’s that something does not have to be literally true to be morally true.↩︎\nWe have to slightly redefine the word “exact” to mean “targets the correct stationary distribution” for this name to make sense↩︎\nRandom graph models and point processes are two great examples↩︎\nfor instance, it gets stuck for long times at single values↩︎\nthe aforementioned point process and graph models↩︎\nPlaying God of War: Ragnarok↩︎\nThe first run of God of War Games were not my cup of tea, but the 2008 game, which is essentially a detailed simulation of what happens when a muscle bear is entrusted with walking an 11 year old up a hill, was really enjoyable. So far this is too.↩︎\nDoes it talk about involutions for not fucking reason? Of course it does. Read past that.↩︎\nYeah, like I have also read my blog. Think of it as being like social media. It is not a representation of me a whole person. It’s actually biased towards stuff that I have either found or find difficult.↩︎\nA friend of mine has a “No one knows I’m a transexual” t-shirt that she likes to wear to supermarkets.↩︎\nNote that both \\(r_k\\) and \\(\\tilde r_k\\) are computed using the same value \\(\\tilde \\theta_{k-1}\\).↩︎\nThe norm here is usually either the total variation norm of the \\(V\\)-norm. But truly it’s not important for the hand waving.↩︎\nIn most cases \\(M(\\theta) \\rightarrow \\infty\\) as \\(\\|\\theta\\| \\rightarrow \\infty\\).↩︎\nBounded and continuous always works. But everything is probably ok for unbounded functions as long as \\(h(\\theta)\\) has a pile of finite moments.↩︎\nThis is roughly true. I basically used the geometric ergodicity bound to bound \\[\n\\sum_{k=N_{j-1}}^{N_j-1} \\left(\\theta_k - \\frac{1}{N_j-1}\\mathbb{E}_\\pi(h(\\theta)\\right)\n\\] and summed it up. There are smarter things to do, but it’s close enough for government work. ↩︎\nSometimes, if you squint, this term will kinda, sorta start to look like \\(\\mathbb{E}_\\pi(\\pi(\\theta)^{-1/2})\\), which isn’t usually toooo big. But also, sometimes it looks totally different. Theory is wild.↩︎\nIf you’ve ever wondered how rbinom(1,p) works, there you are.↩︎\nThink of this as the opposite of an adversarial example. We are trying to find the exact chain that is scared to leave the approximate chain behind. Which is either romantic or creepy, depending on finer details.↩︎\nWell not me. Florian Völlering did it in his Theorem 1.4. I most certainly could not have done it.↩︎\nWell the result does not need this to be a Markov chain!↩︎\nit goes up if \\(\\alpha&gt;\\tilde \\alpha\\) otherwise it goes down↩︎\nThe 1 case can basically never happen except in the trivial case where both acceptance probabilities are the same. And if we thought that was going to happen we would’ve done something bloody else↩︎\nThe the relative error being bounded does not stop the absolute error growing!↩︎\nLook above and recognize the Geometric distribution↩︎"
  },
  {
    "objectID": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html",
    "href": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html",
    "title": "Sparse Matrices 1: The linear algebra of linear mixed effects models and their generalisations",
    "section": "",
    "text": "Back in the early days of the pandemic I though “I’ll have a pandemic project”. I never did my pandemic project.\nBut I did think briefly about what it would be. I want to get the types of models I like to use in everyday life efficiently implemented inside Stan. These models encapsulate (generalised) linear mixed models1, (generalised) additive models, Markovian spatial models2, and other models. A good description of the types of models I’m talking about can be found here.\nMany of these models can be solved efficiently via INLA3, a great R package for fast posterior inference for an extremely useful set of Bayesian models. In focussing on a particular class of Bayesian models, INLA leverages a bunch of structural features to make a very very fast and accurate posterior approximation. I love this stuff. It’s where I started my stats career.\nNone of the popular MCMC packages really implement the lessons learnt from INLA to help speed up their inference. I want to change that.\nThe closest we’ve gotten so far is the nice work Charles Margossian has been doing to get Laplace approximations into Stan.\nBut I want to focus on the other key tool in INLA: using sparse linear algebra to make things fast and scalable.\nI usually work with Stan, but the scale of the C++ coding4 required to even tell if these ideas are useful in Stan was honestly just too intimidating.\nBut the other day I remembered Python. Now I am a shit Python programmer5 and I’m not fully convinced I ever achieved object permanence. So it took me a while to remember it existed. But eventually I realised that I could probably make a decent prototype6 of this idea using some modern Python tools (specifically JAX). I checked with some PyMC devs and they pointed me at what the appropriate bindings would look like.\nSo I decided to go for it.\nOf course, I’m pretty busy and these sort of projects have a way of dying in the arse. So I’m motivating myself by blogging it. I do not know if these ideas will work7. I do not know if my coding skills are up to it8. I do not know if I will lose interest. But it should be fun to find out.\nSo today I’m going to do the easiest part: I’m going to scope out the project. Read on, MacDuff."
  },
  {
    "objectID": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#a-generalised-linear-mixed-effects-ish-model",
    "href": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#a-generalised-linear-mixed-effects-ish-model",
    "title": "Sparse Matrices 1: The linear algebra of linear mixed effects models and their generalisations",
    "section": "A generalised linear mixed effects-ish model",
    "text": "A generalised linear mixed effects-ish model\nIf you were to open the correct textbook, or the Bates, Mächler, Boler, and Walker 2015 masterpiece paper that describes the workings of lme4, you will see the linear mixed model written as \\[\ny = X\\beta + Zb + \\epsilon,\n\\] where\n\nthe columns of \\(X\\) contain the covariates9,\n\\(\\beta\\) is a vector of unknown regression coefficients,\n\\(Z\\) is a known matrix that describes the random effects (basically which observation is linked to which random effect),\n\\(b \\sim N(0, \\Sigma_b)\\) is the vector of random effects with some unknown covariance matrix \\(\\Sigma_b\\),\nand \\(\\epsilon \\sim N(0 ,\\sigma^2 W)\\) is the observation noise (here \\(W\\) is a known diagonal matrix10).\n\nBut unlike Doug Bates and his friends, my aim is to do Bayesian computation. In this situation, \\(\\beta\\) also has a prior on it! In fact, I’m going to put a Gaussian prior \\(\\beta \\sim N(0, R)\\) on it, for some typically known11 matrix \\(R\\).\nThis means that I can treat \\(\\beta\\) and \\(b\\) the same12 way! And I’m going to do just that. I’m going to put them together into a vector \\(u = (\\beta^T, b^T)^T\\). Because the prior on \\(u\\) is Gaussian13, I’m sometimes going to call \\(u\\) the Gaussian component or even the latent14 Gaussian component.\nNow that I’ve smooshed my fixed and random effects together, I don’t really need to keep \\(X\\) and \\(Z\\) separate. So I’m going push them together into a rectangular matrix \\[\nA = [X \\vdots Z].\n\\]\nThis allows us to re-write the model as \\[\\begin{align*}\ny \\mid u, \\sigma & \\sim N(A u, \\sigma^2 W)\\\\\nu \\mid \\theta &\\sim N(0, Q(\\theta)^{-1}).\n\\end{align*}\\]\nWhat the hell is \\(Q(\\theta)\\) and why are we suddenly parameterising a multivariate normal distribution by the inverse of its covariance matrix (which, if you’re curious, is known as a precision matrix)???\nI will take your questions in reverse order.\nWe are parameterising by the precision15 matrix because it will simplify our formulas and lead to faster computations. This will be a major topic for us later!\nAs to what \\(Q(\\theta)\\) is, it is the matrix \\[\nQ(\\theta) = \\begin{pmatrix} \\Sigma_b^{-1} & 0 \\\\ 0 & R^{-1}\\end{pmatrix}\n\\] and \\(\\theta = (\\sigma, \\Sigma_b)\\) is the collection of all16 non-Gaussian parameters in the model. Later, we will assume17 that \\(\\Sigma_b\\) has quite a lot of structure.\nThis is a very generic model. It happily contains things like\n\nLinear regression!\nLinear regression with horseshoe priors!\nLinear mixed effects models!\nLinear regression with splines (smoothing or basis)!\nSpatial models like ICARs, BYMs, etc etc etc\nGaussian processes (with the caveat that we’re mostly focussing on those that can be formulated via precision matrices rather than covariance matrices. A whole blog post, I have.)\nAny combination of these things!\n\nSo if I manage to get this implemented efficiently, all of these models will become efficient too. All it will cost is a truly shithouse18 interface.\nThe only downside of this degree of flexibility compared to just implementing a straight linear mixed model with \\(X\\) and \\(Z\\) and \\(\\beta\\) and \\(b\\) all living separately is that there are a couple of tricks19 to improve numerical stability that we can’t use."
  },
  {
    "objectID": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#lets-get-the-posterior",
    "href": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#lets-get-the-posterior",
    "title": "Sparse Matrices 1: The linear algebra of linear mixed effects models and their generalisations",
    "section": "Let’s get the posterior!",
    "text": "Let’s get the posterior!\nThe nice thing about thing about this model is that it is a normal likelihood with a normal prior, so we can directly compute two key quantities:\n\nThe “full conditional” distribution \\(p(u \\mid y, \\theta)\\), which is useful for getting posterior information about \\(b\\) and \\(\\beta\\), and\nThe marginal posterior \\(p(\\theta \\mid y)\\).\n\nThis means that we do not need to do MCMC on the joint space \\((u, \\theta)\\)! We can instead write a model to draw samples from \\(p(\\theta \\mid y)\\), which is much lower-dimensional and easier20 to sample from, and then compute the joint posterior by sampling from the full conditional.\nI talked a little about the mechanics of this in a previous blog post about conjugate priors, but let’s do the derivations. Why? Because they’re not too hard and it’s useful to have them written out somewhere.\n\nThe full conditional\nFirst we need to compute \\(p(u \\mid y , \\theta)\\). The first thing that we note is that conditional distributions are always proportional to the joint distribution (we’re literally just pretending some things are constant), so we get \\[\\begin{align*}\np(u \\mid y , \\theta) &\\propto p(y \\mid u, \\theta) p(u \\mid \\theta) p(\\theta) \\\\\n&\\propto \\exp\\left[-\\frac{1}{2\\sigma^2} (y - Au)^TW^{-1}(y-Au)\\right]\\exp\\left[-\\frac{1}{2}u^TQ(\\theta)u\\right].\n\\end{align*}\\]\nNow we just need to expand things out and work out what the mean and the precision matrix of \\(p(u \\mid y, \\theta )\\) (which is Gaussian by conjugacy!) are.\nComputing posterior distributions by hand is a dying21 art. So my best and only advice to you: don’t be a hero. Just pattern match like the rest of us. To do this, we need to know what the density of a multivarite normal distribution looks like deep down in its soul.\nBehold: the ugly div box!22\n\nIf \\(u \\sim N(m, P^{-1})\\), then \\[\\begin{align*}\np(u) &\\propto \\exp\\left[- \\frac{1}{2}(u - m)^TP(u-m)\\right] \\\\\n&\\propto \\exp\\left[- \\frac{1}{2}u^TPu + m^TPu\\right],\n\\end{align*}\\] where I just dropped all of the terms that didn’t involve \\(u\\).\n\nThis means the plan is to\n\nExpand out the quadratics in the exponential term so we get something that looks like \\(\\exp\\left[-\\frac{1}{2}u^TPu + z^Tu\\right]\\)\nThe matrix \\(P\\) will be the precision matrix of \\(u \\mid y, \\theta\\).\nThe mean of \\(\\mu \\mid y, \\theta\\) is \\(P^{-1}z\\).\n\nSo let’s do it!\n\\[\\begin{align*}\np(u \\mid y , \\theta) &\\propto \\exp\\left[-\\frac{1}{2\\sigma^2} u^TA^TW^{-1}Au + \\frac{1}{\\sigma^2}(A^TW^{-1}y)^Tu\\right]\\exp\\left[-\\frac{1}{2}u^TQ(\\theta)u\\right] \\\\\n&\\propto \\exp\\left[-\\frac{1}{2}u^T\\left(Q + \\frac{1}{\\sigma^2}A^TW^{-1}A\\right)u +  \\frac{1}{\\sigma^2}(A^TW^{-1}y)^Tu\\right].\n\\end{align*}\\]\nThis means that \\(p(u \\mid y ,\\theta)\\) is multivariate normal with\n\nprecision matrix \\(Q_{u\\mid y,\\theta}(\\theta) = \\left(Q(\\theta) + \\frac{1}{\\sigma^2}A^TW^{-1}A\\right)\\) and\nmean23 \\(\\mu_{u\\mid y,\\theta}(\\theta) = \\frac{1}{\\sigma^2} Q_{u\\mid y,\\theta}(\\theta)^{-1} A^TW^{-1}y\\).\n\nThis means if I build an MCMC scheme to give me \\(B\\) samples \\(\\theta_b \\sim p(\\theta \\mid y)\\), \\(b = 1, \\ldots, B\\), then I can turn them into \\(B\\) samples \\((\\theta_b, u_b)\\) from \\(p(\\theta, u \\mid y)\\) by doing the following.\n\nFor \\(b = 1, \\ldots, B\\)\n\nSimulate \\(u_b \\sim N\\left(\\mu_{u\\mid y,\\theta}(\\theta_b), Q_{u\\mid y,\\theta}(\\theta_b)^{-1}\\right)\\)\nStore the pair \\((\\theta_b, u_b)\\)\n\n\nEasy24 as!\n\n\nWriting down \\(p(\\theta \\mid y)\\)\nSo now we just25 have to get the marginal posterior for the non-Gaussian parameters \\(\\theta\\). We only need it up to a constant of proportionality, so we can express the joint probability \\(p(y, u, \\theta)\\) in two equivalent ways to get \\[\\begin{align*}\np(y, u , \\theta) &= p(y, u, \\theta) \\\\\np(u \\mid \\theta, y) p(\\theta \\mid y) p(y) &= p(y \\mid u, \\theta) p(u \\mid \\theta)p(\\theta). \\\\\n\\end{align*}\\]\nRearranging, we get \\[\\begin{align*}\np(\\theta \\mid y) &= \\frac{p(y \\mid u, \\theta) p(u \\mid \\theta)p(\\theta)}{p(u \\mid \\theta, y)p(y)} \\\\\n&\\propto \\frac{p(y \\mid u, \\theta) p(u \\mid \\theta)p(\\theta)}{p(u \\mid \\theta, y)}.\n\\end{align*}\\]\nThis is a very nice relationship between the functional forms of the various densities we happen to know and the density we are trying to compute. This means that if you have access to the full conditional distribution26 for \\(u\\) you can marginalise \\(u\\) out. No weird integrals required.\nBut there’s one oddity: there is a \\(u\\) on the right hand side, but no \\(u\\) on the left hand side. What we have actually found is a whole continuum of functions that are proportional to \\(p(\\theta \\mid y)\\). It truly does not matter which one we choose.\nBut some choices make the algebra slightly nicer. (And remember, I’m gonna have to implement this later, so I should probably keep and eye on that.)\nA good27 generic choice is \\(u = \\mu_{u\\mid y, \\theta}(\\theta)\\).\nThe algebra here can be a bit tricky28, so let’s write out each function evaluated at \\(u = \\mu_{u\\mid y, \\theta}(\\theta)\\).\nThe bit from the likelihood is \\[\\begin{align*}\np(y \\mid u = \\mu_{u\\mid y, \\theta}(\\theta), \\theta) &\\propto \\sigma^{-n} \\exp\\left[-\\frac{1}{2\\sigma^2}(y - A\\mu_{u\\mid y, \\theta}(\\theta))^TW^{-1}(y-  A\\mu_{u\\mid y, \\theta}(\\theta))\\right]\\\\\n&\\propto \\sigma^{-n}\\exp\\left[\\frac{-1}{2\\sigma^2} \\mu_{u\\mid y, \\theta}(\\theta)^TA^TW^{-1}A\\mu_{u\\mid y, \\theta}(\\theta) + \\frac{1}{\\sigma^2} y^T W^{-1}A \\mu_{u\\mid y, \\theta}(\\theta)\\right],\n\\end{align*}\\] where \\(n\\) is the length of \\(y\\).\nThe bit from the prior on \\(u\\) is \\[\\begin{align*}\np(\\mu_{u\\mid y, \\theta}(\\theta) \\mid \\theta )\n\\propto |Q(\\theta)|^{1/2}\\exp\\left[-\\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TQ(\\theta)\\mu_{u\\mid y, \\theta}(\\theta)\\right].\n\\end{align*}\\]\nFinally, we get that the denominator is \\[\np(\\mu_{u\\mid y, \\theta}(\\theta) \\mid y, \\theta) \\propto |Q_{u\\mid y, \\theta}(\\theta)|^{1/2}\n\\] as the exponential term29 cancels!\nOk. Let’s finish this. (Incidentally, if you’re wondering why Bayesians love MCMC, this is why.)\n\\[\\begin{align*}\np(\\theta \\mid y) &\\propto p(\\theta) \\frac{|Q(\\theta)|}{|Q_{u\\mid y, \\theta}(\\theta)|} \\exp\\left[-\\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^T(Q(\\theta) + \\frac{1}{\\sigma^2}A^TW^{-1}A)\\mu_{u\\mid y, \\theta}(\\theta) + \\frac{1}{\\sigma^2} y^T W^{-1}A \\mu_{u\\mid y, \\theta}(\\theta)\\right] \\\\\n&=  p(\\theta) \\frac{|Q(\\theta)|}{|Q_{u\\mid y, \\theta}(\\theta)|} \\exp\\left[-\\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TQ_{u\\mid y, \\theta}(\\theta)\\mu_{u\\mid y, \\theta}(\\theta) + \\frac{1}{\\sigma^2} y^T W^{-1}A \\mu_{u\\mid y, \\theta}(\\theta)\\right].\n\\end{align*}\\]\nWe can now use the fact that \\(Q_{u\\mid y, \\theta}(\\theta)\\mu_{u\\mid y, \\theta}(\\theta) = A^TW^{-1}y\\) to get\n\\[\\begin{align*}\np(\\theta \\mid y) &\\propto p(\\theta) \\frac{|Q(\\theta)|}{|Q_{u\\mid y, \\theta}(\\theta)|} \\exp\\left[-\\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TA^TW^{-1}y + \\frac{1}{\\sigma^2} y^T W^{-1}A \\mu_{u\\mid y, \\theta}(\\theta)\\right] \\\\\n&=\\frac{|Q(\\theta)|}{|Q_{u\\mid y, \\theta}(\\theta)|} \\exp\\left[\\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TA^TW^{-1}y \\right] .\n\\end{align*}\\]\nFor those who just love a log-density, this is \\[\n\\log(p(\\theta \\mid y)) = \\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TA^TW^{-1}y +\\frac{1}{2} \\log(|Q(\\theta)|) - \\frac{1}{2}\\log(|Q_{u\\mid y, \\theta}(\\theta)|).\n\\] A fairly simple expression30 for all of that work."
  },
  {
    "objectID": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#so-why-isnt-this-just-a-gaussian-process",
    "href": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#so-why-isnt-this-just-a-gaussian-process",
    "title": "Sparse Matrices 1: The linear algebra of linear mixed effects models and their generalisations",
    "section": "So why isn’t this just a Gaussian process?",
    "text": "So why isn’t this just a Gaussian process?\nThese days, people31 are more than passingly familiar32 with Gaussian processes. And so they’re quite possibly wondering why this isn’t all just an extremely inconvenient way to do the exact same computations you do with a GP.\nLet me tell you. It is all about \\(Q(\\theta)\\) and \\(A\\).\nThe prior precision matrix \\(Q(\\theta)\\) is typically block diagonal. This special structure makes it pretty easy to compute the \\(|Q(\\theta)|\\) term33. But, of course, there’s more going on here.\nIn linear mixed effects models, these blocks on the diagonal matrix are typically fairly small (their size is controlled by the number of levels in the variable you’re stratifying by). Moreover, the matrices on the diagonal of \\(Q(\\theta)\\) are the inverses of either diagonal or block diagonal matrices that themselves have quite small blocks34.\nIn models that have more structured random effects35, the diagonal blocks of \\(Q(\\theta)\\) can get quite large36. Moreover, the matrices on these blocks are usually not block diagonal.\nThankfully, these prior precision matrices do have something going for them: most of their entries are zero. We refer to these types of matrices as sparse matrices. There are some marvelous algorithms for factorising sparse matrices that are usually a lot more efficient37 than algorithms for dense matrices.\nMoreover, the formulation here decouples the dimension of the latent Gaussian component from the number of observations. The data only enters the posterior through the reduction \\(A^Ty\\), so if the number of observations is much larger than the number of latent variables38 and \\(A\\) is sparse39, the operation scales linearly in the number of observations (and obviously superlinearly40 in the row-dimension of \\(A\\)).\nSo the prior precision41 is a sparse matrix. What about the precision matrix of \\([u \\mid y, \\theta]\\)?\nIt is also sparse! Recall that \\(A = [Z \\vdots X]\\). This means that \\[\n\\frac{1}{\\sigma^2}A^TW^{-1}A = \\frac{1}{\\sigma^2}\\begin{pmatrix} Z^T W^{-1}Z & Z^T W^{-1}X \\\\ X^T W^{-1} Z & X^TW^{-1}X \\end{pmatrix}.\n\\] \\(Z\\) is a matrix that links the stacked vector of random effects \\(b\\) to each observation. Typically, the likelihood \\(p(y_i \\mid \\theta)\\) will only depend on a small number of entries of \\(b\\), which suggests that most elements in each row of \\(Z\\) will be zero. This, in turn, implies that \\(Z\\) is sparse and so is42 \\(Z^TW^{-1}Z\\).\nOn the other hand, the other three blocks are usually43 fully dense. Thankfully, though, the usual situation is that \\(b\\) has far more elements that \\(\\beta\\), which means that \\(A^TW^{-1}A\\) is still sparse and we can still use our special algorithms44\nAll of this suggests that, under usual operating conditions, \\(Q_{u\\mid y, \\theta}\\) is also a sparse matrix.\nAnd that’s great because that means that we can compute the log-posterior using only 3 main operations:\n\nComputing \\(\\log(|Q(\\theta)|)\\). This matrix is block diagonal so you can just multiply together the determinants45 of the diagonal blocks, which are relatively cheap to compute.\nComputing \\(\\mu_{u \\mid y, \\theta}(\\theta)\\). This requires solving the sparse linear system \\(Q_{u \\mid y, \\theta} \\mu_{u \\mid y, \\theta} = \\frac{1}{\\sigma^2}A^TW^{-1}y\\). This is going to rely on some fancy pants sparse matrix algorithm.\nComputing \\(\\log(|Q_{u \\mid y, \\theta}(\\theta)|)\\). This is, thankfully, a by-product of the things we need to compute to solve the linear system in the previous task."
  },
  {
    "objectID": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#what-i-what-i-what-i-gotta-do-what-i-gotta-do-to-get-this-model-in-pymc",
    "href": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#what-i-what-i-what-i-gotta-do-what-i-gotta-do-to-get-this-model-in-pymc",
    "title": "Sparse Matrices 1: The linear algebra of linear mixed effects models and their generalisations",
    "section": "What I? What I? What I gotta do? What I gotta do to get this model in PyMC?",
    "text": "What I? What I? What I gotta do? What I gotta do to get this model in PyMC?\nSo this is where shit gets real.\nEssentially, I want to implement a new distribution in PyMC that will take approprite inputs and output the log-density and its gradient. There are two ways to do this:\n\nPanic\nPray\n\nFor the first option, you write a C++46 backend and register it as an Aesara node. This is how, for example, differential equation solvers migrated into PyMC.\nFor the second option, which is going to be our goal, we light our Sinead O’Connor votive candle and program up the model using JAX. JAX is a glorious feat of engineering that makes compilable and autodiff-able Python code. In a lot of cases, it seamlessly lets you shift from CPUs to GPUs and is all around quite cool.\nIt also has approximately zero useful sparse matrix support. (It will let you do very basic things47 but nothing as complicated as we are going to need.)\nSo why am I taking this route? Well firstly I’m curious to see how well it works. So I am going to write JAX code to do all of my sparse matrix operations and see how efficiently it autodiffs it.\nNow I’m going to pre-register my expectations. I expect it to be a little bit shit. Or, at least, I expect to be able to make it do better.\nThe problem is that computing a gradient requires a single reverse-mode48 autodiff sweep. This does not seem like a problem until you look at how this sort of thing needs to be implemented and you realise that every gradient call is going to need to generate and store the entire damn autodiff tree for the log-density evaluation. And that autodiff tree is going to be large. So I am expecting the memory scaling on this to be truly shite.\nThankfully there are two ways to fix this. One of them is to implement a custom Jacobian-vector product49 and register it with JAX so it knows most of how to do the derivative. The other way is to implement this shit in C++ and register it as a JAX primitive. And to be honest I’m very tempted. But that is not where I am starting.\nThe other problem is going to be exposing this to users. The internal interface is going to be an absolute shit to use. So we are gonna have to get our Def Leppard on and sprinkle some syntactical sugar all over it.\nI’m honestly less concerned about this challenge. It’s important but I am not expecting to produce anything good enough to put into PyMC (or any other package). But I do think it’s a good idea to keep this sort of question in mind: it can help you make cleaner, more useful code.\n\nWhat comes next?\nWell you will not get a solution today. This blog post is more than long enough.\nMy plan is to do three things.\n\nImplement the relevant sparse matrix solver in a JAX-able form. (This is mostly gonna be me trying to remember how to do something I haven’t done in a very long time.)\nBind50 the (probably) inefficient version into PyMC to see how that process works.\nTry the custom jvp and vjp interfaces in JAX to see if they speed things up relative to just autodiffing through my for loops.\n(Maybe) Look into whether hand-rolling some C++ is worth the effort.\n\nWill I get all of this done? I mean, I’m skeptical. But hey. If I do it’ll be nice."
  },
  {
    "objectID": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#footnotes",
    "href": "posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model.html#footnotes",
    "title": "Sparse Matrices 1: The linear algebra of linear mixed effects models and their generalisations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\naka linear multilevel models↩︎\nPopular in epidemiology↩︎\nINLA = Laplace approximations + sparse linear algebra to do fast, fairly scalable, and accurate Bayesian inference on a variety of Bayesian models. It’s particularly good at things like spatial models.↩︎\nIn its guts, Stan is a fully templated C++ autodiff library, so I would need to add specific sparse matrix support. And then there’s be some truly gross stuff with the Stan language and its existing types. And so on and so on and honestly it just broke my damn brain. So I started a few times but never finished.↩︎\nI just don’t ever use it. I semi-regularly read and debug other people’s code, but I don’t typically write very much myself. I use R because that’s what my job needs me to use. So a shadow aim here is to just put some time into my Python. By the end of this I’ll be like Britney doing I’m a Slave 4 U.↩︎\nOr maybe more, but let’s not be too ambitious.↩︎\nI’m pretty sure they will.↩︎\nMy sparse matrix data structures are rusty as fuck.↩︎\nand the intercept if it’s needed↩︎\nReally this costs me nothing and can be useful with multiple observations.↩︎\nDefault options include the identity matrix or some multiple of the identity matrix.↩︎\nREML heads don’t dismay. You can do all kinds of weird shit by choosing some of these matrices in certain ways. I’m not gonna stop you. I love and support you. Good vibes only.↩︎\nThe priors on \\(\\beta\\) and \\(b\\) are independent Gaussian so it has to be.↩︎\nhomosexual↩︎\nInverse correlation matrix↩︎\nexcluding the fixed ones, like \\(W\\) and \\(A\\) and \\(R\\). ↩︎\nSuch a dirty word. For all of the models we care about, this is block diagonal. So this assumption is our restriction to a specific class of models.↩︎\nI would suggest a lot of syntactic sugar if you were ever going to expose this stuff to users.↩︎\nSee the Bates et al. paper. Their formulation is fabulous but doesn’t extend nicely to the situations I care about! Basically they optimise for the situation where \\(\\Sigma_b\\) can be singular, which is an issue when you’re doing optimisation. But I’m not doing optimisation and I care about the case where the precision matrix is defined as a singular matrix (and therefore \\(\\Sigma_b\\) does not exist. This seems like a truly wild idea, but it occurs quite naturally in many important models like smoothing splines and ICAR models (which are extremely popular in spatial epidemiology).↩︎\nIt’s easier in two ways. Firstly, MCMC likes lower-dimensional targets. They are typically easier to sample from! Secondly, the posterior geometry of \\(p(\\theta \\mid y)\\) is usually pretty simple, while the joint posterior \\(p(\\theta, u \\mid y)\\) has an annoying tendency to have a funnel in it, which forces us to do all kinds of annoying reparameterisation tricks to stop the sampler from shitting the bed.↩︎\nComputers!↩︎\nCSS is my passion.↩︎\nIt’s possible to rearrange things to lose that \\(\\frac{1}{\\sigma^2}\\), which I admit looks a bit weird. It cancels out down the line.↩︎\nI have, historically, not had the greatest grip on whether or not things are easy.↩︎\nSee previous footnote.↩︎\nOr a good approximation to it. Laplace approximations work very well for this to extend everything we’re doing here from a linear mixed-ish model to a generalised linear mixed-ish model.↩︎\nThis is actually a bit dangerous on the face of it because it depends on \\(\\theta\\). You can convince yourself it’s ok. Choosing \\(u=0\\) is less stress inducing, but I wanted to bring out the parallel to using a Laplace approximation to \\(p(u \\mid \\theta, y)\\), in which case we really want to evaluate the ratio at the point where the approximation is the best (aka the conditional mean).↩︎\nA common mistake is to forget the parameter dependent proportionality constants from the normal distribution. You didn’t need them before because you were conditioning on \\(\\theta\\) so they were all constant. But now \\(\\theta\\) is unknown and if we forget them an angel will cry.↩︎\nHonest footnote: This started as \\(p(\\mu_{u\\mid y, \\theta}(\\theta) \\mid y, \\theta) \\propto 1\\) because I don’t read my own warnings.↩︎\nThe brave or foolish amongst you might want to convince yourselves that this collapses to exactly the marginal likelihood we would’ve gotten from Rasmussen and Williams had we made a sequence of different life choices. In particular if \\(A = I\\) and \\(Q(\\theta) = \\Sigma(\\theta)^{-1}\\).↩︎\nOr, at least, people who have made it this far into the post.↩︎\nYou like GPs bro? Give me a sequence of increasingly abstract definitions. I’m waiting.↩︎\nMultiply the determinants of the matrices along the diagonal.↩︎\nLook at the Bates et al paper. Specifically section 2.2. lme4 is a really clever thing.↩︎\nexamples: smoothing splines, AR(p) models, areal spatial models, some Gaussian processes if you’re careful↩︎\n\\(10^4\\)–\\(10^6\\) is not unheard of↩︎\nA dense matrix factorisation of an \\(n\\times n\\) matrix costs \\(\\mathcal{O}(n^3)\\). The same factorisation of a sparse matrix can cost as little as \\(\\mathcal{O}(n)\\) if you’re very lucky. More typically it clocks in a \\(\\mathcal{O}(n^{1.5})\\)–\\(\\mathcal{O}(n^{2})\\), which is still a substantial saving!↩︎\nThis happens for a lot of designs, or when a basis spline or a Markovian Gaussian process is being used↩︎\nThis happens a lot, but not always. For instance subset-of-regressors/predictive process-type models have a dense \\(A\\). In this case, if \\(A\\) has \\(m\\) rows an \\(n\\) columns, this is an \\(\\mathcal{O}(mn)\\), which is more expensive than a sparse \\(A\\) unless \\(A\\) has roughly \\(m\\) non-zeros per row..↩︎\nbut usually not cubically. See above footnote.↩︎\nIt’s important that we are talking about precision matrices here and not covariance matrices as the inverse of a sparse matrix is typically dense. For instance, an AR(1) prior with autocorrelation parameter \\(\\rho\\) has a prior has a sparse precision matrix that looks something like \\[\nQ = \\frac{1}{\\tau^2}\\begin{pmatrix}\n1 & -\\rho &&&&& \\\\\n-\\rho&1 + \\rho^2& -\\rho&&&& \\\\\n&-\\rho& 1 + \\rho^2 &- \\rho&&& \\\\\n&&-\\rho& 1 + \\rho^2&-\\rho&& \\\\\n&&&-\\rho&1+\\rho^2 &-\\rho & \\\\\n&&&&-\\rho&1 + \\rho^2& - \\rho \\\\\n&&&&&-\\rho&1\n\\end{pmatrix}.\n\\] On the other hand, the covariance matrix is fully dense \\[\nQ^{-1} = \\tau^2\\begin{pmatrix}\n\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5&\\rho^6&\\rho^7 \\\\\n\\rho^2&\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5&\\rho^6 \\\\\n\\rho^3&\\rho^2&\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5 \\\\\n\\rho^4&\\rho^3&\\rho^2&\\rho&\\rho^2&\\rho^3&\\rho^4 \\\\\n\\rho^5&\\rho^4&\\rho^3&\\rho^2&\\rho&\\rho^2&\\rho^3 \\\\\n\\rho^6&\\rho^5&\\rho^4&\\rho^3&\\rho^2&\\rho&\\rho^2 \\\\\n\\rho^7&\\rho^6&\\rho^5&\\rho^4&\\rho^3&\\rho^2&\\rho \\\\\n\\end{pmatrix}.\n\\]\nThis is a generic property: the inverse of a sparse matrix is usually dense (it’s dense as long as the graph associated with the sparse matrix has a single connected component there’s a matrix with the same pattern of non-zeros that has a fully dense inverse) and the entries satisfy geometric decay bounds.↩︎\nRemember: \\(W\\) is diagonal and known.↩︎\nNot if you’re doing some wild dummy coding shit or modelling text, but typically.↩︎\nYou’d think that dense rows and columns would be a problem but they’re not. A little graph theory and a little numerical linear algebra says that as long as they are the last variables in the model, the algorithms will still be efficient. That said, if you want to dig in, it is possible to use supernodal (eg CHOLMOD) and multifrontal (eg MUMPS) methods to group the operations in such a way that it’s possible to use level-3 BLAS operations. CHOLMOD even spins this into a GPU acceleration scheme, which is fucking wild if you think about it: sparse linear algebra rarely has the arithmetic intensity or data locality required to make GPUs worthwhile (you spend all of your time communicating, which is great in a marriage, terrible in a GPU). But some clever load balancing, tree-based magic, and multithreading apparently makes it possible. Like truly, I am blown away by this. We are not going to do any of this because absolutely fucking not. And anyway. It’s kinda rare to have a huge number of covariates in the sorts of models that use these complex random effects. (Or if you do, you better light your Sinead O’Connor votive candle because honestly you have a lot of problems and you’re gonna need healing.)↩︎\nIf you’ve been reading the footnotes, you’ll recall that sometimes one of these precision matrices on the diagonal will be singular. Sometimes that’s because you fucked up your programming. But other times it’s because you’re using something like an ICAR (intrinsic conditional autoregressive) prior on one of your components. The precision matrix for this model is \\(Q_\\text{ICAR} = \\tau_\\text{ICAR} = \\tau \\text{Adj}(\\mathcal{G})\\), where \\(\\operatorname{Adj}(\\mathcal{G})\\) is the adjacency matrix of some fixed graph \\(\\mathcal{G}\\) (typically describing something like which postcodes are next to each other). Some theory suggests that if \\(\\mathcal{G}\\) has \\(d\\) connected components, the zero determinant should be replaced with \\(\\tau^{(m - d)/2}\\), where \\(m\\) is the number of vertices in \\(\\mathcal{G}\\).↩︎\nI guess there’s nothing really stopping you from writing in pure Python except a creeping sense of inadequacy.↩︎\neg build a sparse matrix↩︎\nHoney, we do not have time. Understanding autodiff is not massively important in the grand scheme of this blogpost (or, you know, probably in real life unless you do some fairly specific things). I’ll let Charles explain it.↩︎\nOr, a custom vector-Jacobian product, which is not a symmetrical choice.↩︎\nI bind you Nancy!↩︎"
  },
  {
    "objectID": "posts/2021-10-14-priors1/priors1.html",
    "href": "posts/2021-10-14-priors1/priors1.html",
    "title": "Priors: Night work (Track 1)",
    "section": "",
    "text": "I have feelings. Too many feelings. And ninety six point seven three percent of them are about prior distributions1. So I am going to write a few blog posts about prior distributions.\nTo be very honest, this is mostly a writing exercise2 to get me out of a slump.\nSo let’s do this."
  },
  {
    "objectID": "posts/2021-10-14-priors1/priors1.html#no-love-deep-web",
    "href": "posts/2021-10-14-priors1/priors1.html#no-love-deep-web",
    "title": "Priors: Night work (Track 1)",
    "section": "No love, deep web",
    "text": "No love, deep web\nAs far as I am concerned it’s really fucking stupid to try to write about priors on their own. They are meaningless outside of their context. But, you know, this is a blog. So I get to be stupid.\nSo what is a prior distribution? It is whatever you want it to be. It is a probability distribution3 that … I don’t know. Exists4.\nOk. This is not going well. Let’s try again.\nA prior distribution is, most of the time, a probability distribution on the parameters of a statistical model. For all practical purposes, we tend to work with its density, so if the parameter \\(\\theta\\), which could be a scalar but, in any interesting case, isn’t, has prior \\(p(\\theta)\\)."
  },
  {
    "objectID": "posts/2021-10-14-priors1/priors1.html#captain-fantastic-and-the-brown-dirt-cowboy",
    "href": "posts/2021-10-14-priors1/priors1.html#captain-fantastic-and-the-brown-dirt-cowboy",
    "title": "Priors: Night work (Track 1)",
    "section": "Captain fantastic and the brown dirt cowboy",
    "text": "Captain fantastic and the brown dirt cowboy\nBut what does it all meeeeeeeean?\nWe have a prior distribution specified, gloriously, by it’s density. And unlike destiny, density is meaningless. It only makes sense when we integrate it up to get a probability \\[\n\\Pr(A) = \\int_A p(\\theta)\\,d\\theta.\n\\]\nSo what does the prior probabilty \\(\\Pr(A)\\) of a set \\(A\\) actually mean in real life? The answer may shock you: it means something between nothing and everything.\nScenario 1: Let’s imagine that we were trying to estimate the probability that someone in some relative homogeneous subgroup of customers completed a purchase on our website. It’s a binary process, so the parameter of interest can probably just be the probability that a sale is made. While we don’t know what the probability of a sale is for the subgroup of interest, we know a lot sales on our website in general (in particular, we know that about 3% of visits result in sales). So if I also believe that it would be wildly unlikely for 20% of visits to result in a sale, I could posit a prior like a \\(\\text{Beta}(0.4,5)\\) prior that captures (a version of) these two pieces of information.\n\n  ## Step 1: \n  \nfn &lt;- \\(x) (qbeta(0.5,x[1], x[2]) - 0.02)^2 + \n  (qbeta(0.9, x[1], x[2]) - 0.2)^2\n\nbest &lt;- optim(c(1/2,1/2), fn)\n\n## Step 3: Profit.\n## (AKA round and check)\nqbeta(0.9, 0.4, 5)\nqbeta(0.5, 0.4, 5)\n\nScenario 2: Let’s imagine I want to do variable selection. I don’t know why. I was just told I want to do variable selection. So I fire up the Bayesian Lasso5 and then threshold in some way. In this case, the prior encode a hoped-for property of my posterior. (To paraphrase Lana, hope is a dangerous thing for a woman like you to have because the Bayeisan Lasso does not work to the point that the original paper doesn’t even suggest using it for variable selection6 it just, idk, liked the name. Statistics is wild.)\nScenario 3: I’m doing a regression with just one variable (because why not) and I think that the relationship between the response \\(y\\) and the covariate \\(x\\) is non-linear. That is, I think there is some unknown to me function \\(f(x)\\) such that \\(\\mathbb{E}(y_i) = f(x_i)\\). So I ask a friend and they tell me to use a Gaussian Process prior for \\(f(\\cdot)\\) with an exponential covariance function.\nWhile I can write down the density for the joint prior of \\((f(x_1), f(x_2,), \\ldots, f(x_n))\\), I do not know7 what this prior means in any substantive sense. But I can tell you, you’re gonna need that maths degree to even try.\nAnd should you look deeper, you will find more and more scenarios where priors are doing different things for different reasons8. For each of these priors in each of these scenarios, we will be able to compute the posterior (or a reasonable computational approximation to it) and then work with that posterior to answer our questions.\nDifferent people9 will use priors different ways even for very similar problems10. This remains true even though they are nominally working under the same inferential framework.\nBayesians are chaotic."
  },
  {
    "objectID": "posts/2021-10-14-priors1/priors1.html#mapping-out-a-sky-what-you-feel-like-planning-a-sky",
    "href": "posts/2021-10-14-priors1/priors1.html#mapping-out-a-sky-what-you-feel-like-planning-a-sky",
    "title": "Priors: Night work (Track 1)",
    "section": "Mapping out a sky / What you feel like, planning a sky",
    "text": "Mapping out a sky / What you feel like, planning a sky\nSondheim’s ode to pointillism feels relevant here. The reality of the prior distribution—and the whole reason the concept is so slippery and chaotic—is that you are, dot by dot, constructing the world of your inference. This act of construction is fundamental to understanding how Bayesian methods work, how to justify your choices, and how to use a Bayesian workflow to solve complex problems.\nTo torture the metaphor, our prior distribution is just our paint, unmixed, slowly congealing, possibly made of ground up mummys. It is nothing without a painter and a brush.\nThe painter is the likelihood or, more generally, the generative link between the parameter values and the actual data, \\(p(y \\mid \\theta)\\). The brush is the computational engine you use to actually produce the posterior painting11.\nThis then speaks to the core challenge with writing about priors: it depends on how you use them. It is a fallacy, or perhaps a foolishness, or perhaps a heresy12. Hell, when trying to understand a single inference The Prior Can Only Be Understood In The Context Of The Likelihood13. In the context of an entire workflow, The Experiment is just as Important as the Likelihood in Understanding the Prior.\nFor instance, using independent Cauchy priors for the coefficients in a linear regression model will result in a perfectly ok posterior. Whereas the same priors used in a logistic regression, you may end up with posteriors with such heavy tails that they don’t have a mean! (Do we care? Well, yes. If we want reasonable uncertainty intervals we probably want 2 or so moments otherwise those large deviations are gonna getcha!)"
  },
  {
    "objectID": "posts/2021-10-14-priors1/priors1.html#so-what",
    "href": "posts/2021-10-14-priors1/priors1.html#so-what",
    "title": "Priors: Night work (Track 1)",
    "section": "So what?",
    "text": "So what?\nAll of this is fascinating. And it is a lot less chaotic than it initially sounds.\nThe reality is that while two Bayesians may use different priors and, hence, produce different posteriors for the same data set.This can be extreme. For example, if I am trying to estimate the mean of data generated by \\(y_i \\sim N(\\mu, 1)\\), then I can choose a prior14 (that depends on the data) so that the posterior mean \\(\\mathbb{E}(\\mu \\mid y) =1\\). Or, to put it differently, I can get any answer I want if I choose an prior carefully (and in a data-dependent manner).\nBut this isn’t necessarily a problem. This is because the posteriors produced by two sensible priors for the same problem will produce fairly similar results15. The prior I used to cheat in the previous example would not be considered sensible by anyone looking at it16.\nBut what is a sensible prior? Can you tell if a prior is sensible or not in its particular context? Well honey, how long have you got. The thing about starting a (potential) series of blog posts is that I don’t really know how far I’m going to get, but I would really like to talk a lot about that over the next little while."
  },
  {
    "objectID": "posts/2021-10-14-priors1/priors1.html#footnotes",
    "href": "posts/2021-10-14-priors1/priors1.html#footnotes",
    "title": "Priors: Night work (Track 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe rest are about the night I saw Patti LuPone trying to get through the big final scene in War Paint as part of her costume loudly disintegrated.↩︎\nI’m told it’s useful to warm up sometimes because this pandemic has me ice cold.↩︎\nSometimes.↩︎\nExcept, and I cannot stress this enough, when it doesn’t.↩︎\nPlease do not do this!↩︎\nExcept for once in the abstract in a sentence that is in no way shape or formed backed up in the text. Park and Casella (2008)↩︎\nI do know. I know a very large amount about Gaussian processes. But lord in heaven I have seen the greatest minds of my generation subtly fuck up the interpretation of GP priors. Because it’s increadibly hard. Maybe I’ll blog about it one day. Because this is in markdown so I can haz equations.↩︎\nSome reasons are excellent. Some, like the poor Bayesian Lasso, are simply misguided.↩︎\nor the same person in different contexts↩︎\nAre any two statistical problems ever the same?↩︎\nYes. I have a lot of feelings about this too, but meh. A good artist can make great art with minimal equipment (see When Doves Cry), but most people are not the genius Prince was so just use good tools and stress less!↩︎\nI have written extensively about priors in the context of the Arianist heresy because of course I fucking have. Part 1, Part 2, Part 3. Apologies for mathematics eaten by a recent formatting change!↩︎\nEditors forced the word often into the published title and, like, who’s going to fight?↩︎\n\\(N(2-\\bar{y},n^{-1})\\)↩︎\nWhat does this even mean? Depends on your context really. But a working definition is that the big picture features of the posterior are similar enough that if you were to use it to make a decision, that decision doesn’t change very much.↩︎\nBut omg subtle high dimensional stuff and I guess I’ll talk about that later maybe too?↩︎"
  },
  {
    "objectID": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html",
    "href": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html",
    "title": "The king must die (repost)",
    "section": "",
    "text": "And then there was Yodeling Elaine, the Queen of the Air. She had a dollar sign medallion about as big as a dinner plate around her neck and a tiny bubble of spittle around her nostril and a little rusty tear, for she had lassoed and lost another tipsy sailor—Tom Waits\nIt turns out I turned thirty two1 and became unbearable. Some of you may feel, with an increasing sense of temporal dissonance, that I was already unbearable2. Others will wonder how I can look so good at my age3. None of that matters to me because all I want to do is talk about the evils of marketing like the 90s were a vaguely good idea4.\nThe thing is, I worry that the real problem in academic statistics in 2017 is not a reproducibility crisis, so much as that so many of our methods just don’t work. And to be honest, I don’t really know what to do about that, other than suggest that we tighten our standards and insist that people proposing new methods, models, and algorithms work harder to sketch out the boundaries of their creations. (What a suggestion. Really. Concrete proposals for concrete change. But it’s a blog. If ever there was a medium to be half-arsed in it’s this one. It’s like twitter for people who aren’t pithy.)"
  },
  {
    "objectID": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#berätta-för-mig-om-det-är-sant-att-din-hud-är-doppad-i-honung",
    "href": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#berätta-för-mig-om-det-är-sant-att-din-hud-är-doppad-i-honung",
    "title": "The king must die (repost)",
    "section": "Berätta för mig om det är sant att din hud är doppad i honung",
    "text": "Berätta för mig om det är sant att din hud är doppad i honung\nSo what is the object of my impotent ire today. Well nothing less storied than the Bayesian Lasso.\nIt should be the least controversial thing in this, the year of our lord two thousand and seventeen, to point out that this method bears no practical resemblance to the Lasso. Or, in the words of Law and Order: SVU, “The [Bayesian Lasso] is fictional and does not depict any actual person or event”."
  },
  {
    "objectID": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#who-do-you-think-you-are",
    "href": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#who-do-you-think-you-are",
    "title": "The king must die (repost)",
    "section": "Who do you think you are?",
    "text": "Who do you think you are?\nThe Bayesian Lasso is a good example of what’s commonly known as the Lupita Nyong’o fallacy5, which goes something like this: Lupita Nyong’o had a break out role in Twelve Years a Slave, she also had a heavily disguised role in one of ’ the Star Wars films (the specific Star Wars film is not important. I haven’t seen it and I don’t care). Hence Twelve Years a Slave exists in the extended Star Wars universe.6\nThe key point is that the (classical) Lasso plays a small part within the Bayesian Lasso (it’s the MAP estimate) in the same way that Lupita Nyong’o played a small role in that Star Wars film. But just as the presence of Ms Nyong’o does not turn Star Wars into Twelve Years a Slave, the fact that the classical Lasso can be recovered as the MAP estimate of the Bayesian Lasso does not make the Bayesian Lasso useful.\nAnd yet people still ask if they can be fit in Stan. In that case, Andrew answered the question that was asked, which is typically the best way to deal with software enquiries7. But I am brave and was not asked for my opinion, so I’m going to talk about why the Bayesian Lasso doesn’t work."
  },
  {
    "objectID": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#hiding-all-away",
    "href": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#hiding-all-away",
    "title": "The king must die (repost)",
    "section": "Hiding all away",
    "text": "Hiding all away\nSo why would anyone not know that the Bayesian Lasso doesn’t work? Well, I don’t really know. But I will point out that all of the results that I’ve seen in this directions (not that I’ve been looking hard) have been published in the prestigious but obtuse places like Annals of Statistics, the journal we publish in when we either don’t want people without a graduate degree in mathematical statistics to understand us or when we want to get tenure.\nBy contrast, the original paper is very readable and published in JASA, where we put papers when we are ok with people who do not have a graduate degree in mathematical statistics being able to read them, or when we want to get tenure8.\nTo be fair to Park and Casella, they never really say that the Baysian Lasso should be used for sparsity. Except for one sentence in the introduction where they say the median gives approximately sparse estimators and the title which links it to the most prominent and popular method for estimating a sparse signal. Marketing eh. (See, I’m Canadian now9).\n##The devil has designed my death and is waiting to be sure\nSo what is the Bayesian LASSO (and why did I spend 600 words harping on about something before defining it? The answer will shock you. Actually the answer will not shock you, it’s because it’s kinda hard to do equations on this thing10.)\nFor data observed with Gaussian error, the Bayesian Lasso takes the form \\[\n\\mathbf{y} \\mid \\boldsymbol{\\beta} \\sim N( \\mathbf{X} \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma})\n\\]\nwhere, instead of putting a Normal prior on \\(\\boldsymbol{\\beta}\\) as we would in a bog-standard Bayesian regression, we instead use independent Laplace priors \\[\np(\\beta_i) = \\frac{\\lambda}{2} \\exp(-\\lambda | \\beta_i|).\n\\]\nHere the tuning parameter11 \\(\\lambda = c(p,s_0,\\mathbf{X})\\tilde{\\lambda}\\) where \\(p\\) is the number of covariates, \\(s_0\\) is the number of “true” non-zero elements of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Sigma}\\) is known, and \\(\\tilde{\\lambda}\\) is an unknown scaling parameter that should be \\(\\mathcal{O}(1)\\).\nImportant Side note: This isn’t the exact same model as Park and Castella used as they didn’t use the transformation \\[\n\\lambda = c(p,s_0,\\mathbf{X}) \\tilde{\\lambda}\n\\] but rather just dealt with \\(\\lambda\\) as the parameter. We will see below, and it’s born out by many papers in this field, that the best possible value of \\(\\lambda\\) will depend on this structural/design information\nIf we know how \\(\\lambda\\) varies as the structural/design information changes, it’s a much better idea to put a prior on \\(\\tilde{\\lambda}\\) than on \\(\\lambda\\) directly. Why? Because a prior on \\(\\lambda\\) needs to depend on p, \\(s_0\\), and X and hence needs to be changed for each problem, while a prior on \\(\\tilde{\\lambda}\\) can be used for many problems. One possible option is \\(c(p,s_0,\\mathbf{X}) = 2\\|\\mathbf{X}\\|\\sqrt{\\log p }\\), which is a rate optimal parameter for the (non-Bayesian) Lasso. Later, we’ll do a back-of-the-envelope calculation that suggests we might not need the square root around the logarithmic term.\n\nWhy do we scale priors\nThe critical idea behind the Bayesian Lasso is that we can use the i.i.d. Laplace priors to express the substantive belief that the most of the \\(\\beta_j\\) are (approximately) zero. The reason for scaling the prior is that the values of \\(\\lambda\\) that are consistent with this belief depend on \\(p\\), \\(s_0\\), and \\(X\\).\nFor example, \\(\\lambda = 1\\), the Bayesian Lasso will not give an approximately sparse signal.\nWhile we could just use a prior for \\(\\lambda\\) that has a very heavy right tail (something like an inverse gamma), this is at odds with a good practice principle of making sure all of thee parameters in your models are properly scaled to make them order 1. Why do we do this? Because it makes it much much easier to set sensible priors.\nSome of you may have noticed that the scaling \\(c(p,s_0,\\mathbf{X})\\) can depend on the unknown sparsity \\(s_0\\). This seems like cheating. People who do asymptotic theory call this sort of value for \\(\\lambda\\) an oracle value, mainly because people studying Bayesian asymptotics are really really into database software.\nThe idea is that this is the value of \\(\\lambda\\) that gives the model the best chance of working. When maths-ing, you work out the properties of the posterior with the oracle value of \\(\\lambda\\) and then you use some sort of smoothness argument to show that the actual method that is being used to select (or average over) the parameter gives almost the same answer.\nIt’s also worth noting that the scaling here doesn’t (directly12) depend on the number of observations, only the number of covariates. This is appropriate: it’s ok for priors to depend on things that should be known a priori (like the number of parameters) or things that can be worked with13 (like the scaling of \\(X\\)). It’s a bit weirder if it depends on the number of observations (that tends to break things like coherent Bayesian updating, while the other dependencies don’t)."
  },
  {
    "objectID": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#only-once-in-sheboygan.-only-once.",
    "href": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#only-once-in-sheboygan.-only-once.",
    "title": "The king must die (repost)",
    "section": "Only once in Sheboygan. Only once.",
    "text": "Only once in Sheboygan. Only once.\nSo what’s wrong with the Bayesian Lasso? Well the short version is that the Laplace prior doesn’t have enough mass near zero relative to the mass in the tails to allow for a posterior that has a lot of entries that are almost zero and some entries that are emphatically not zero.\nBecause the Bayesian Lasso prior does not have a spike at zero, none of the entries will be a priori exactly zero, so we need some sort of rule to separate the “zero” entries from the “non-zero” entries. The way that we’re going to do this is to choose a cutoff \\(\\epsilon\\) where we assume that if \\(|\\beta_j| &lt;\\epsilon\\), then \\(\\beta_j =0\\).\nSo how do we know that the Lasso prior doesn’t put enough mass in important parts of the parameter space? Well there are two ways. I learnt it during the exciting process of writing a paper that the reviewers insisted should have an extended section about sparsity (although this was at best tangential to the rest of the paper), so I suddenly needed to know about Bayesian models of sparsity. So I read those Annals of Stats papers. (That’s why I know I should be scaling \\(\\lambda\\)!).\nWhat are the key references? Well all the knowledge that you seek is here and here.\nBut a much easier way to work out that the Bayesian Lasso is bad is to do some simple maths.\nBecause the \\(\\beta_j\\) are a priori independent, we get a prior on the effective sparsity \\(s_\\epsilon = \\#\\{j : |\\beta_j|  &gt; \\epsilon\\}\\) \\[\ns_\\epsilon \\sim \\text{Bin}(p, \\Pr(|\\beta_j| &gt; \\epsilon)).\n\\] For the Bayesian Lasso, that probability can be computed as \\[\n\\Pr ( | \\beta_j | &gt; \\epsilon ) = e^{- \\lambda \\epsilon},\n\\] so \\[\ns_\\epsilon \\sim \\text{Bin}\\left(p, e^{-\\lambda \\epsilon}\\right).\n\\]\nIdeally, the distribution of this effective sparsity would be centred on the true sparsity.\nSo we’d like to choose \\(\\lambda\\) so that \\[\n\\mathbb{E}(s_\\epsilon)= p e^{- \\lambda \\epsilon}= s_0.\n\\]\nA quick re-arrangement suggests that \\[\n\\lambda = \\epsilon^{-1} \\log(p) - \\epsilon^{-1} \\log(s_0).\n\\]\nNow, we are interested in signals with \\(s_0 = o(p)\\), i.e. where only a very small number of the \\(\\beta_j\\) are non-zero. This suggests we can safely ignore the second term as it will be much smaller than the first term.\nTo choose \\(\\epsilon\\), we can work from the general principle that we want to choose it so that the effect of the “almost zero” \\(\\beta_j\\) \\[\n\\sum_{j:|\\beta_j| &lt; \\epsilon} \\beta_j X_{:j}\n\\] is small. (here \\(X_{:j}\\) is the \\(j\\)th column of the matrix \\(X\\).)\nFrom this, it’s pretty clear that \\(\\epsilon\\) is going to have to depend on \\(p\\), \\(s_0\\), and \\(X\\) as well! But how?\nWell, first we note that \\[\n\\sum_{j:|\\beta_j| &lt; \\epsilon} \\beta_j X_{:j} \\leq \\epsilon \\max_{i =1,\\ldots, n}\\sum_{j=1}^p |X_{ij}| = \\epsilon \\|X\\|_\\infty.\n\\] Hence we can make this asymptotically small (as \\(p\\rightarrow \\infty\\)) if \\[\n\\epsilon = o\\left(\\|X\\|_\\infty^{-1}\\right).\n\\] Critically, if we have scaled the design matrix so that each covariate is at most \\(1\\), ie \\[\n\\max_{i=1,\\ldots,n} |X_{ij}| \\leq 1, \\qquad \\text{for all } j = 1,\\ldots, p,\n\\] then this reduces to the much more warm and fuzzy \\[\n\\epsilon = o\\left(p^{-1}\\right).\n\\]\nThis means that we need to take \\(\\lambda = \\mathcal{O}(p \\log(p))\\) in order to ensure that we have our prior centred on sparse vectors (in the sense that the prior mean for the number of non-zero components is always much less than \\(p\\))."
  },
  {
    "objectID": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#show-some-emotion",
    "href": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#show-some-emotion",
    "title": "The king must die (repost)",
    "section": "Show some emotion",
    "text": "Show some emotion\n\n\n\n\n\n\n\n\n\nSo for the Bayesian Lasso, a sensible parameter is \\(\\lambda = p\\log p\\), which will usually have a large number of components less than the threshold \\(\\epsilon\\) and a small number that are larger.\nBut this is still a bad prior.\nTo see this, let’s consider the prior probability of seeing a \\(\\beta_j\\) larger than one \\[\n\\Pr ( | \\beta_j | &gt; 1) = p^{-p} \\downarrow \\downarrow \\downarrow 0.\n\\]\nThis is the problem with the Bayesian Lasso: in order to have a lot of zeros in the signal, you are also forcing the non-zero elements to be very small. A plot of this function is above, and it’s clear that even for very small values of \\(p\\) the probability of seeing a coefficient bigger than one is crushingly small.\nBasically, the Bayesian Lasso can’t give enough mass to both small and large signals simultaneously. Other Bayesian models (such as the horseshoe and the Finnish horseshoe) can support both simultaneously and this type of calculation can show that (although it’s harder. See Theorem 6 here).\n(The scaling that I derived in the previous section is a little different to the standard Lasso scaling of \\(\\lambda = \\mathcal{O} (p \\sqrt{\\log p})\\), but the same result holds: for large \\(p\\) the probability of seeing a large signal is vanishingly small.)"
  },
  {
    "objectID": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#maybe-i-was-mean-but-i-really-dont-think-so",
    "href": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#maybe-i-was-mean-but-i-really-dont-think-so",
    "title": "The king must die (repost)",
    "section": "Maybe I was mean, but I really don’t think so",
    "text": "Maybe I was mean, but I really don’t think so\nThis analysis is all very back of the envelope, but it contains a solid grain of truth14.\nIf you fit a Bayesian Lasso in Stan with an unknown scaling parameter \\(\\lambda\\), you will not see estimates that are all zero, like this analysis suggests. This is because the posterior for \\(\\lambda\\) tries to find the values of the parameters that best fit the data and not the values that give an \\(\\epsilon\\)-sparse signal.\nIn order to fit the data, it is important that the useful covariates have large \\(\\beta\\)s, which, in turn, forces the \\(\\beta\\)s that should be zero to be larger than our dreamt of \\(\\epsilon\\).\nAnd so you see posteriors constructed with the Bayesian Lasso exisiting in some sort of eternal tension: the small \\(\\beta\\)s are too big, and the large \\(\\beta\\)s are typically shrunken towards zero.\nIt’s the sort of compromise that leaves everyone unhappy.\nLet’s close it out with the title song.\n\nAnd I’m so afraid your courtiers\nCannot be called best friends\n\nLightly re-touched from the original, posted on Andrew’s blog. Orignal verison, 2 November, 2017."
  },
  {
    "objectID": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#footnotes",
    "href": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost.html#footnotes",
    "title": "The king must die (repost)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(2021 edit) I am no longer 32. I am still unbearable.↩︎\nFair point↩︎\nAnswer: Black Metal↩︎\nThey were not. The concept of authenticity is just another way for the dominant culture to suppress more interesting ones.↩︎\n(2021 edit): Really, Daniel? Really?↩︎\n(2021): Ok. That ended better than I feared.↩︎\nIt’s usually a fool’s game to try to guess why people are asking particular questions. It probably wouldn’t be hard for someone to catalogue the number of times I’ve not followed my advice on this, but in life as in statistics, consistency is really only a concern if everything else is going well.↩︎\n2021: Look at me trying to land a parallel construction.↩︎\n2021: The other week someone asked if I was Canadian, which is a sure sign that my accent is just broken.↩︎\n2021: Prophetic words↩︎\nCould we put a prior on this? Sure. And in practice this is what we should probably do. But for today, we are going to keep it fixed.↩︎\nIt depends on \\(\\|X\\|\\) which could depend on the number of observations.↩︎\nThere’s a lot of space for interesting questions here.↩︎\nIt’ws also fully justified by people who have written far more mathematically sophisticated papers on this topic!↩︎"
  },
  {
    "objectID": "posts/2023-01-21-markov/markov.html",
    "href": "posts/2023-01-21-markov/markov.html",
    "title": "Markovian Gaussian processes: A lot of theory and some practical stuff",
    "section": "",
    "text": "Gaussian processes are lovely things. I’m a big fan. They are, however, thirsty. They will take your memory, your time, and anything else they can. Basically, the art of fitting Gaussian process models is the fine art of reducing the GP model until it’s simple enough to fit while still being flexible enough to be useful.\nThere’s a long literature on effective approximation to Gaussian Processes that don’t turn out to be computational nightmares. I’m definitely not going to summarise them here, but I’ll point to an earlier (quite technical) post that mentioned some of them. The particular computational approximation that I am most fond of makes use of the Markov property and efficient sparse matrix computations to reduce memory use and make the linear algebra operations significantly faster.\nOne of the odder challenges with Markov models is that information about how Markov structures work in more than one dimension can be quite difficult to find. So in this post I am going to lay out some of the theory.\nA much more practical (and readable) introduction to this topic can be found in this lovely paper by Finn, David, and Håvard. So don’t feel the burning urge to read this post if you don’t want to. I’m approaching the material from a different viewpoint and, to be very frank with you, I was writing something else and this section just became extremely long so I decided to pull it out into a blog post.\nSo please enjoy today’s entry in Dan writes about the weird corners of Gaussian processes. I promise that even though this post doesn’t make it seem like this stuff is useful, it really is. If you want to know anything else about this topic, essentially all of the Markov property parts of this post come from Rozanov’s excellent book Markov Random Fields."
  },
  {
    "objectID": "posts/2023-01-21-markov/markov.html#gaussian-processes-via-the-covariance-operator",
    "href": "posts/2023-01-21-markov/markov.html#gaussian-processes-via-the-covariance-operator",
    "title": "Markovian Gaussian processes: A lot of theory and some practical stuff",
    "section": "Gaussian processes via the covariance operator",
    "text": "Gaussian processes via the covariance operator\nBy the end of today’s post we will have defined1 a Markovian process in terms of its reproducing Kernel Hilbert space (RKHS), that is the space of functions that contain the posterior mean2 when there are Gaussian observations. This space always exists and its inner product is entirely determined by the covariance function of a GP. That said, for a given covariance function, the RKHS can. be difficult to find. Furthermore, the problem with basing our modelling off a RKHS is that it is not immediately obvious how we will do the associated computations This is in contrast to a covariance function approach, where it is quite easy3 to work out how to convert the model specification to something you can attack with a computer. By the end of this post we will have tacked that.\nThe extra complexity of the RKHS pays off in modelling flexibility, both in terms of the types of model that can be build and the spaces4 you can build them on. I am telling you this now because things are about to get a little mathematical.\nTo motivate the technique, let’s consider the covariance operator \\[\n[\\mathcal{C}f](s) = \\int_T c(s, s') f(s') \\, ds',\n\\] where \\(T\\) is the domain over which the GP is defined (usually \\(\\mathbb{R}^d\\) but maybe you’re feeling frisky).\nTo see how this could be useful, we are going to need to think a little bit about how we can simulate a multivariate Gaussian random variable \\(N(0, \\Sigma)\\). To do this, we first compute the square root5 \\(L = \\Sigma^{1/2}\\) and sample a vector of iid standard normal variables \\(z \\sim N(0,I)\\). Then \\(u = Lz \\sim N(0, \\Sigma)\\). You can check it by checking the covariance. (it’s ok. I’ll wait.)\nWhile the square root of the covariance operator \\(\\mathcal{C}^{1/2}\\) is a fairly straightforward mathematical object6, the analogue of the iid vector of standard normal random variables is a bit more complex.\n\nWhite noise and its associated things\nThankfully I’ve covered this in a previous blog. The engineering definition of white noise as a GP \\(w(\\cdot)\\) such that for every \\(s\\), \\(w(s)\\) is an iid \\(N(0,1)\\) random variable is not good enough for our purposes. Such a process is hauntingly irregular7 and it’s fairly difficult to actually do anything with it. Instead, we consider white noise as a random function defined on the subsets of our domain. This feels like it’s just needless technicality, but it turns out to actually be very very useful.\n\nDefinition 1 (White noise) A (complex) Gaussian white noise is a random measure8 \\(W(\\cdot)\\) such that, for every9 disjoint10 pair of sets \\(A, B\\) satisfies the following properties\n\n\\(W(A) \\sim N(0, |A|)\\)\nIf \\(A\\) and \\(B\\) are disjoint then \\(W(A\\cup B) = W(A) + W(B)\\)\nIf \\(A\\) and \\(B\\) are disjoint then \\(W(A)\\) and \\(W(B)\\) are uncorrelated11, ie \\(\\mathbb{E}(W(A) \\overline{W(B)}) = 0\\).\n\n\nThis doesn’t feel like we are helping very much because how on earth am I going to define the product \\(\\mathcal{C}^{1/2} W\\)? Well the answer, you may be shocked to discover, requires a little bit more maths. We need to define an integral, which turns out to not be shockingly difficult to do. The trick is to realise that if I have an indicator function \\[\n1_A(s) = \\begin{cases} 1, \\qquad &s \\in A \\\\ 0, & s \\not \\in A \\end{cases}\n\\] then12 \\[\n\\int_T 1_A(s)\\, dW(s) = \\int_A dW(s) = W(A) \\sim N(0, |A|).\n\\] In that calculation, I just treated \\(W(s)\\) like I would any other measure. (If you’re more of a probability type of girl, it’s the same thing as noticing \\(\\mathbb{E}(1_A(X) = \\Pr(X \\in A)\\).)\nWe can extend the above by taking the sum of two indicator function \\[\nf(s) = f_11_{A_1}(s) + f_2 1_{A_2}(s),\n\\] where \\(A_1\\) and \\(A_2\\) are disjoint and \\(f_1\\) and \\(f_2\\) are any real numbers. By the same reasoning above, and using the linearity of the integral, we get that \\[\\begin{align*}\n\\int_T f(s) \\, dW(s) &= f_1 \\int_{A_1} \\,d W(s) + f_2 \\int_{A_2} \\,d W(s) \\\\\n&= N(0, f_1^2 |A_1| + f_2^2 |A_2|) \\\\\n&= N\\left(0, \\int_T f(s)^2 \\,ds\\right),\n\\end{align*}\\] where the last line follows by doing the ordinary13 integral of \\(f(s)\\).\nIt turns out that every interesting function can be written as the limit of piecewise constant functions14 and we can therefore define for any function15 \\(f\\in L^2(T)\\) \\[\n\\int f(s) \\, dW(s) \\sim N\\left(0, \\int_T f(s)^2 \\,ds\\right).\n\\]\nWith this notion in hand, we can finally define the action of an operator on white noise.\n\nDefinition 2 (The action of an operator on white noise) Let \\(\\mathcal{A}\\) be an operator on some Hilbert space of functions \\(H\\) with adjoint \\(\\mathcal{A}^*\\), then we define \\(\\mathcal{A}W\\) to be the random measure that satisfies, for every \\(f \\in \\operatorname{Dom}(\\mathcal{A^*})\\), \\[\n\\int_T f(s) \\, d (\\mathcal{A}W)(s) = \\int_T \\mathcal{A}^*f(s) \\, dW(s).\n\\]\n\n\n\nThe generalised Gaussian process \\(\\eta = \\mathcal{C}^{1/2}W\\)\nOne of those inconvenient things that you may have noticed from above is that \\(\\mathcal{C}^{1/2}W\\) is not going to be a function. It is going to be a measure or, as it is more commonly known, a generalised Gaussian process. This is the GP analogue of a generalised function and, as such, only gives an actual value when you integrate it against some sufficiently smooth function.\n\nDefinition 3 (Generalised Gaussian Process) A generalised Gaussian process \\(\\xi\\) is a random signed measure (or a random generalised function) that, for any \\(f \\in C^\\infty_0(T)\\), \\(\\int_T f(s)\\,d\\xi(s)\\) is Gaussian. We will often write \\[\n\\xi(f) = \\int_T f(s)\\,d\\xi(s),\n\\] which helps us understand that a generalised GP is indexed by functions.\n\nIn order to separate this out from the ordinary GP \\(u(s)\\), we will write it as \\[\n\\eta = \\mathcal{C}^{1/2}W.\n\\] These two ideas coincide in the special case where \\[\n\\eta = u(s)\\,ds,\n\\] which will occur when \\(\\mathcal{C}^{1/2}\\) smooths the white noise sufficiently. In all of the cases we really care about today, this happens. But there are plenty of Gaussian processes that can only be considered as generalised GPs16\n\n\nApproximating GPs when \\(\\mathcal{C}^{-1/2}\\) is a differential operator\nThis type of construction for \\(\\eta\\) is used in two different situations: kernel convolution methods directly use the representation, and the SPDE methods of Lindgren, Lindström and Rue use it indirectly.\nI’m interested in the SPDE method, as it ties into today’s topic. Also because it works really well. This method uses a slightly modified version of the above equation \\[\n\\mathcal{C}^{-1/2}\\eta = W,\n\\] where \\(\\mathcal{C}^{-1/2}\\) is the (left) inverse of \\(\\mathcal{C}^{1/2}\\). I have covered this method in a previous post, but to remind you the SDPE method in its simplest form involves three steps:\n\nApproximate \\(\\eta = \\sum_{j=1}^n u_j \\psi_j(s)\\,ds\\) for some set of weights \\(u \\sim N(0, Q^{-1})\\) and a set of deterministic functions \\(\\psi_j\\) that we are going to use to approximate the GP\nApproximate17 the test function \\(f = \\sum_{k=1}^n f_k \\psi_k(s)\\) for some set of deterministic weights \\(f_j\\)\nPlug these approximations into the equation \\(\\mathcal{C}^{-1/2} \\eta = W\\) to get the equation \\[\n\\sum_{k,j=1}^n u_j f_k \\int_T \\psi_k(s) \\mathcal{C}^{-1/2} \\psi_j(s)\\,ds \\sim N\\left(0, \\sum_{j,k=1}^n \\psi_j(s)\\psi_k(s)\\,ds\\right)\n\\]\n\nAs this has to be true for every vector \\(f\\), this is equivalent to the linear system \\[\nK u \\sim N(0, C),\n\\] where \\(K_{kj} =  \\int_T \\psi_k(s) \\mathcal{C}^{-1/2} \\psi_j(s)\\,ds\\) and \\(C_{kj} = \\sum_{j,k=1}^n \\psi_j(s)\\psi_k(s)\\).\nObviously this method is only going to be useful if it’s possible to compute the elements of \\(K\\) and \\(C\\) efficiently. In the special case where \\(\\mathcal{C}^{-1/2}\\) is a differential operator18 and the basis functions are chosen to have compact support19, these calculations form the basis of the finite element method for solving partial differential equations.\nThe most important thing, however, is that if \\(\\mathcal{C}^{-1/2}\\) is a differential operator and the basis functions have compact support, the matrix \\(K\\) is sparse and the matrix \\(C\\) can be made20 diagonal, which means that \\[\nu \\sim N(0, K^{-1} C K^{-T})\n\\] has a sparse precision matrix. This can be used to make inference with these GPs very efficient and is the basis for GPs in the INLA software.\nA natural question to ask is when will we end up with a sparse precision matrix? The answer is not quite when \\(\\mathcal{C}^{-1/2}\\) is a differential operator. Although that will lead to a sparse precision matrix (and a Markov process), it is not required. So the purpose of the rest of this post is to quantify all of the cases where a GP has the Markov property and we can make use of the resulting computational savings."
  },
  {
    "objectID": "posts/2023-01-21-markov/markov.html#the-markov-property-for-on-abstract-spaces",
    "href": "posts/2023-01-21-markov/markov.html#the-markov-property-for-on-abstract-spaces",
    "title": "Markovian Gaussian processes: A lot of theory and some practical stuff",
    "section": "The Markov property for on abstract spaces",
    "text": "The Markov property for on abstract spaces\nPart of the reason why I introduced the notion of a generalised Gaussian process is that it is useful in the definition of the Markov process. Intuitively, we know what this definition is going to be: if I split my space into three disjoint sets \\(A\\), \\(\\Gamma\\) and \\(B\\) in such a way that you can’t get from \\(A\\) to \\(B\\) without passing through \\(\\Gamma\\), then the Markov property should say, roughly, that every random variable \\(\\{x(s): s\\in A\\}\\) is conditionally independent of every random variable \\(\\{x(s): s \\in B\\}\\) given (or conditional on) knowing the values of the entire set \\(\\{x(s): s \\in \\Gamma\\}\\).\n\n\n\nA graphical illustration of the three sets used above Markov property.\n\n\nThat definition is all well and good for a hand-wavey approach, but unfortunately it doesn’t quite hold up to mathematics. In particular, if we try to make \\(\\Gamma\\) a line21, we will hit a few problems. So instead let’s do this properly.\nAll of the material here is covered in Rozanov’s excellent but unimaginatively named book Markov Random Fields.\nTo set us up, we should consider the types of sets we have. There are three main sets that we are going to be using: the open22 set \\(S_1 \\subset T\\), its boundary23 \\(\\Gamma \\supseteq \\partial S\\). For example, if \\(T  = \\mathbb{R}^2\\) and \\(S\\) is the interior of the unit circle, and its open complement \\(S_2 = S_1^C \\backslash \\partial S_1\\). For a 2D example, if \\(S_1\\) is the interior of the unit circle, then \\(\\Gamma\\) could be the unit circle, and \\(S_2\\) would be the exterior of the unit circle.\nOne problem with these sets, is that while \\(S_1\\) will be a 2D set, \\(\\Gamma\\) is only one dimensional (it’s a circle, so it’s a line!). This causes some troubles mathematically, which we need to get around by using the \\(\\epsilon\\) fattening of \\(\\Gamma\\), which is the set \\[\n\\Gamma^\\epsilon = \\{s \\in T : d(s, \\Gamma) &lt; \\epsilon\\},\n\\] where \\(d(s, \\Gamma)\\) is the distance from \\(s\\) to the nearest point in \\(\\Gamma\\).\nWith all of this in hand we can now give a general definition of the Markov property.\n\nDefinition 4 (The Markov property for a generalised Gaussian process) Consider a zero mean generalised GP24 \\(\\xi\\). For any25 subset \\(A \\subset T\\), we define the collection of random variables26 \\[\nH(A) = \\operatorname{span}\\{\\xi(f): \\operatorname{supp}(f) \\subseteq A\\}.\n\\] We will call \\(\\{H(A); A \\subseteq T\\}\\) the random field27 associated with \\(\\xi\\).\nLet \\(\\mathcal{G}\\) be a system of domains28 in \\(T\\). We say that \\(\\xi\\) has the Markov29 property (with respect to \\(\\mathcal{G}\\)) if, for all \\(S_1 \\in \\mathcal{G}\\) and for any sufficiently small \\(\\epsilon &gt; 0\\), \\[\n\\mathbb{E}(xy \\mid H(\\Gamma^\\epsilon)) = 0, \\qquad x \\in H(S_1), y \\in H(S_2),\n\\] where \\(\\Gamma = \\partial S_1\\) and \\(S_2 = S_1^C \\backslash \\Gamma\\).\n\n\nRewriting the Markov property I: Splitting spaces\nThe Markov property defined above is great and everything, but in order to manipulate it, we need to think carefully about the how the domains \\(S_1\\), \\(\\Gamma^\\epsilon\\) and \\(S_2\\) can be used to divide up the space \\(H(T)\\). To do this, we need to basically localise the Markov property to one set of \\(S_1\\), \\(\\Gamma\\), \\(S_2\\). This concept is called a splitting30 of \\(H(S_1)\\) and \\(H(S_2)\\) by \\(H(\\Gamma^\\epsilon)\\).\n\nDefinition 5 For some domain \\(S_1\\) and \\(\\Gamma \\supseteq \\partial S_1\\), set \\(S_2 = (S_1 \\cup \\Gamma)^c\\). The space \\(H(\\Gamma^\\epsilon)\\) splits \\(H(S_1)\\) and \\(H(S_2)\\) if \\[\nH(T) = H(S_1 \\ominus \\Gamma^\\epsilon) \\oplus H(\\Gamma^\\epsilon) \\oplus H(S_2 \\ominus \\Gamma^\\epsilon),\n\\] where \\(\\oplus\\) is the sum of orthogonal components31 and \\(x\\in H(S \\ominus \\Gamma^\\epsilon)\\) if and only if there is some \\(y \\in H(S)\\) such that32 \\[\nx = y - \\mathbb{E}(y \\mid H(\\Gamma^\\epsilon)).\n\\]\n\nThis emphasizes that we can split our space into three separate components: inside \\(S_1\\), outside \\(S_1\\) and on the boundary of \\(S_1\\) and the ability to do that for any33 domain is the key part of the Markov34 property.\nA slightly more convenient way to deal with splitting spaces is the case where the we have overlapping sets \\(A\\), \\(B\\) that cover the domain (ie \\(A \\cup B = T\\)) and the splitting set is their intersection \\(S = A \\cap B\\). In this case, the splitting equation becomes \\[\nH(A)^\\perp \\perp H(B)^\\perp.\n\\] I shan’t lie: that looks wild. But it makes sense when you take \\(A = S_1 \\cup \\Gamma^\\epsilon\\) and \\(B = S_2 \\cup \\Gamma^\\epsilon\\), in which case \\(H(A)^\\perp = H(S_2)\\) and \\(H(B)^\\perp = H(S_1)\\).\nThe final thing to add before we can get to business is a way to get rid of all of the annoying \\(\\epsilon\\)s. The idea is to take the intersection of all of the \\(H(\\Gamma^\\epsilon)\\) as the splitting space. If we define \\[\nH_+(\\Gamma) = \\bigcap_{\\epsilon&gt;0} H(\\Gamma^\\epsilon)\n\\] we can re-write35 the splitting equation as \\[\\begin{align*}\n&H_+(\\Gamma) = H_+(S_1 \\cup \\Gamma) \\cap H_+(S_1 \\cup \\Gamma) \\\\\n& H_+(S_1 \\cup \\Gamma)^\\perp \\perp H_+(S_2 \\cup \\Gamma)^\\perp.\n\\end{align*}\\]\nThis gives the following statement of the Markov property.\n\nDefinition 6 Let \\(\\mathcal{G}\\) be a system of domains36 in \\(T\\). We say that \\(\\xi\\) has the Markov property (with respect to \\(\\mathcal{G}\\)) if, for all \\(S_1 \\in \\mathcal{G}\\), \\(\\Gamma\\supseteq \\partial S_1\\) ,\\(S_2 = S_1^C \\backslash \\Gamma\\), we have, for some \\(\\epsilon &gt; 0\\) \\[\nH_+(\\Gamma^\\epsilon) = H_+(S_1 \\cup \\Gamma^\\epsilon) \\cap H_+(S_1 \\cup \\Gamma^\\epsilon)\n\\] and \\[\nH_+(S_1 \\cup \\Gamma)^\\perp \\perp H_+(S_2 \\cup \\Gamma)^\\perp.\n\\]\n\n\n\nRewriting the Markov property II: The dual random field \\(H^*(A)\\)\nWe are going to fall further down the abstraction rabbit hole in the hope of ending up somewhere useful. In this case, we are going to invent an object that has no reason to exist and we will show that it can be used to compactly restate the Markov property. It will turn out in the next section that it is actually a useful characterization that will lead (finally) to an operational characterisation of a Markovian Gaussian process.\n\nDefinition 7 (Dual random field) Let \\(\\xi\\) be a generalised Gaussian process with an associated random field \\(H(A)\\), \\(A \\subseteq T\\) and let \\(\\mathcal{G}\\) be a complete system of open domains in \\(T\\). The dual to the random field \\(H(A)\\), \\(A \\subseteq T\\) on the system \\(\\mathcal{G}\\) is the random field \\(H^*(A)\\), \\(A \\subseteq T\\) that satisfies \\[\nH^*(T) = H(T)\n\\] and \\[\nH^*(A) = H_+(A^c)^\\perp, \\qquad A \\in \\mathcal{G}.\n\\]\n\nThis definition looks frankly a bit wild, but I promise you, we will use it.\nThe reason for its structure is that it directly relates to the Markov property. In particular, the existence of a dual field implies that, if we have any \\(S_1 \\in \\mathcal{G}\\), then \\[\\begin{align*}\nH_+(S_1 \\cup \\bar{\\Gamma^\\epsilon}) \\cap H_+(S_1 \\cup \\bar{\\Gamma^\\epsilon}) &= H^*((S_1 \\cup \\bar{\\Gamma^\\epsilon})^c)^\\perp \\cap H^*((S_2 \\cup \\bar{\\Gamma^\\epsilon})^c)^\\perp \\\\\nH^*((S_1 \\cup \\bar{\\Gamma^\\epsilon})^c \\cup (S_2 \\cup \\bar{\\Gamma^\\epsilon})^c) \\\\\n&= H_+((S_1 \\cup \\bar{\\Gamma^\\epsilon}) \\cap (S_2 \\cup \\bar{\\Gamma^\\epsilon})) \\\\\n&= H_+(\\Gamma^\\epsilon).\n\\end{align*}\\] That’s the first thing we need to show to demonstrate the Markov property.\nThe second part is much easier. If we note that \\((S_2 \\cup \\Gamma)^c = S_1 \\backslash \\Gamma\\), it follows that \\[\nH_+(S_1 \\cup \\Gamma)^\\perp = H^*(S_2 \\backslash \\Gamma).\n\\]\nThis gives us our third (and final) characterisation of the (second-order) Markov property.\n\nDefinition 8 Let \\(\\mathcal{G}\\) be a system of domains37 in \\(T\\). Assume that the random field \\(H(\\cdot)\\) has an associated dual random field \\(H^*(\\cdot)\\).\nWe say that \\(H(A)\\), \\(A \\in \\mathcal{G}\\) has the Markov property (with respect to38 \\(\\mathcal{G}\\)) if and only if for all \\(S_1 \\in \\mathcal{G}\\), \\[\nH^*(S_1 \\backslash \\Gamma) \\perp H^*(S_2 \\backslash \\Gamma).\n\\] When this holds, we say that the dual field is orthogonal with respect to \\(\\mathcal{G}\\).\n\nThere is probably more to say about dual fields. For instance, the dual of the dual field is the original field. Neat, huh. But really, all we need to do is know that an orthogonal dual field implies a the Markov property. Because next we are going to construct a dual field, which will give us an actually useful characterisation of Markovian GPs.\n\n\nBuilding out our toolset with the conjugate GP\nIn this section, our job is to construct a dual random field. To do this, we are going to exploit the notion of a conjugate39 Gaussian process, which is a generalised40 GP \\(\\xi^*\\) such that41 \\[\n\\mathbb{E}(\\xi(f)\\xi^*(g)) = \\int_T f(s)g(s)\\,ds.\n\\] It is going to turn out that \\(H^*(\\cdot)\\) is the random field generated by \\(\\xi^*\\). The condition that \\(H(T) = H^*(T)\\) can be assumed a fortiori. What we need to show is that the existence of a conjugate Gaussian process implies that, for all \\(S \\subset \\mathcal{G}\\), \\(H^*(S) \\perp H^*( S^C)\\).\nWe will return to the issue of whether or not \\(\\xi^*\\) actually exists later, but assuming it does let’s see how it’s associated random field \\(H*(S)\\) relates to \\(H_+(S^c)^\\perp\\) for \\(S\\in \\mathcal{G}\\). While it is not always true that these things are equal, it is always true that \\[\nH^*(S) \\subseteq H_+(S^c)^\\perp.\n\\] We will consider when equality holds in the next section. But first let’s show the inclusion.\nThe space \\(H^*(S)\\) contains all random variables of the form \\(\\xi^*(u)\\), where the support of \\(u\\) is compact in \\(S\\), which means that it is a positive distance from \\(S^C\\). That means that, for some \\(\\epsilon &gt; 0\\), the support of \\(u\\) is outside42 of \\((S^c)^\\epsilon\\). So if we fix that \\(u\\) and consider any smooth \\(v\\) with support in43 \\((S^c)^\\epsilon\\), then, from the definition of the conjugate GP, we have44 \\[\n\\mathbb{E}(\\xi(v)\\xi^*(u)) = \\int_T u(s) v(s)\\, ds = 0.\n\\] This means that \\(\\xi^*(u)\\) is perpendicularity to \\(\\xi(v)\\) and, therefore, \\(\\xi^*(u) \\in H((S^c)^\\epsilon)^\\perp\\). Now, \\(H_+(S^c)\\) is defined as the intersection of these spaces, but it turns out that45 for any spaces \\(A\\) and \\(B\\), \\[\n(A \\cap B)^\\perp = A^\\perp \\cup B^\\perp.\n\\] This is because \\(A\\cap B \\subset A\\) and so every function that’s orthogonal to functions in \\(A\\) is also orthogonal to functions in \\(A\\cap B\\). The same goes for \\(B\\). We have shown that \\[\nH_+(S^c) = \\bigcup_{\\epsilon &gt; 0} H((S^c)^\\epsilon)^\\perp\n\\] and every \\(\\eta^* \\in H^*(S)\\) is in \\(H((S^c)^\\epsilon)^\\perp\\) for some \\(\\epsilon &gt;0\\). This gives the inclusion \\[\nH^*(S) \\subseteq H_+(S^c)^\\perp.\n\\]\nTo give conditions for when it’s an actual equality is a bit more difficult. It, maybe surprisingly, involves thinking carefully about the reproducing kernel Hilbert space of \\(\\xi\\). We are going to take this journey together in two steps. First we will give a condition on the RKHS that guarantees that \\(\\xi^*\\) exists. Then we will look at when \\(H^*(S) = H_+(S^c)^\\perp\\).\n\n\nWhen does \\(\\xi^*\\) exits? or, A surprising time with the reproducing kernel Hilbert space\nFirst off, though, we need to make sure that \\(\\xi^*\\) exists. Obviously46 if it exists then it is unique and \\(\\xi^{**} = \\xi\\).\nBut does it exist? The answer turns out to be sometimes. But also usually. To show this, we need to do something that is, frankly, just a little bit fancy. We need to deal with the reproducing kernel Hilbert space47. This feels somewhat surprising, but it turns out that it is a fundamental object48 and intrinsically tied to the space \\(H(T)\\).\nThe reproducing kernel space, which we will now49 call \\(V(T)\\) because we are using \\(H\\) for something else in this section, is a set of deterministic generalised functions \\(\\psi\\), that can be evaluated at \\(C_0^\\infty(T)\\) functions50 as \\[\n\\psi(u) = \\int_T u(s)\\,d\\psi(s), \\qquad u \\in C_0^\\infty(T).\n\\] A generalised function \\(\\psi \\in V(T)\\) if there is a corresponding random variable in \\(\\eta \\in H(T)\\) that satisfies \\[\n\\psi(u) = \\mathbb{E}\\left[\\xi(u) \\eta\\right], \\qquad u \\in C_0^\\infty(T).\n\\] It can be shown51 that there is a one-to-one correspondence between \\(H(T)\\) and \\(V(T)\\), in the sense that for every \\(\\psi\\) there is a unique \\(\\eta = \\eta(\\psi) \\in H(T)\\).\nWe can use this correspondence to endow \\(V(T)\\) with an inner product \\[\n\\langle \\psi_1, \\psi_2\\rangle_{V(T)} = \\mathbb{E}(\\eta(\\psi_1), \\eta(\\psi_2)).\n\\]\nSo far, so abstract. The point of the conjugate GP is that it gives us an explicit construction of the52 mapping \\(\\eta\\). And, importantly for the discussion of existence, if there is a conjugate GP then the RKHS has a particular relationship with \\(C_0^\\infty(T)\\).\nTo see this, let’s assume \\(\\xi^*\\) exists. Then, for each \\(v \\in C_0^\\infty(T)\\), the generalised function \\[\n\\psi_v(u) = \\int_T u(s) v(s)\\,ds\n\\] is in \\(V(T)\\) because, by the definition of \\(\\xi^*\\) we have that \\[\n\\phi_v(u) = \\mathbb{E}(\\xi(u)\\xi^*(v)) = \\int_T u(s) v(s)\\,ds.\n\\] Hence, the embedding is given by \\(\\eta(v) = \\xi^*(v)\\).\nNow, if we do a bit of mathematical trickery and equate things that are isomorphic, \\(C_0^\\infty(T) \\subseteq V(T)\\). On its face, that doesn’t make much sense because on the left we have a space of actual functions and on the right we have a space of generalised functions. To make it work, we associate each smooth function \\(v\\) with the generalised function \\(\\psi_v\\) defined above.\nThis make \\(V(T)\\) the closure53 of \\(C_0^\\infty(T)\\) under the norm \\[\n\\|v\\|^2_{V(T)} = \\mathbb{E}\\left(\\xi^*(v)^2\\right).\n\\] and hence we have showed that if there is a conjugate GP, then \\[\nC_0^\\infty(T) \\subseteq V(T), \\qquad \\overline{C_0^\\infty(T)} = V(T).\n\\] It turns out that if \\(C_0^\\infty(T)\\) is dense in \\(V(T)\\) then that implies that there exists a conjugate function defined through the isomorphism \\(\\eta(\\cdot)\\). This is because \\(H(T) = \\eta(V(T))\\) and \\(\\eta\\) is continuous. Hence if we choose \\(\\xi^*(v) = \\eta(v)\\) then \\(H^*(T) = H(T)\\).\nWe have shown the following.\n\nTheorem 1 A conjugate GP exists if and only if \\(C_0^\\infty(T)\\) is dense in \\(V(T)\\).\n\nThis is our first step towards making statements about the stochastic process \\(\\xi\\) into statements about the RKHS. We shall continue along this road.\nYou might, at this point, be wondering if that condition ever actually holds. The answer is yes. It does fairly often. For instance, if \\(\\xi\\) is a stationary GP with spectral density \\(f(\\omega)\\), the biorthogonal function exists if and only if there is some \\(k&gt;0\\) such that \\[\n\\int (1 + |\\omega|^2)^{-k}f(\\omega)^{-1}\\,d\\omega &lt; \\infty.\n\\] This basically says that the theory we are developing doesn’t work for GPs with extremely smooth sample paths (like a GP with the square-exponential covariance function). This is not a restriction that bothers me at all.\nFor non-stationary GPs that aren’t too smooth, this will also hold as long as nothing too bizarre is happening at infinity.\n\n\nBut when does \\(H^*(S) = H_+(S^c)^\\perp\\)?\nWe have shown already54 that \\[\nH((S^c)^\\epsilon)^\\perp = \\left\\{\\xi^*(u): u \\in V(T),\\, \\operatorname{supp}(u) \\subseteq [(S^c)^\\epsilon]^c\\right\\}\n\\] (that last bit with all the complements can be read as “the support of \\(u\\) is inside \\(S\\) and always more than \\(\\epsilon\\) from the boundary.”). It follows then that \\[\nH_+(S^c)^\\perp = \\bigcup_{\\epsilon&gt;0}\\left\\{\\xi^*(u):  u \\in V(T),\\, \\operatorname{supp}(u) \\subseteq [(S^c)^\\epsilon]^c\\right\\}.\n\\] This is nice because it shows that \\(H_+(S^c)^\\perp\\) is related to the space \\[\nV(S) = \\bigcup_{\\epsilon&gt;0}\\left\\{  u \\in V(T),\\, \\operatorname{supp}(u) \\subseteq [(S^c)^\\epsilon]^c\\right\\},\n\\] that is if \\(v\\in V(T)\\) is a function that is the limit of a sequence of functions \\(v_n \\in V(T)\\) with \\(\\operatorname{supp}(v_n) = [(S^c)^\\epsilon]^c\\) for some \\(\\epsilon&gt;0\\), then \\(\\xi^*(v) \\in H_+(S^c)^\\perp\\) and every such random variable has an associated \\(v\\).\nSo, in the sense55 of isomorphisms these are equivalent, that is \\[\nH_+(S^c)^\\perp \\cong V(S).\n\\]\nThis means that if we can show that \\(H^*(S) \\cong V(S)\\), then we have two spaces that are isomorphic to the same space and use the same isomorphism \\(\\xi^*\\). This would mean that the spaces are equivalent.\nThis can also be placed in the language of function spaces. Recall that \\[\nH^*(S) = \\overline\\{\\xi(u): u \\in C_0^\\infty(S)\\}.\n\\] Hence \\(H^*(S)\\) will be isomorphic to \\(V(S)\\) if and only if \\[\nV(S) = \\overline{C_0^\\infty(S)},\n\\] that is, if and only if every \\(v \\in V(S)\\) is the limit of a sequence of smooth functions compactly supported within \\(S\\).\nThis turns out to not always be true, but it’s true in the situations that we most care about. In particular, we get the following theorem, which I am certainly not going to prove.\n\nAssume that the conjugate GP \\(\\xi^*\\) exists. Assume that either of the following holds:\n\nMultiplication by a function \\(w \\in C_0^\\infty\\) is bounded in \\(V(T)\\), ie \\[\n\\|wu \\|_{V(T)} \\leq C(w) \\|u\\|_{V(T)}, \\qquad u \\in C_0^\\infty (T).\n\\]\nThe shift operator is bounded under both the RKHS norm and the covariance56 norm for small \\(s_0\\), ie \\[\n\\|u(\\cdot - s_0)\\| \\leq C \\|u\\|, \\qquad u \\in C_0^\\infty(T)\n\\] holds in both norms for all \\(s_0 \\leq s_\\max\\), \\(s_\\max &gt;0\\) sufficiently small.\n\nThen \\(H^*(\\cdot)\\) is the dual of \\(H(\\cdot)\\) over the system of sets that are bounded or have bounded complements in \\(T\\).\n\nThe second condition is particularly important because it always holds for stationary GPs with \\(C=1\\) as their covariance structure is shift invariant. It’s not impossible to come up with examples of generalised GPs that don’t satisfy this condition, but they’re all a bit weird (eg the “derivative” of white noise). So as long as your GP is not too weird, you should be fine.\n\n\nAt long last, an RKHS characterisation of the Markov property\nAnd with that, we are finally here! We have that \\(H^*(S)\\) is the dual random field to \\(H(S)\\), \\(S\\in G\\) and we have a lovely characterisation of \\(H^*(S)\\) in terms of the RKHS \\(V(S)\\). We can combine this with our definition of a Markov property for GPs with a dual random field and get that a GP \\(\\xi\\) is Markovian if and only if \\[\nH^*(S_1 \\backslash \\Gamma) \\perp H^*(S_2 \\backslash \\Gamma).\n\\] We can use the isomorphism to say that if \\(\\eta_j \\in H^*(S_j \\backslash \\Gamma)\\), \\(j=1,2\\), then there is a \\(v_j \\in V(S_j \\backslash \\Gamma)\\) such that \\[\n\\eta_j = \\xi^*(v_j).\n\\] Moreover, this isomorphism is unitary (aka it preserves the inner product) and so \\[\n\\mathbb{E}(\\eta_1 \\eta_2) = \\langle v_1, v_2\\rangle_{V(T)}.\n\\] Hence, \\(\\xi\\) has the Markov property if and only if \\[\n\\langle v_1, v_2\\rangle_{V(T)} = 0, \\qquad v_j \\in V(S_j \\backslash \\Gamma),\\,S_1 \\in \\mathcal{G},\\, S_2 = S_1^c,\\, j=1,2.\n\\]\nLet’s memorialise this as a theorem.\n\nTheorem 2 A GP \\(\\xi\\) with a conjugate GP \\(\\xi^*\\) is Markov if and only if its RKHS is local, ie if \\(v_1\\) and \\(v_2\\) have disjoint supports, then \\[\n\\langle v_1, v_2\\rangle_{V(T)} = 0.\n\\]\n\nThis result is particularly nice because it entirely characterises the RHKS inner product of a Markovian GP. The reason for this is a deep result from functional analysis called Peetre’s Theorem, which states, in our context, that locality implies that the inner product has the form \\[\n\\langle v_1, v_2\\rangle_{V(T)} = \\sum_{\\mathbf{k}, \\mathbf{j}} \\int_T a_{\\mathbf{k}\\mathbf{j}}(s)\\frac{\\partial^{|\\mathbf{k}|}u}{\\partial s_\\mathbf{k}} \\frac{\\partial^{|\\mathbf{j}|}u}{\\partial s_\\mathbf{j}}\\,ds,\n\\] where57 \\(a_{\\mathbf{k}\\mathbf{j}}(s)\\) are integrable functions and only a finite number of them are non-zero at any point \\(s\\).\nThis connection between the RKHS and the dual space also gives the following result for stationary GPs.\n\nTheorem 3 Let \\(\\xi\\) be a stationary Gaussian process. Then \\(\\xi\\) has the Markov property if and only if its spectral density is the inverse of a non-negative, symmetric polynomial.\n\nThis follows from the characterisation of the RKHS as having the inner product as \\[\n\\langle v_1, v_2\\rangle_{V(T)} = \\int_T \\hat{v_1}(\\omega) \\hat{v_2}(\\omega) f(\\omega)^{-1}\\,d\\omega,\n\\] where \\(\\hat{v_1}\\) is the Fourier transform of \\(v_1\\) and the fact that a differential operator can is transformed to a polynomial in Fourier space.\n\n\nPutting this all in terms of \\(\\eta\\)\nWaaaay back near the top of the post I described a way to write a (generalised) GP in terms of its covariance operator and the white noise process \\[\n\\eta = \\mathcal{C}^{1/2}W.\n\\] From the discussions above, it follows that the corresponding conjugate GP is given by \\[\n\\eta^* = C^{-1/2}W.\n\\] This means that the RKHS inner product is given by \\[\\begin{align*}\n\\langle v_1, v_2 \\rangle_{V(T)} = \\mathbb{E}(\\eta^*(v_1)\\eta^*(v_2))\\\\\n&= \\mathbb{E}\\left[\\int_T \\mathcal{C}^{-1/2}v_1(s)\\,dW(s)\\int_T \\mathcal{C}^{-1/2}v_2(s)\\,dW(s)\\right] \\\\\n&= \\int_T v_1(s)\\mathcal{C}^{-1}v_2(s)\\,ds\n\\end{align*}\\] From the discussion above, if \\(\\eta\\) is Markovian, then \\(\\mathcal{C}^{-1}\\) is58 a differential59 operator."
  },
  {
    "objectID": "posts/2023-01-21-markov/markov.html#using-the-rkhs-to-build-computationally-efficient-approximations-to-markovian-gps",
    "href": "posts/2023-01-21-markov/markov.html#using-the-rkhs-to-build-computationally-efficient-approximations-to-markovian-gps",
    "title": "Markovian Gaussian processes: A lot of theory and some practical stuff",
    "section": "Using the RKHS to build computationally efficient approximations to Markovian GPs",
    "text": "Using the RKHS to build computationally efficient approximations to Markovian GPs\nTo close out this post, let’s look at how we can use the RKHS to build an approximation to a Markovian GP. This is equivalent60 to the SPDE method that was very briefly sketched above, but it only requires knowledge of the RKHS inner product.\nIn particular, if we have a set of basis functions \\(\\psi_j\\), \\(j=1,\\ldots,n\\), we can define the approximate RKHS \\(V_n(T)\\) as the space of all functions \\[\nf(s) = \\sum_{j=1}^n f_j \\psi_j(s)\n\\] equipped with the inner product \\[\n\\langle f, g \\rangle_{V_n(T)} = f^T Q g,\n\\] where the LHS \\(f\\) and \\(g\\) are functions and on the right they are the vectors of weights, and \\[\nQ_{ij} = \\langle \\psi_i, \\psi_j\\rangle_{V(T)}.\n\\]\nFor a finite dimensional GP, the matrix that defines the RKHS inner product is61 the inverse of the covariance matrix. Hence the finite dimensional GP \\(u^{(n)}(\\cdot)\\) associated with the RKHS \\(V_n(T)\\) is the random function \\[\nu^{(n)}(s) = \\sum_{j = 1}^n u_j \\psi_j(s),\n\\] where the weights \\(u \\sim N(0, Q^{-1})\\).\nIf the GP is Markovian and the basis functions have compact support, then \\(Q\\) is a sparse matrix and maybe he’ll love me again."
  },
  {
    "objectID": "posts/2023-01-21-markov/markov.html#footnotes",
    "href": "posts/2023-01-21-markov/markov.html#footnotes",
    "title": "Markovian Gaussian processes: A lot of theory and some practical stuff",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nor redefined if you’ve read my other post↩︎\nFor other observation models it contains the posterior mode↩︎\nStep 1: Open Rasmussen and Williams.↩︎\nFor example, the process I’m about to describe is not meaningfully different for a process on a sphere. Whereas if you want to use a covariance function on a sphere you are stuck trying to find a whole new class of positive definite functions. It’s frankly very annoying. Although if you want to build a career out of characterising positive definite functions on increasingly exotic spaces, you probably don’t find it annoying.↩︎\nOr the Cholesky factor if you add a bunch of transposes in the right places, but let’s not kid ourselves this is not a practical discussion of how to do it↩︎\nAlbeit a bit advanced. It’s straightforward in the sense that for an infinite-dimensional operator it happens to work a whole like a symmetric positive semi-definite matrix. It is not straightforward in the sense that your three year old could do it. Your three year old can’t do it. But it will keep them quiet in the back seat of the car while you pop into the store for some fags. It’s ok. The window’s down.↩︎\nFor any subset \\(B\\), \\(\\sup_{s\\in B} w(s) = \\infty\\) and \\(\\inf_{s \\in B} w(s) = -\\infty\\)↩︎\nCountably additive set-valued function taking any value in \\(\\mathbb{C}\\)↩︎\nmeasurable↩︎\n\\(A \\cap B = \\emptyset\\)↩︎\nIf \\(W(A)\\) is also Gaussian then this is the same as them being independent↩︎\nRecall that \\(T\\) is our whole space. Usually \\(\\mathbb{R}^d\\), but it doesn’t matter here.↩︎\nA bit of a let down really.↩︎\nlike \\(f(s)\\) but with more subsets↩︎\n\\(L^2(T)\\) is the space of functions with the property that \\(\\int_T f(s)^2\\,ds &lt; \\infty\\).↩︎\neg the Gaussian free field in physics, or the de Wijs process.↩︎\nYou can use a separate set of basis functions here, but I’m focusing on simplicity↩︎\nThe standard example is \\[\n\\mathcal{C}^{-1/2} = \\kappa^2 - \\sum_{j=1}^d \\frac{\\partial^2}{\\partial s_j^2}.\n\\]↩︎\nIn particular piecewise linear tent functions build on a triangulation↩︎\nRead the paper, it’s a further approximation but the error is negligible↩︎\n(\\(d-1\\))-dimensional sub-manifold↩︎\nThis set does not include its boundary↩︎\nThis is defined as the set \\(\\partial S_1 = \\bar{S_1} \\backslash S_1\\), where \\(\\bar{S_1}\\) is the closure of \\(S_1\\). But let’s face it. It’s the fucking boundary. It means what you think it means.↩︎\nI’m using \\(\\xi\\) here as a generic generalised GP, rather than \\(\\eta\\), which is built using an ordinary GP. This doesn’t really make much of a difference (the Markov property for one is the same as the other), but it makes me feel better.↩︎\nmeasurable↩︎\nHere \\(\\operatorname{supp}(f)\\) is the support of \\(f\\), that is the values of \\(s\\) such that \\(f(s) \\neq 0\\).↩︎\nThis is the terminology of Rozanov. Random Field is also another term for stochastic process. Why only let words mean one thing?↩︎\nnon-empty connected open sets↩︎\nStrictly, this is the weak or second-order Markov property↩︎\nIf you’re curious, this is basically the same thing as a splitting \\(\\sigma\\)-algebra. But, you know, sans the \\(\\sigma\\)-algebra bullshit.↩︎\nThat is, any \\(x \\in H(T)\\) can be written as the sum \\(x = x_1 + x_2 + x_3\\), where \\(x_1 \\in  H(S_1 \\ominus \\Gamma^\\epsilon)\\), \\(x_2 \\in H(\\Gamma^\\epsilon)\\), and \\(x_3 \\in H(S_2 \\ominus \\Gamma^\\epsilon)\\) are mutually orthogonal (ie \\(\\mathbb{E}(x_1x_2) = \\mathbb{E}(x_1x_3) = \\mathbb{E}(x_2x_3) =0\\)!).↩︎\nThis is using the idea that the conditional expectation is a projection.↩︎\nTypically any open set, or any open connected set, or any open, bounded set. A subtlety that I don’t really want to dwell on is that it is possible to have a GP that is Markov with respect to one system of domains but not another.↩︎\nThe Markov property can be restated in this language as for every system of complementary domains and boundary \\(S_1\\), \\(\\Gamma\\), \\(S_2\\), there exists a small enough \\(\\epsilon &gt; 0\\) such that \\(\\Gamma^\\epsilon\\) splits \\(S_1\\) and \\(S_2\\)↩︎\nTechnically we are assuming that for small enough \\(\\epsilon\\) \\(H(\\Gamma^\\epsilon) = \\operatorname{span}\\left(H(\\Gamma^\\epsilon \\cap S_1) \\cup H_+(\\Gamma) \\cup H(\\Gamma^\\epsilon \\cap S_2)\\right)\\). This is not a particularly onerous assumption.↩︎\nnon-empty connected open sets↩︎\nnon-empty connected open sets↩︎\nThe result works with some subsystem \\(\\mathcal{G_0}\\). To prove it for \\(\\mathcal{G}\\) it’s enough to prove it for some subset \\(\\mathcal{G}_0\\) that separates points of \\(T\\). This is a wildly technical aside and if it makes no sense to you, that’s very much ok. Frankly I’m impressed you’ve hung in this long.↩︎\nRozanov also calls this the biorthogonal GP. I like conjugate more.↩︎\nUp to this point, it hasn’t been technically necessary for the GP to be generalised. However, here is very much is. It turns out that if realisations of \\(\\xi\\) are almost surely continuous, then realisations of \\(\\xi^*\\) are almost surely generalised functions.↩︎\nI’m writing this as if all of these GPs are real valued, but for full generality, we should be dealing with complex GPs. Just imagine I put complex conjugates in all the correct places. I can’t stop you.↩︎\nThat is, inside \\(S\\) and more than \\(\\epsilon\\) from the boundary↩︎\n\\(v\\) can be non-zero inside \\(S\\) but only if it’s less than \\(\\epsilon\\) away from the boundary.↩︎\nIt’s zero because the two functions are never non-zero at the same time, so their product is zero.↩︎\nHere, and probably in a lot of other places, we are taking the union of spaces to be the span of their sum. Sorry.↩︎\nReally Daniel. Really. (It’s an isomorphism so if you do enough analysis courses this is obvious. If that’s not clear to you, you should just trust me. Trust issues aren’t sexy. Unless you have cum gutters. In which case, I’ll just spray my isomorphisms on them and you can keep scrolling TikTok.)↩︎\nThis example is absolutely why I hate that we’ve settled on RKHS as a name for this object because the thing that we are about to construct does not always have a reproducing kernel property. Cameron-Martin space is less confusing. But hey. Whatever. The RKHS for the rest of this section is not always a Hilbert space with a reproducing kernel. We are just going to have to be ok with that.↩︎\nNothing about this analysis relies on Gaussianity. So this is a general characterisation of a Markov property for any stochastic process with second moments.↩︎\nIn previous blogs, this was denoted \\(H_c(T)\\) and truly it was too confusing when I tried to do it here. And by that point I wasn’t going back and re-naming \\(H(T)\\).↩︎\n\\(C_0^\\infty(T)\\) is the space of all infinitely differentiable compactly supported functions on \\(T\\)↩︎\nThe trick is to notice that the set of all possible \\(\\xi(u)\\) is dense in \\(H(T)\\).↩︎\nunitary↩︎\nthe space containing the limits (in the \\(V(T)\\)-norm) of all sequences in \\(v_n \\in C_0^\\infty(T)\\)↩︎\nIf you take some limits↩︎\nI mean, really. Basically we say that \\(A \\cong B\\) if there is an isomorphism between \\(A\\) and \\(B\\). Could I be more explicit? Yes. Would that make this unreadable? Also yes.↩︎\n\\(\\|u\\|^2 = \\mathbb{E}(\\xi(u)^2)\\).↩︎\n\\(\\mathbf{j} = (j_1, j_2, \\ldots)\\) is a multi-index, which can be interpreted as \\(|\\mathbf{j}| = \\sum_{\\ell\\geq 1 }j_\\ell\\), and \\[\n\\frac{\\partial^{|\\mathbf{j}|}u}{\\partial s_\\mathbf{j}} = \\frac{\\partial^{|\\mathbf{j}|}u}{\\partial^{j_1}s_{1}\\partial^{j_2}s_{2}\\cdots}.\n\\]↩︎\nin every local coordinate system↩︎\nBecause \\(\\mathcal{C}^{-1}\\) defines an inner product, it’s actually a symmetric elliptic differential operator↩︎\nTechnically, you need to choose different basis functions for \\(f\\). In particular, you need to choose \\(f = \\sum_{j=1}^n f_j \\phi_j\\) where \\(\\phi_j = \\mathcal{C}^{-1/2} \\psi_j\\). This is then called a Petrov-Galerkin approximation and truly we don’t need to think about it at all. Also I am completely eliding issues of smoothness in all of this. It maters, but it doesn’t matter too much. So let’s just assume everything exists.↩︎\nIf you don’t believe me you are welcome to read the monster blog post, where it’s an example.↩︎"
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "",
    "text": "This is part three of an ongoing exercise in hubris. Part one is here. Part two is here. The overall aim of this series of posts is to look at how sparse Cholesky factorisations work, how JAX works, and how to marry the two with the ultimate aim of putting a bit of sparse matrix support into PyMC, which should allow for faster inference in linear mixed models, Gaussian spatial models. And hopefully, if anyone ever gets around to putting the Laplace approximation in, all sorts of GLMMs and non-Gaussian models with splines and spatial effects.\nIt’s been a couple of weeks since the last blog, but I’m going to just assume that you are fully on top of all of those details. To that end, let’s jump in."
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#what-is-jax",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#what-is-jax",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "What is JAX?",
    "text": "What is JAX?\nJAX is a minor miracle. It will take python+numpy code and make it cool. It will let you JIT1 compile it! It will let you differentiate it! It will let you batch2. JAX refers to these three operations as transformations.\nBut, as The Mountain Goats tell us God is present in the sweeping gesture, but the devil is in the details. And oh boy are those details going to be really fucking important to us.\nThere are going to be two key things that will make our lives more difficult:\n\nNot every operation can be transformed by every operation. For example, you can’t always JIT or take gradients of a for loop. This means that some things have to be re-written carefully to make sure it’s possible to get the advantages we need.\nJAX arrays are immutable. That means that once a variable is defined it cannot be changed. This means that things like a = a + 1 is not allowed! If you’ve come from an R/Python/C/Fortran world, this is the weirdest thing to deal with.\n\nThere are really excellent reasons for both of these restrictions. And looking into the reasons is fascinating. But not a topic for this blog3\nJAX has some pretty decent4 documentation, a core piece of which outlines some of the sharp edges you will run into. As you read through the documentation, the design choices become clearer.\nSo let’s go and find some sharp edges together!"
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#to-jax-or-not-to-jax",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#to-jax-or-not-to-jax",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "To JAX or not to JAX",
    "text": "To JAX or not to JAX\nBut first, we need to ask ourselves which functions do we need to JAX?\nIn the context of our problem we, so far, have three functions:\n\n_symbolic_factor_csc(A_indices, A_indptr), which finds the non-zero indices of the sparse Cholesky factor and return them in CSC format,\n_deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr), which takes the entries of the matrix \\(A\\) and re-creates them so they can be indexed within the larger pattern of non-zero elements of \\(L\\),\n_sparse_cholesky_csc_impl(L_indices, L_indptr, L_x), which actually does the sparse Cholesky factorisation.\n\nLet’s take them piece by piece, which is also a good opportunity to remind everyone what the code looked like."
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#symbolic-factorisation",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#symbolic-factorisation",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "Symbolic factorisation",
    "text": "Symbolic factorisation\n\ndef _symbolic_factor_csc(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] &gt; j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) &gt; 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\nThis function only needs to be computed once per non-zero pattern. In the applications I outlined in the first post, this non-zero pattern is fixed. This means that you only need to run this function once per analysis (unlike the others, that you will have to run once per iteration!).\nAs a general rule, if you only do something once, it isn’t all that necessary to devote too much time into optimising it. There are, however, some obvious things we could do.\nIt is, for instance, pretty easy to see how you would implement this with an explicit tree5 structure instead of constantly np.appending the children array. This is far better from a memory standpoint.\nIt’s also easy to imagine this as a two-pass algorithm, where you build the tree and count the number of non-zero elements in the first pass and then build and populate L_indices in the second pass.\nThe thing is, neither of these things fixes the core problem for using JAX to JIT this: the dimensions of the internal arrays depend on the values of the inputs. This is not possible.\nIt seems like this would be a huge limitation, but in reality it isn’t. Most functions aren’t like this one! And, if we remember that JAX is a domain language focussing mainly on ML applications, this is very rarely the case. It is always good to remember context!\nSo what are our options? We have two.\n\nLeave it in Python and just eat the speed.\nBuild a new JAX primitive and write the XLA compilation rule6.\n\nToday are opting for the first option!"
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#the-structure-changing-copy",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#the-structure-changing-copy",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "The structure-changing copy",
    "text": "The structure-changing copy\n\ndef _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\nThis is, fundamentally, a piece of bookkeeping. An annoyance of sparse matrices. Or, if you will, explicit cast between different sparse matrix types7. This is a thing that we do actually need to be able to differentiate, so it needs to live in JAX.\nSo where are the potential problems? Let’s go line by line.\n\nn = len(A_indptr) - 1: This is lovely. n is used in a for loop later, but because it is a function of the shape of A_indptr, it is considered static and we will be able to JIT over it!\nL_x = np.zeros(len(L_indices)): Again, this is fine. Sizes are derived from shapes, life is peachy.\nfor j in range(0, n):: This could be a problem if n was an argument or derived from values of the arguments, but it’s derived from a shape so it is static. Praise be! Well, actually it’s a bit more involved than that.\n\nThe problem with the for loop is what will happen when it is JIT’d. Essentially, the loop will be statically unrolled8. That is fine for small loops, but it’s a bit of a pain in the arse when n is large.\nIn this case, we might want to use the structured control flow in jax.lax9 In this case we would need jax.lax.fori_loop(start, end, body_fun, init_value). This makes the code look less pythonic, but probably should make it faster. It is also, and I cannot stress this enough, an absolute dick to use.\n(In actuality, we will see that we do not need this particular corner of the language here!)\n\ncopy_idx = np.nonzero(...): This looks like it’s going to be complicated, but actually it is a perfectly reasonable composition of numpy functions. Hence, we can use the same jax.numpy functions with minimal changes. The one change that we are going to need to make in order to end up with a JIT-able and differentiable function is that we need to tell JAX how many non-zero elements there are. Thankfully, we know this! Because the non-zero pattern of \\(A\\) is a subset of the non-zero pattern of \\(L\\), we know that\n\n\nnp.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]], A_indices[A_indptr[j]:A_indptr[j+1]])\n\nwill have exactly len(A_indices[A_indptr[j]:A_indptr[j+1]]) True values, and so np.nonzero(...) will have that many. We can pass this information to jnp.nonzero() using the optional size argument.\nOh no! We have a problem! This return size is a function of the values of A_indptr rather than a function of the shape. This means we’re a bit fucked.\nThere are two routes out:\n\nDeclare A_indptr to be a static parameter, or\nChange the representation from CSC to something more convenient.\n\nIn this case we could do either of these things, but I’m going to opt for the second option, as it’s going to be more useful going forward.\nBut before we do that, let’s look at the final line in the code.\n\nL_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]: The final non-trivial line of the code is also a problem. The issue is that these arrays are immutable and we are asking to change the values! That is not allowed!\n\nThe solution here is to use a clunkier syntax. In JAX, we need to replace\n\nx[ind] = a\n\nwith the less pleasant\n\nx = x.at[ind].set(a)\n\nWhat is going on under the hood to make the second option ok while the first is an error is well beyond the scope of this little post. But the important thing is that they compile down to an in-place10 update, which is all we really care about."
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#re-doing-the-data-structure.",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#re-doing-the-data-structure.",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "Re-doing the data structure.",
    "text": "Re-doing the data structure.\nOk. So we need a new data structure. That’s annoying. The rule, I guess, is always that if you need to innovate, you should innovate very little if you can get away with it, or a lot if you have to.\nWe are going to innovate only the tiniest of bits.\nThe idea is to keep the core structure of the CSC data structure, but to replace the indptr array with explicitly storing the row indices and row values as a list of np.arrays. So A_index will now be a list of n arrays that contain the row indices of the non-zero elements of \\(A\\), while A_xwill now be a list of n arrays that contain the values of the non-zero elements of \\(A\\).\nThis means that the matrix \\[\nB = \\begin{pmatrix}\n1 &&5 \\\\\n2&3& \\\\\n&4&6\n\\end{pmatrix}\n\\] would be stored as\n\nB_index = [np.array([0,1]), np.array([1,2]), np.array([0,2])]\nB_x = [np.array([1,2]), np.array([3,4]), np.array([5,6])]\n\nThis is a considerably more pythonic11 version of CSC. So I guess that’s an advantage.\nWe can easily go from CSC storage to this modified storage.\n\ndef to_pythonic_csc(indices, indptr, x):\n  index = np.split(indices, indptr[1:-1])\n  x = np.split(x, indptr[1,-1])\n  return index, x"
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#a-jax-tracable-structure-changing-copy",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#a-jax-tracable-structure-changing-copy",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "A JAX-tracable structure-changing copy",
    "text": "A JAX-tracable structure-changing copy\nSo now it’s time to come back to that damn for loop. As flagged earlier, for loops can be a bit picky in JAX. If we use them as is, then the code that is generated and then compiled is unrolled. You can think of this as if the JIT compiler automatically writes a C++ program and then compiles it. If you were to examine that code, the for loop would be replaced by n almost identical blocks of code with only the index j changing between them. This leads to a potentially very large program to compile12 and it limits the compiler’s ability to do clever things to make the compiled code run faster13.\nThe lax.fori_loop() function, on the other hand, compiles down to the equivalent of a single operation14. This lets the compiler be super clever.\nBut we don’t actually need this here. Because if you take a look at the original for loop we are just applying the same two lines of code to each triple of lists in A_index, A_x, and L_index (in our new15 data structure).\nThis just screams out for a map applying a single function independently to each column.\nThe challenge is to find the right map function. An obvious hope would be jax.vmap. Sadly, jax.vmap does not do that. (At least not without more padding16 than a drag queen.) The problem here is a misunderstanding of what different parts of JAX are for. Functions like jax.vmap are made for applying the same function to arrays of the same size. This makes sense in their context. (JAX is, after all, made for machine learning and these shape assumptions fit really well in that paradigm. They just don’t fit here.)\nAnd I won’t lie. After this point I went wild. lax.map did not help. And I honest to god tried lax.scan, which is will solve the problem but at what cost?.\nBut at some point, you read enough of the docs to find the answer.\nThe correct answer here is to use the JAX concept of a pytree. Pytrees are essentially17 lists of arrays. They’re very flexible and they have a jax.tree_map function that lets you map over them! We are saved!\n\nimport numpy as np\nfrom jax import numpy as jnp\nfrom jax import tree_map\n\ndef _structured_copy_csc(A_index, A_x, L_index):\n    def body_fun(A_rows, A_vals, L_rows):\n      out = jnp.zeros(len(L_rows))\n      copy_idx =  jnp.nonzero(jnp.in1d(L_rows, A_rows), size = len(A_rows))[0] \n      out = out.at[copy_idx].set(A_vals)\n      return out\n    L_x = tree_map(body_fun, A_index, A_x, L_index)\n    return L_x\n\n\nTesting it out\nOk so now lets see if it works. To do that I’m going to define a very simple function \\[\nf(A, \\alpha, \\beta) = \\|\\alpha I + \\beta \\operatorname{tril}(A)\\|_F^2,\n\\] that is the sum of the squares of all of the elements of \\(\\alpha I + \\beta \\operatorname{tril}(A)\\). There’s obviously an easy way to do this, but I’m going to do it in a way that uses the function we just built.\n\ndef test_func(A_index, A_x, params):\n  I_index = [jnp.array([j]) for j in range(len(A_index))]\n  I_x = [jnp.array([params[0]]) for j in range(len(A_index))]\n  I_x2 = _structured_copy_csc(I_index, I_x, A_index)\n  return jnp.sum((jnp.concatenate(I_x2) + params[1] * jnp.concatenate(A_x))**2)\n\nNext, we need a test case. Once again, we will use the 2D Laplacian on a regular \\(n \\times n\\) grid (up to a scaling). This is a nice little function because it’s easy to make test problems of different sizes.\n\nfrom scipy import sparse\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A_lower = sparse.tril(sparse.kronsum(one_d, one_d) + sparse.eye(n*n), format = \"csc\")\n    A_index = jnp.split(jnp.array(A_lower.indices), A_lower.indptr[1:-1])\n    A_x = jnp.split(jnp.array(A_lower.data), A_lower.indptr[1:-1])\n    return (A_index, A_x)\n\nWith our test case in hand, we can check to see if JAX will differentiate for us!\n\nfrom jax import grad, jit\nfrom jax.test_util import check_grads\n\ngrad_func = grad(test_func, argnums = 2)\n\nA_index, A_x = make_matrix(50)\nprint(f\"The value at (2.0, 2.0) is {test_func(A_index, A_x, (2.0, 2.0))}.\")\nprint(f\"The gradient is {np.array(grad_func(A_index, A_x, (2.0, 2.0)))}.\")\n\nThe value at (2.0, 2.0) is 379600.0.\n\n\nThe gradient is [ 60000. 319600.].\n\n\nFabulous! That works!"
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#but-what-about-jit",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#but-what-about-jit",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "But what about JIT?",
    "text": "But what about JIT?\nJIT took fucking ages. I’m talking “it threw a message” amounts of time. I’m not even going to pretend that I understand why. But I can hazard a guess.\nMy running assumption, taken from the docs, is that as long as the function only relies of quantities that are derived from the shapes of the inputs (and not the values), then JAX will be able to trace through and JIT through the functions with ease.\nThis might not be true for tree_maps. The docs are, as far as I can tell, silent on this matter. And a cursory look through the github repo did not give me any hints as to how tree_map() is translated.\nLet’s take a look to see if this is true.\n\nimport timeit\nfrom functools import partial\njit_test_func = jit(test_func)\n\nA_index, A_x = make_matrix(5)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x, (2.0, 2.0)), number = 1)\nprint(f\"n = 5: {[round(t, 4) for t in times]}\")\n\nn = 5: [1.6695, 0.0001, 0.0, 0.0, 0.0]\n\n\nWe can see that the first run includes compilation time, but after that it runs a bunch faster. This is how a JIT system is supposed to work! But the question is: will it recompile when we run it for a different matrix?\n\n_ = jit_test_func(A_index, A_x, (2.0, 2.0)) \nA_index, A_x = make_matrix(20)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x, (2.0, 2.0)), number = 1)\nprint(f\"n = 20: {[round(t, 4) for t in times]}\")\n\nn = 20: [38.5779, 0.0006, 0.0003, 0.0003, 0.0003]\n\n\nDamn. It recompiles. But, as we will see, it does not recompile if we only change A_x.\n\n# What if we change A_x only\n_ = jit_test_func(A_index, A_x, (2.0, 2.0)) \nA_x2 = tree_map(lambda x: x + 1.0, A_x)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x2, (2.0, 2.0)), number = 1)\nprint(f\"n = 20, new A_x: {[round(t, 4) for t in times]}\")\n\nn = 20, new A_x: [0.0006, 0.0007, 0.0005, 0.0003, 0.0003]\n\n\nThis gives us some hope! This is because the structure of A (aka A_index) is fixed in our application, but the values A_x changes. So as long as the initial JIT compilation is reasonable, we should be ok.\nUnfortunately, there is something bad happening with the compilation. For \\(n=10\\), it takes (on my machine) about 2 seconds for the initial compilation. For \\(n=20\\), that increases to 16 seconds. Once \\(n = 30\\), this balloons up to 51 seconds. Once we reach the lofty peaks18 of \\(n=40\\), we are up at 149 seconds to compile.\nThis is not good. The function we are JIT-ing is very simple: just one tree_map. I do not know enough19 about the internals of JAX, so I don’t want to speculate too wildly. But it seems like it might be unrolling the tree_map before compilation, which is … bad."
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#lets-admit-failure",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#lets-admit-failure",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "Let’s admit failure",
    "text": "Let’s admit failure\nOk. So that didn’t bloody work. I’m not going to make such broad statements as you can’t use the JAX library in python to write a transformable sparse Cholesky factorisation, but I am more than prepared to say that I cannot do such a thing.\nBut, if I’m totally honest, I’m not enormously surprised. Even in looking at the very simple operation we focussed on today, it’s pretty clear that the operations required to work on a sparse matrix don’t look an awful lot like the types of operations you need to do the types of machine learning work that is JAX’s raison d’être.\nAnd it is never surprising to find that a library designed to do a fundamentally different thing does not easily adapt to whatever random task I decide to throw at it.\nBut there is a light: JAX is an extensible language. We can build a new JAX primitive (or, new JAX primitives) and manually write all of the transformations (batching, JIT, and autodiffing).\nAnd that is what we shall do next! It’s gonna be a blast!"
  },
  {
    "objectID": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#footnotes",
    "href": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey.html#footnotes",
    "title": "Sparse Matrices 3: Failing at JAX",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’ve never come across this term before, you can Google it for actual details, but the squishy version is that it will compile your code so it runs fast (like C code) instead of slow (like python code). JIT stands for just in time, which means that the code is compiled when it’s needed rather than before everything else is run. It’s a good thing. It makes the machine go bing faster.↩︎\nI give less of a shit about the third transformation in this context. I’m not completely sure what you would batch when you’re dealing with a linear mixed-ish model. But hey. Why not.↩︎\nIf you’ve ever spoken to a Scala advocate (or any other pure functional language), you can probably see the edges of why the arrays need to be immutable.\\[\n\\phantom{a}\n\\] Restrictions to JIT-able control flow has to do with how it’s translated onto the XLA compiler, which involves tracing through the code with an abstract data type with the same shape as the one that it’s being called with. Because this abstract data type does not have any values, structural parts of the code that require knowledge of specific values of the arguments will be lost. You can get around this partially by declaring those important values to be static, which would make the JIT compiler re-compile the function each time that value changes. We are not going to do that. \\[\n\\phantom{a}\n\\] Restrictions to gradients have to do (I assume) with reverse-mode autodiff needing to construct the autodiff tree at compile time, which means you need to be able to compute the number of operations from the types and shapes of the input variables and not from their values.↩︎\nCoverage is pretty good on the using bit, but, as is usual, the bits on extending the system are occasionally a bit … sparse. (What in the hairy Christ is a transposition rule actually supposed to do????)↩︎\nForest↩︎\naka implement the damn thing in C++ and then do some proper work on it.↩︎\nIt is useful to think of a sparse matrix type as the triple (value_type, indices, indptr). This means that if we are going to do something like add sparse matrices, we need to first cast them both to have the same type. After the cast, addition of two different sparse matrices becomes the addition of their x attributes. The same holds for scalar multiplication. Sparse matrix-matrix multiplication is a bit different because you once again need to symbolically work out the sparsity structure (aka the type) of the product. ↩︎\nI think. That’s certainly what’s implied by the docs, but I don’t want to give the impression that I’m sure. Because this is complicated.↩︎\nWhat is jax.lax? Oh honey you don’t want to know.↩︎\naka there’s no weird copying↩︎\nWhatever that means anyway↩︎\nslowwwwww to compile↩︎\nThe XLA compiler does very clever things. Incidentally, loop unrolling is actually one of the optimisations that compilers have in their pocket. Just not one that’s usually used for loops as large as this.↩︎\nRead about XLA High Level Operations (HLOs) here. The XLA documentation is not extensive, but there’s still a lot to read.↩︎\nThis is why we have a new data structure.↩︎\nMy kingdom for a ragged array.↩︎\nYes. They are more complicated than this. But for our purposes they are lists of arrays.↩︎\n\\(n=50\\) takes so long it prints a message telling us what to do if we need to do if we want to file a bug! Compilation eventually clocks in at 361 seconds.↩︎\naka I know sweet bugger all↩︎"
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html",
    "href": "posts/2022-08-29-priors4/priors4.html",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "",
    "text": "At some point in the distant past, I wrote three posts about prior distributions. The first was very basic, because why not. The second one talked about conjugate priors. The third one talked about so-called objective priors.\nI am suddenly1 of a mood to write some more on this2 topic.\nThe thing is, so far I’ve only really talked about methods for setting prior distributions that I don’t particularly care for. Fuck that. Let’s talk about things I like. There is enough negative energy3 in the world.\nSo let’s talk about priors. But the good stuff. The aim is to give my answer to the question “how should you set a prior distribution?”."
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html#bro-do-you-even-know-what-a-parameter-is",
    "href": "posts/2022-08-29-priors4/priors4.html#bro-do-you-even-know-what-a-parameter-is",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "Bro do you even know what a parameter is?",
    "text": "Bro do you even know what a parameter is?\nYou don’t. No one does. They’re not real.\nParameters are polite fictions that we use to get through the day. They’re our weapons of mass destruction. They’re the magazines we only bought for the articles. They are our girlfriends who live in Canada4.\nOne way we can see this is to ask ourselves a simple5: \\[\ny_i \\sim \\text{Negative-Binomial}(\\mu, \\alpha), \\qquad i = 1,\\ldots, n\\text{?}\n\\]\nThe answer6 7 would be two.\nBut let me ask a different question. How many parameters are there in this model8 \\[\\begin{align*}\ny_i\\mid u_i &\\sim \\text{Poisson}(\\mu u_i) \\\\\nu_i &\\sim \\text{Gamma}(\\alpha^{-1}, \\alpha^{-1}),\\qquad i=1,\\ldots, n\\text{?}\n\\end{align*}\\]\nOne answer to this question would be \\(n+2\\). In this interpretation of the question everything in the model that isn’t directly observed is a parameter.\nBut there is another view.\nMathematically, these two models are equivalent. That is, if you marginalise9 out the \\(u_i\\) you get \\[\\begin{align*}\n\\Pr(y=k) &=\\frac{\\mu^k\\alpha^{-1/\\alpha}}{\\Gamma(\\alpha^{-1})\\Gamma(k+1)} \\int_0^\\infty u^k e^{-\\mu u} u^{1/\\alpha-1}e^{-u/\\alpha}\\,du \\\\\n&= \\frac{\\mu^k\\alpha^{-1/\\alpha}}{\\Gamma(\\alpha^{-1})\\Gamma(k+1)}\\int_0^\\infty u^{k + 1/\\alpha-1}e^{-(\\mu + \\alpha^{-1})u}\\,du \\\\\n&= \\frac{\\mu^k\\alpha^{-1/\\alpha}}{\\Gamma(\\alpha^{-1})\\Gamma(k+1)}\\int_0^\\infty \\left(\\frac{t}{\\mu+\\alpha^{-1}}\\right)^{k + 1/\\alpha-1}e^{-t}\\frac{1}{\\mu + \\alpha^{-1}}\\,dt \\\\\n&=\\frac{\\Gamma(k + \\alpha^{-1})}{\\Gamma(\\alpha^{-1})\\Gamma(k+1)} \\left(\\frac{\\mu}{\\mu + \\alpha^{-1}}\\right)^k \\left(\\frac{\\alpha^{-1}}{\\mu + \\alpha^{-1}}\\right)^{1/\\alpha} .\n\\end{align*}\\] This is exactly the negative binomial distribution with mean \\(\\mu\\) and variance \\(\\mu(1 + \\alpha \\mu)\\).\nSo maybe there are two parameters.\nDoes it make a difference? Sometimes. For instance, if you were following ordinary practice in Bayesian machine learning, you would (approximately) marginalise out \\((\\mu, \\lambda)\\) in the first model, but in the second model you’d probably treat them as tuning hyper-parameters10 in the second and optimise11 over them.\nMoreover, in the second model we can ask what other priors could we put on the \\(u_i\\)?. There is no equivalent question for the first model. This could be useful, for instance, if we believe that the overdispersion may differ among population groups. It is considerably easier to extend the random effects formulation into a multilevel model.\nOk. So it doesn’t really matter too much. It really depends on what you’re going to do with the model when you’re breaking your model into things that we need to set priors for and things where the priors are a structural part of the model."
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html#a-hello-boys-into-a-party-date-on-flexibility",
    "href": "posts/2022-08-29-priors4/priors4.html#a-hello-boys-into-a-party-date-on-flexibility",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "A hello boys into a party date: on flexibility",
    "text": "A hello boys into a party date: on flexibility\nThere are a lot of ways to set prior distributions. I’ve covered some in previous posts and there are certainly more. But today I’m going to focus on one constructive method that I’m particular fond of: penalised complexity priors.\nThese priors fall out from a certain way of seeing parameters. The idea is that some parameters in a model function as flexibility parameters. These naturally have a base value, which corresponds to the simplest model that they index. I’ll refer to the distribution you get when the parameter takes its base value as the base model.\n\nExample 1 (Overdispersion of a negative binomial) The negative binomial distribution has two parameters: a mean \\(\\mu\\) and an overdispersion parameter \\(\\alpha\\) so the variance is \\(\\mu(1 + \\alpha \\mu)\\). The mean parameter is not a flexibility parameter. Conceptually, changing the mean12 does not make a distribution more or less complex, it simply shuttles it around.\nOn the other hand, the overdispersion parameter \\(\\alpha\\) is a flexibility parameter. It’s special value is \\(\\alpha =0\\), which corresponds to a Poisson distribution, which is the base model for the negative binomial distribution.\n\n\nExample 2 (Student-t degrees of freedom) The three parameter student-t distribution has density (parameterised by its standard deviation assuming \\(\\nu &gt; 2\\)!) \\[\np(y \\mid \\mu, \\sigma, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sigma\\nu \\sqrt{\\frac{\\pi}{\\nu-2}} \\Gamma\\left(\\frac{\\nu}{2}\\right)}\\left(1 + \\frac{\\frac{\\nu-2}{\\nu}\\left(\\frac{y - \\mu}{\\sigma}\\right)^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}, \\qquad \\nu &gt; 2.\n\\] This has mean \\(\\mu\\) and variance \\(\\sigma^2\\). The slightly strange parameterisation and the restriction to \\(\\nu&gt;0\\) is useful because it lets us specify a prior on the variance itself and not some parameter that is the variance divided by some function13 of \\(\\nu\\).\nThe natural base model here is \\(N(\\mu, \\sigma^2)\\), which corresponds to \\(\\nu = \\infty\\).\n\n\nExample 3 (Variance of a Gaussian random effect) A Gaussian distribution has two parameters: a mean \\(\\mu\\) and a standard deviation \\(\\tau\\). Once again, \\(\\mu\\) is not a flexibility parameter, but in some circumstances \\(\\tau\\) can be.\nTo see this, imagine that we have a simple random intercept model \\[\\begin{align*}\ny_{ij} \\mid u_j &\\sim N(u_j, \\sigma^2),\\qquad i=1,\\ldots,n, j =1,\\ldots,J \\\\\nu_j &\\sim N(\\mu, \\tau).\n\\end{align*}\\] In this case, we don’t really view \\(\\sigma\\) as a flexibility parameter, but \\(\\tau\\) is. Why the distinction? Well let’s think about what happens at special value \\(0\\).\nWhen \\(\\sigma = 0\\) we are saying that there is no variability in the data if we know the corresponding \\(u_i\\). This is, frankly, quite weird and it’s not necessarily a base model we would believe14 in.\nOn the other hand, if \\(\\tau =0\\), then we are say that all of the groups have the same mean. This is a useful and interesting base model that could absolutely happen in most data. So we say that while \\(\\sigma\\) isn’t necessarily a flexibility parameter in the model, \\(\\tau\\) definitely is.\nIn this case the base model is the degenerate distribution15 where the mean of each group is equal to \\(\\mu\\).\n\nThe second example shows that the idea of a flexibility parameter is deeply contextual. Once again, we run into the idea that Statistical Arianism16 is bad. Parameters and their prior distributions can only be fully understood if you know their context within the entire model."
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html#sure-youre-flexible-but-lets-not-over-do-the-dutch-wink",
    "href": "posts/2022-08-29-priors4/priors4.html#sure-youre-flexible-but-lets-not-over-do-the-dutch-wink",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "Sure you’re flexible, but let’s not over-do the Dutch wink",
    "text": "Sure you’re flexible, but let’s not over-do the Dutch wink\nNow that we have the concept of a flexibility parameter, let’s think about how we should use it. In particular, we should ask exactly what we want our prior to do. In the paper we listed 8 things that we want the prior to do:\n\nThe prior should contain information17 18 19\nThe prior should be aware of model structure\nIf we move our model to a new application, it should be clear how we can change the information contained in our prior. We can do this by explicitly including specific information in the prior.\nThe prior should limit20 the flexibility of an overparameterised model\nRestrictions of the prior to identifiable sub-manifolds21 of the parameter space should be sensible.\nThe prior should be specified to control what a parameter does in the context22 of the model (rather than its numerical value)\nThe prior should be computationally23 feasible\nThe prior should perform well24.\n\nThese desiderata are aspirational and I in no way claim that we successfully satisfied them. But we tried. And we came up with a pretty useful proposal.\nThe idea is simple: if our model has a flexibility parameter we should put a prior on it that penalises the complexity of the model. That is, we want most of the prior mass to be near25 the base value.\nIn practice, we try to do this by penalising the complexity of each component of a model. For instance, consider the following model for a flexible regression: \\[\\begin{align*}\ny_i \\mid f, u_i &\\sim N(u_i +f(z_i), \\sigma^2) \\\\\nf &\\sim \\text{Smoothing-spline}(\\lambda)\\\\\nu_i &\\sim N( \\mu + x_i^T\\beta , \\tau^2).\n\\end{align*}\\] The exact definition26 of a smoothing spline that we are using is not wildly important, but it is specified27 by a smoothing parameter \\(\\lambda\\), and when \\(\\lambda=\\infty\\) we get our base model (a function that is equal to zero everywhere). This model has two components (\\(f\\) and \\(u\\)) and they each have one smoothing parameter (\\(\\lambda\\), with base model at \\(\\lambda = \\infty\\), and \\(\\tau\\), with base model at \\(\\tau = 0\\)).\nThe nice thing about splitting a model up into components and building priors for each component is that we can build generic priors for each component that can be potentially be tuned to make them appropriate for the global model. Is this a perfect way to realise our second aim? No. But it’s an ok place to start28."
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html#the-speed-of-a-battered-sav-proximity-to-the-base-model",
    "href": "posts/2022-08-29-priors4/priors4.html#the-speed-of-a-battered-sav-proximity-to-the-base-model",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "The speed of a battered sav: proximity to the base model",
    "text": "The speed of a battered sav: proximity to the base model\nOk. So you’re Brad Pitt. Wait. No.\nOk. So we need to build a prior that penalises complexity by putting most of its prior mass near the base model. In order to do this we need to first specify what we mean by near.\nThere are a lot of things that we could mean. The easiest choice would be to just use the natural distance from the base model in the parameter space. But this isn’t necessarily a good idea. Firstly, it falls flat when the base model is at infinity. But more importantly, it violates our 6th aim by ignoring the context of the parameter and just setting a prior on its numerical value.\nSo instead we are going to parameterise distance by asking ourselves a simple question: for a component with flexibility parameter \\(\\xi\\), how much more complex would our model component be if we used the value \\(\\xi\\) instead of the base value \\(\\xi_\\text{base}\\)?\nWe can measure this complexity using the Kullback-Leibler divergence (or KL divergence if you’re nasty) \\[\n\\operatorname{KL}(f || g) = \\int_\\Theta f(t) \\log\\left(\\frac{f(t)}{g(t)}\\right)\\,dt.\n\\] This is a quantity from information theory that directly measures how much information would be lost29 if we replaced the more complex model \\(f\\) with the simpler model \\(g\\). The more information that would be lost, the more complex \\(f\\) is relative to \\(g\\).\nWhile the Kullback-Leibler divergence looks a bit intimidating the first time you see it, it’s got a lot of nice properties:\n\nIt’s always non-negative.\nIt doesn’t depend on how you parameterise the distribution. If you do a smooth, invertible change of variables to both distribution the KL divergence remains unchanged.\nIt’s related to the information matrix and the Fisher distance. In particular, let \\(f(\\theta \\mid \\xi)\\) be a family of distributions parameterised by \\(\\xi\\). Then, near \\(\\xi_0\\), \\[\n\\operatorname{KL}(f(\\cdot \\mid \\xi_0 +\\delta)  || f(\\cdot \\mid \\xi_0)) = \\frac{\\delta^2}{2} I(\\xi_0) + o(\\delta^2),\n\\] where \\(I(\\xi) = \\mathbb{E}(\\log p(f(y \\mid \\xi))^2)\\) is the Fisher information. The quantity on the right hand side is the square of a distance from the base model.\nIt can be related to the total variation distance30 \\[\n\\|f - g\\|_\\text{TV} \\leq \\sqrt{\\frac{1}{2} \\operatorname{KL}(f || g)}.\n\\]\n\nBut it also has some less charming properties:\n\nThe KL divergence is not a distance!\nThe KL divergence is not symmetric, that is \\(\\operatorname{KL}(f || g) \\neq \\operatorname{KL}(g || f)\\)\n\nThe first of these properties is irrelevant to us. The second interesting. I’d argue that it is an advantage. We can think in an analogy: if your base model is a point at the bottom of a valley, there is a big practical difference between how much effort it takes to get from the base model to another model that is on top of a hill compared to the amount of effort it takes to go in the other direction. This type of asymmetry is relevant to us: it’s easier for data to tell a simple model that it should be more complex than it is to tell a complex model to be simpler. We want our prior information to somewhat even this out, so we put less prior mass on models that are more complex and more on models that are more complex.\nThere is one more little annoyance: if you look at the two distance measures that the KL divergence is related to, you’ll notice that in both cases, the KL divergence is related to the square of the distance and not the distance itself.\nIf we use the KL divergence itself as a distance proxy, it will increase too sharply31 and we may end up over-penalising. To that end, we use the following “distance” measure \\[\nd(\\xi) = \\sqrt{2 \\operatorname{KL}(f(\\cdot \\mid \\xi) || f(\\cdot \\mid \\xi_0))}.\n\\] If you’re wondering about that 2, it doesn’t really matter but it makes a couple of things ever so slightly cleaner down the road.\nOk. Let’s compute some of these distances!\n\nExample 4 (Overdispersion of a negative binomial (continued)) The negative binomial distribution is discrete so \\[\\begin{multline}\n\\frac{1}{2}d^2(\\alpha) = \\sum_{k=1}^\\infty \\frac{\\Gamma(k + \\alpha^{-1})}{\\Gamma(\\alpha^{-1})\\Gamma(k+1)}  \\left(\\frac{\\mu}{\\mu + \\alpha^{-1}}\\right)^k \\left(\\frac{\\alpha^{-1}}{\\mu + \\alpha^{-1}}\\right)^{1/\\alpha} \\\\\n\\times \\left[\\log \\Gamma(k  +\\alpha^{-1}) - \\log \\Gamma(\\alpha^{-1})  - k \\log(\\mu + \\alpha^{-1})\\right. \\\\ \\left. + \\alpha^{-1}\\log \\left(\\alpha^{-1}(\\mu + \\alpha^{-2})\\right)  + \\mu \\right].\n\\end{multline}\\] This has two problems: I can’t work out what it is and it might32 end up depending on \\(\\mu\\).\nThankfully we can use our alternative representation of the negative binomial to note that \\(u_i \\sim \\text{Gamma}(\\alpha^{-1}, \\alpha^{-1})\\) and so we could just as well consider \\(u_i\\) the model component that we want to penalise the complexity of. In this case we need the KL divergence33 between Gamma distributions \\[\\begin{align*}\n\\operatorname{KL}(\\text{Gamma}(a^{-1},a^{-1}) || \\text{Gamma}(b^{-1},b^{-1})) =& (a^{-1}-b^{-1}) \\psi(a^{-1}) \\\\ &\\quad- \\log\\Gamma(a^{-1}) + \\log\\Gamma(b^{-1}) \\\\ &\\quad\n+ b^{-1}(\\log a^{-1} - \\log b^{-1})\\\\ &\\quad + b^{-1}-a^{-1},\n\\end{align*}\\] where \\(\\psi(a)\\) is the digamma function.\nAs \\(b\\rightarrow 0\\), the KL divergence becomes34 \\[\\begin{align*}\n&b^{-1}  (\\log(a^{-1}) - \\psi(a^{-1})) + \\log\\Gamma(b^{-1}) - b^{-1}\\log b^{-1} + b^{-1}  + o(b^{-1})\\\\\n=& b^{-1} (\\log(a^{-1}) - \\psi(a^{-1})) + b^{-1} \\log b^{-1} - b^{-1} - b^{-1}\\log b^{-1} + b^{-1} \\\\\n= &b^{-1}  (\\log(a^{-1}) - \\psi(a^{-1})) + o(b^{-1}).\n\\end{align*}\\]\nNow, you will notice that as \\(b\\rightarrow 0\\) the KL divergence heads off to infinity. This happens a lot when the base model is much simpler than the flexible model. Thankfully, we will see later that we can ignore the factor of \\(b^{-1}\\) and get a PC prior that’s valid against the base model \\(\\text{Gamma}(b^{-1}, b^{-1})\\) for all sufficiently small \\(b&gt;0\\). This is not legally the same thing as having one for \\(b=0\\), but it is morally the same.\nWith this, we get \\[\nd(\\alpha) = \\sqrt{2\\log(\\alpha^{-1}) - 2\\psi(\\alpha^{-1}) }.\n\\]\nIf the digamma function is a bit too hardcore for you, the approximation \\[\n\\psi(\\alpha^{-1}) = \\log(\\alpha^{-1}) - \\frac{\\alpha}{2} + \\mathcal{O}(\\alpha^2)\n\\] gives the approximate distance \\[\nd(\\alpha) \\approx \\sqrt{\\alpha}.\n\\] That is, the distance we are using is approximately the standard deviation of \\(u_i\\).\nLet’s see if this approximation35 is any good.\n\nlibrary(tidyverse)\ntibble(alpha = seq(0.01, 20, length.out = 1000),\n       exact = sqrt(2*log(1/alpha) - 2*digamma(1/alpha)),\n       approx = sqrt(alpha)\n       ) |&gt;\n  ggplot(aes(x = alpha, y = exact)) + \n  geom_line(colour = \"red\") +\n  geom_line(aes(y = approx), colour = \"blue\", linetype = \"dashed\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt’s ok but it’s not perfect.\n\n\nExample 5 (Student-t degrees of freedom (Continued)) In our original paper, we computed the distance for the degrees of freedom numerically. However, Yongqiang Tang derived an analytic expression for it. \\[\nd(\\nu) = \\sqrt{1 +  \\log\\left(\\frac{2\\Gamma((\\nu+1)/2)^2}{(\\nu-2)\\Gamma(\\nu/2)^2}\\right) - (\\nu + 1)(\\psi((\\nu+1)/2) - \\psi(\\nu/2))}.\n\\]\nIf we note that \\[\n\\log(\\Gamma(z)) = \\left(z- \\frac{1}{2}\\right)\\log z - z + \\frac{1}{2}\\log(2\\pi)  + \\frac{1}{12z} + \\mathcal{O}(z^{-1}),\n\\] we can use this (and the above asymptotic expansion of the digamma function) to get We can use the same asymptotic approximations as above to get \\[\\begin{align*}\nd(\\nu)^2 \\approx& {} 1 + \\log \\left(\\frac{2}{\\nu-2}\\right) \\\\\n&\\quad {} + 2\\left(\\frac{\\nu}{2}\\log \\frac{\\nu+1}{2} - \\frac{\\nu+1}{2} + \\frac{1}{2}\\log(2\\pi)  + \\frac{1}{6(\\nu+1)}\\right) \\\\\n&\\quad -2\\left(\\frac{\\nu-1}{2}\\log \\frac{\\nu}{2} - \\frac{\\nu}{2} + \\frac{1}{2}\\log(2\\pi)  + \\frac{1}{6\\nu}\\right) \\\\\n&\\quad {} - (\\nu + 1)(\\log((\\nu+1)/2) - \\frac{1}{\\nu+1}- \\log(\\nu/2) + \\frac{1}{\\nu}) \\\\\n=& \\log \\left(\\frac{\\nu^2}{(\\nu+1)(\\nu-2)}\\right)   - \\frac{\\nu +2}{3\\nu(\\nu+1)}.\n\\end{align*}\\]\nLet’s check this approximation numerically.\n\ntibble(nu = seq(2.1, 300, length.out = 1000),\n       exact = sqrt(1 + log(2/(nu-2)) + \n                      2*lgamma((nu+1)/2) - 2*lgamma(nu/2) - \n                      (nu + 1)* (digamma((nu+1)/2)-\n                                   digamma(nu/2))),\n       approx = sqrt(log(nu^2/((nu-2)*(nu+1))) - (nu+2)/(3*nu*(nu+1)))\n       ) |&gt;\n  ggplot(aes(x = nu, y = exact)) + \n  geom_line(colour = \"red\") +\n  geom_line(aes(y = approx), colour = \"blue\", linetype = \"dashed\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nOnce again, this is not a terrible approximation, but it’s also not an excellent one.\n\n\nExample 6 (Variance of a Gaussian random effect (Continued)) The distance calculation for the standard deviation of a Gaussian random effect has a very similar structure to the negative binomial case. We note, via wikipedia, that \\[\\begin{align*}\n\\operatorname{KL}(N(\\mu, \\tau^2) || N(\\mu, \\epsilon^2)) &= \\log \\frac{\\tau}{\\epsilon} + \\frac{\\tau^2}{\\epsilon^2} - \\frac{1}{2}  \\\\\n&= \\frac{\\tau^2}{\\epsilon^2}\\left(1 + \\frac{\\epsilon^2}{\\tau^2}\\log \\frac{\\tau}{\\epsilon}- \\frac{\\epsilon^2}{2\\tau^2}\\right).\n\\end{align*}\\]\nThis implies that \\[\nd(\\tau) = \\epsilon^{-1}\\tau + o(\\epsilon^{-1}).\n\\] We shall see later that the scaling on the \\(\\tau\\) doesn’t matter so for all intents and purposed \\[\nd(\\tau) = \\tau.\n\\]"
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html#spinning-off-the-flute-into-a-flat-bag-turning-a-distance-into-a-prior",
    "href": "posts/2022-08-29-priors4/priors4.html#spinning-off-the-flute-into-a-flat-bag-turning-a-distance-into-a-prior",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "Spinning off the flute into a flat bag: Turning a distance into a prior",
    "text": "Spinning off the flute into a flat bag: Turning a distance into a prior\nSo now that we have a distance measure, we need to turn it into a prior. There are lots of ways we can do this. Essentially any prior we put on the distance \\(d(\\xi)\\) can be transformed into a prior on the flexibility parameter \\(\\xi\\). We do this through the change of variables formula \\[\np_\\xi(\\xi) = p_d(d(\\xi))\\left|\\frac{d}{d\\xi} d(\\xi)\\right|,\n\\] where \\(p_d(\\cdot)\\) is the prior density for the distance parameterisation\nBut which prior should we use on the distance? A good default choice is a prior that penalises at a constant rate. That is, we want \\[\n\\frac{p_d(d + \\delta)}{p_d(d)} = r^{\\delta}\n\\] for some \\(0&lt;r&lt;1\\). This condition says that the rate at which the density decreases does not change as we move through the parameter space. This is extremely useful because any other (monotone) distribution is going to have a point at which the bulk changes to the tail. As we are putting our prior on \\(d\\), we won’t necessarily be able to reason about this point.\nConstant-rate penalisation implies that the prior on the distance scale is an exponential distribution and, hence, we get our generic PC prior for a flexibility parameter \\(\\xi\\) \\[\np(\\xi) = \\lambda e^{-\\lambda d(\\xi)}\\left|\\frac{d}{d\\xi} d(\\xi)\\right|.\n\\]\n\nExample 7 (Overdispersion of a negative binomial (continued)) The exact PC prior for the overdispersion parameter in the negative binomial distribution is \\[\np(\\alpha) = \\frac{\\lambda}{\\alpha^{2}}\\frac{\\left|\\psi'\\left(\\alpha^{-1}\\right)-\\alpha\\right|}{ \\sqrt{2 \\log (\\alpha^{-1}) - 2 \\psi(\\alpha^{-1})}} \\exp \\left[ -\\lambda \\sqrt{2 \\log (\\alpha^{-1}) - 2 \\psi(\\alpha^{-1})}\\right],\n\\] where \\(\\psi'(\\cdot)\\) is the derivative of the digamma function.\nOn the other hand, if we use the approximate distance we get \\[\np_\\text{approx}(\\alpha) = \\frac{\\lambda}{2\\sqrt{\\alpha}} e^{-\\lambda \\sqrt{\\alpha}}.\n\\]\n\nlambda &lt;- 1\ndat &lt;- tibble(alpha = seq(0.01, 20, length.out = 1000),\n       exact = lambda / alpha^2 * abs(trigamma(1/alpha) - alpha)/\n         sqrt(2*log(1/alpha) -\n                2*digamma(1/alpha))*\n         exp(-lambda*sqrt(2*log(1/alpha) - \n                            2*digamma(1/alpha))),\n       approx = lambda/(2*sqrt(alpha))*exp(-lambda*sqrt(alpha))\n       ) \ndat |&gt;\n  ggplot(aes(x = alpha, y = exact)) + \n  geom_line(colour = \"red\") +\n  geom_line(aes(y = approx), colour = \"blue\", linetype = \"dashed\") +\n  theme_bw()\n\n\n\n\n\n\n\ndat |&gt;\n  ggplot(aes(x = alpha, y = exact - approx)) + \n  geom_line(colour = \"black\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThat’s a pretty good agreement!\n\n\nExample 8 (Student-t degrees of freedom (Continued)) An interesting feature of the PC prior (and any prior where the density on the distance scale takes its maximum at the base model) is that the implied prior on \\(\\nu\\) has no finite moments. In fact, if your prior on \\(\\nu\\) has finite moments, the density on the distance scale is zero at zero!\nThe exact PC prior for the degrees of freedom in a Student-t distribution is \\[\np(\\nu) = \\lambda \\frac{\\frac{1}{\\nu-2} + \\frac{\\nu+1}{2}\\left[\\psi'\\left(\\frac{\\nu+1}{2}\\right)-\\psi'\\left(\\frac{\\nu}{2}\\right)\\right]}{4d(\\nu)}e^{-\\lambda d(\\nu)},\n\\] where \\(d(\\nu)\\) is given above.\nThe approximate PC prior is \\[\np_\\text{approx}(\\nu) = \\lambda\\frac{\\nu(\\nu+2)(2\\nu+9) + 4}{3\\nu^2(\\nu+1)^2(\\nu-2)} \\left(\\frac{\\nu^2}{(\\nu+1)(\\nu-2)}\\right)^\\lambda e^{   - \\lambda\\frac{\\nu +2}{3\\nu(\\nu+1)}}.\n\\] Let’s look at the difference.\n\ndist_ex &lt;- \\(nu) sqrt(1 + log(2/(nu-2)) + \n                      2*lgamma((nu+1)/2) - 2*lgamma(nu/2) - \n                      (nu + 1)* (digamma((nu+1)/2)-\n                                   digamma(nu/2)))\ndist_ap &lt;- \\(nu) sqrt(log(nu^2/((nu-2)*(nu+1))) - (nu+2)/(3*nu*(nu+1)))\n\nlambda &lt;- 1\ndat &lt;- tibble(nu = seq(2.1, 30, length.out = 1000),\n       exact = lambda * (1/(nu-2) + (nu+1)/2 * (trigamma((nu+1)/2) - trigamma(nu/2)))/(4*dist_ex(nu)) * exp(-lambda*dist_ex(nu)),\n       approx = lambda * (nu*(nu+2)*(2*nu + 9) + 4)/(3*nu^2*(nu+1)^2*(nu-2)) * exp(-lambda*dist_ap(nu))\n       ) \ndat |&gt;\n  ggplot(aes(x = nu, y = exact)) + \n  geom_line(colour = \"red\") +\n  geom_line(aes(y = approx), colour = \"blue\", linetype = \"dashed\") +\n  theme_bw()\n\n\n\n\n\n\n\ndat |&gt;\n  ggplot(aes(x = nu, y = exact - approx)) + \n  geom_line(colour = \"black\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe approximate prior isn’t so good for \\(\\nu\\) near 2. In the original paper, the distance was tabulated for \\(\\nu &lt; 9\\) and a different high-precision asymptotic expansion was given for \\(\\nu&gt;9\\).\nIn the original paper, we also plotted some common priors for the degrees of freedom on the distance scale to show just how informative flat-ish priors on \\(\\nu\\) can be! Note that the wider the uniform prior on \\(\\nu\\) is the more informative it is on the distance scale.\n\n\n\n(Left) Exponential priors on \\(\\nu\\) shown on the distance scale, from right to left the mean of the prior increases (5, 10, 20). (Right) \\(\\text{Uniform}[2, M]\\) priors on \\(\\nu\\) shown on the distance scale. From left to right \\(M\\) increases (20, 50, 100).\n\n\n\n\nExample 9 (Variance of a Gaussian random effect (Continued)) This is the easy one because the distance is equal to the standard deviation! The PC prior for the standard deviation of a Gaussian distribution is an exponential prior \\[\np(\\sigma) = \\lambda e^{-\\lambda \\sigma}.\n\\] More generally, if \\(u \\sim N(0, \\sigma^2 R)\\) is a multivariate normal distribution, than the PC prior for \\(\\sigma\\) is still \\[\np(\\sigma) = \\lambda e^{-\\lambda \\sigma}.\n\\] The corresponding prior on \\(\\sigma^2\\) is \\[\np(\\sigma^2) = \\frac{\\lambda}{2\\sqrt{\\sigma^2}}e^{-\\lambda\\sqrt{\\sigma^2}}.\n\\] Sometimes, for instance if you’re converting a model from BUGS or you’re looking at the smoothing parameter of a smoothing spline, you might specify your normal distribution in terms of the precision, which is the inverse of the variance. If \\(u \\sim N(0, \\gamma^{-1}Q^{-1})\\), then the corresponding PC prior (using the change of variables \\(\\gamma = \\sigma^{-2}\\)) is \\[\np(\\gamma) = \\frac{\\lambda}{2}\\gamma^{-3/2} e^{-\\lambda \\gamma^{-1/2}}.\n\\]\nThis case was explored extensively in the context of structured additive regression models (think GAMs but moreso) by Klein and Kneib, who found that the choice of exponential prior on the distance scale gave more consistent performance than either a half-normal or a half-Cauchy distribution."
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html#closing-the-door-how-to-choose-lambda",
    "href": "posts/2022-08-29-priors4/priors4.html#closing-the-door-how-to-choose-lambda",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "Closing the door: How to choose \\(\\lambda\\)",
    "text": "Closing the door: How to choose \\(\\lambda\\)\nThe big unanswered question is how do we choose \\(\\lambda\\). The scaling of a prior distribution is vital to its success, so this is an important question.\nAnd I will just say this: work it out your damn self.\nThe thing about prior distributions that shamelessly include information is that, at some point, you need to include36 some information. And there is no way for anyone other than the data analyst to know what the information to include is.\nBut I can outline a general procedure.\nImagine that for your flexibility parameter \\(\\xi\\) you have some interpretable transformation of it \\(Q(\\xi)\\). For instance if \\(\\xi = \\sigma^2\\), then a good choice for \\(Q(\\cdot)\\) would be \\(Q(\\sigma^2)=\\sigma\\). This is because standard deviations are on the same scale as the observations37, and we have intuition about that happens one standard deviation from the mean.\nWe then use problem-specific information can help us set a natural scale for \\(Q(\\xi)\\). We do this by choosing \\(\\lambda\\) so that \\[\n\\Pr(Q(\\xi) &gt; U) = \\alpha\n\\] for some \\(U\\), which we would consider large38 for our problem, and \\(0&lt;\\alpha&lt;1\\).\nFrom the properties of the exponential distribution, we can see that we can satisfy this if we choose \\[\n\\lambda = - \\frac{\\log(\\alpha)}{d^{-1}(Q^{-1}(U))}.\n\\] This can be found numerically if it needs to be.\nThe simplest case is the standard deviation of the normal distribution, because in this case \\(Q(\\sigma) = \\sigma\\) and \\(d^{-1}(Q^{-1}(U)) = U\\). In general, if \\(u \\sim N(0, \\sigma R)\\) and \\(R\\) is not a correlation matrix, you should take into account the diagonal of \\(R\\) when choosing \\(Q\\). For instance, choosing \\(Q\\) to be the geometric mean39 of the marginal variances of the \\(u_i\\) is a good idea!\nWhen a model has more than one component, or a component has more than one flexibility parameter, it can be the case that \\(Q(\\cdot)\\) depends on multiple parameters. For instance, if I hadn’t reparameterised the Student-t distribution to have variance independent of \\(\\nu\\), a PC prior on \\(\\sigma\\) would have a quantity of interest that depends on \\(\\nu\\). We will also see this if I ever get around to writing about priors for Gaussian processes."
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html#the-dream-pc-priors-in-practice",
    "href": "posts/2022-08-29-priors4/priors4.html#the-dream-pc-priors-in-practice",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "The Dream: PC priors in practice",
    "text": "The Dream: PC priors in practice\nThus we can put together a PC prior as the unique prior that follows the following four principles:\n\nOccam’s razor: We have a base model that represents simplicity and we prefer our base model.\nMeasuring complexity: We define the prior using the square root of the KL divergence between the base model and the more flexible model. The square root ensures that the divergence is on a similar scale to a distance, but we maintain the asymmetry of the divergence as as a feature (not a bug).\nConstant penalisation: We use an exponential prior on the distance scale to ensure that our prior mass decreases evenly as we move father away from the base model.\nUser-defined scaling: We need the user to specify a quantity of interest \\(Q(\\xi)\\) and a scale \\(U\\). We choose the scaling of the prior so that \\(\\Pr(Q(\\xi) &gt; U) = \\alpha\\). This ensures that when we move to a new context, we are able to modify the prior by using the relevant information about \\(Q(\\xi)\\).\n\nThese four principles define a PC prior. I think the value of laying them out explicitly is that users and critics can clearly and cleanly identify if these principles are relevant to their problem and, if they are, they can implement them. Furthermore, if you need to modify the principles (say by choosing a different distance measure), there is a clear way to do that.\nI’ve come to the end of my energy for this blog post, so I’m going to try to wrap it up. I will write more on the topic later, but for now there are a couple of things I want to say.\nThese priors can seem quite complex, but I assure you that are a) useful, b) used, and c) not too terrible in practice. Why? Well fundamentally because you usually don’t have to derive them yourselves. Moreover, a lot of that complexity is the price we pay for dealing with densities. We think that this is worth it and the lesson that the parameterisation that you are given may not be the correct parameterisation to use when specifying your prior is an important one!\nThe original paper contains a bunch of other examples. The paper was discussed and we wrote a rejoinder40, which contains an out-of-date list of other PC priors people have derived. If you are interested in some other people’s views of this idea, a good place to start is the discussion of the original paper.\nThere are also PC priors for Gaussian Processes, disease mapping models, AR(p) processes, variance parameters in multilevel models, and many more applications.\nPC priors are all over the INLA software package and its documentation contains a bunch more examples.\nTry them out. They’ll make you happy."
  },
  {
    "objectID": "posts/2022-08-29-priors4/priors4.html#footnotes",
    "href": "posts/2022-08-29-priors4/priors4.html#footnotes",
    "title": "Priors part 4: Specifying priors that appropriately penalise complexity",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve not turned on my computer for six weeks and tbh I finished 3 games and I’m caught up on TV and the weather is shite.↩︎\n“But what about sparse matrices?!” exactly 3 people ask. I’ll get back to them. But this is what I’m feeling today.↩︎\nI am told my Mercury is in Libra and truly I am not living that with those posts. Maybe Mercury was in Gatorade when I wrote them. So if we can’t be balanced at least let’s like things.↩︎\nOur weapons of ass destruction that lives in Canada?↩︎\nNegative binomial parameterised by mean and overdispersion so that its mean is \\(\\mu\\) and the variance is \\(\\mu(1+\\alpha \\mu)\\) because we are not flipping fucking coins here↩︎\nHello and welcome to Statistics for Stupid Children. My name is Daniel and I will be your host today.↩︎\nIf we didn’t have stupid children we’d never get dumb adults and then who would fuck me? You? You don’t have that sort of time. You’ve got a mortgage to service and interest rates are going up. You’ve got your Warhammer collection and it is simply not going to paint itself. You’ve been meaning to learn how to cook Thai food. You simply do not have the time. (I’m on SSRIs so it’s never clear what will come first: the inevitable decay and death of you and your children and your children’s children; the interest, eventual disinterest, and inevitable death of the family archivist from the far future who digs up your name from the digital graveyard; the death of the final person who will ever think of you, thereby removing you from the mortal realm entirely; the death of the universe; or me. Fucking me is a real time commitment.)↩︎\nGamma is parameterised by shape and rate, so \\(u_i\\) has mean 1 and variance \\(\\alpha\\).↩︎\nintegrate↩︎\nSometimes, people still refer to these as hyperparameters and put priors on them, which would clarify things, but like everything in statistics there’s no real agreed upon usage. Because why would anyone want that?↩︎\nsomehow↩︎\nlocation parameter↩︎\nThis is critical: we do not know \\(\\nu\\) so the only way we can put a sensible prior on the scaling parameter is if we disentangle the role of these two parameters!↩︎\nIn fact, if my model estimated the data-level variance to be nearly zero I would assume I’ve fucked something up elsewhere and my model is either over-fitting or I have a redundancy in my model (like if \\(J = n\\)).↩︎\nThere are some mathematical peculiarities that we will run into later when the base model is singular. But they’re not too bad.↩︎\nThe Arianist heresy is that God, Jesus, and the Holy Spirit are three separate beings rather than consubstantial. It’s the reason for that bit of the Nicene. The statistical version most commonly occurs when you consider you model for your data conditional on the parameters (you likelihood) and your model for the parameters (your prior) as separate objects. This can lead to really dumb priors and bad inferences.↩︎\nComplaining that a prior is adding information is like someone complaining to you that his boyfriend has stopped fucking him and you subsequently discovering that this is because his boyfriend died a few weeks ago. Like I’m sorry Jonathan, I know even the sight of a traffic cone sets your bussy a-quiverin’, but there really are bigger concerns and I’m gonna need you to focus.↩︎\nIn this story, the bigger concerns are things like misspecification, incorrect assumptions, data problems etc etc, the traffic cone is an unbiased estimator, Jonathan is our stand in for a generic data analyst, and Jonathan’s bussy is said data scientist’s bussy.↩︎\nYes, I know that there are problems with giving my generic data analyst a male name. Did I carefully think through the gender and power dynamics in my bussy simile? I think the answer to that is obvious.↩︎\nWe use priors for the same reason that other people use penalties: we don’t want to go into a weird corner of our model space unless our data explicitly drags us there↩︎\nThis is a bit technical. When a model is over-parameterised, it’s not always possible to recover all of the parameters. So we ideally want to make sure that if there are bunch of asymptotically equivalent parameters, our prior operates sensibly on that set. An example of this will come in a future post where I’ll talk about priors for the parameters of a Gaussian process.↩︎\nThat Arianism thing creeping in again!↩︎\nThere are examples of theoretically motivated priors where it’s wildly expensive to compute their densities. We will see one in a later post about GPs.↩︎\nSure, Jan. Of course we want that. But we believed that it was important to include this in a list of desiderata because we never want to say “our prior has motivation X and therefore it is good”. It is not enough to be pure, you actually have to work.↩︎\nWhat do I mean by near? Read on McDuff.↩︎\nThink of it as a P-spline if you must. The the important thing is that the weights of the basis functions are jointly normal with mean zero and precision matrix \\(\\lambda Q\\).↩︎\nGiven the knots, which are fixed↩︎\nI might talk about more advanced solutions at some point.↩︎\nStrictly how many bits would we need ↩︎\nThe largest absolute difference between the probability that an event \\(A\\) happens under \\(f\\) and \\(g\\).↩︎\nWhen performing the battered sav, it’s important to not speed up too quickly lest you over-batter.↩︎\nIt also might not. I don’t care to work it out.↩︎\nThe “easy” way to get this is to use the fact that the Gamma is in the exponential family and use the general formula for KL divergences in exponential families. The easier way is to look it up on Wikipedia↩︎\nUsing asymptotic expansions for the log of a Gamma function at infinity↩︎\nI’ll be dead before I declare that something is an approximation without bloody checking how good it is.↩︎\nWe have already included information that \\(\\xi\\) is a flexibility parameter with base model \\(\\xi_\\text{base}\\), but that is model-specific information. Now we move on to problem specific information.↩︎\nthe have the same units↩︎\nSame thing happens if we want a particular quantity not to be too small, just swap the signs↩︎\nAlways average on the natural scale. For non-negative parameters geometric means make a lot more sense than arithmetic means!↩︎\nHomosexually titled You just keep on pushing my love over the borderline: a rejoinder. I’m still not sure how I got away with that.↩︎"
  },
  {
    "objectID": "posts/2023-01-30-diffusion/diffusion.html",
    "href": "posts/2023-01-30-diffusion/diffusion.html",
    "title": "Diffusion models; or Yet another way to sample from an arbitrary distribution",
    "section": "",
    "text": "The other day I went to the cinema and watched M3GAN, a true movie masterpiece1 about the death and carnage that ensues when you simply train your extremely complex ML model and don’t do proper ethics work. And that, of course, made me want to write a little bit about something relatively hip, hop, and happening2 in the ML/AI space. But, like, I’m not gonna be that on trend3 because fuck that noise, so I’m gonna talk about diffusion models.\nIt’s worth noting that I know bugger all about diffusion models. But when they first came out, I had a quick look at how they worked and then promptly forgot about them because, let’s face it, I work on different things. But hey. If that’s not enough4 knowledge to write a blog post, I don’t know what is.\nAnd here’s the thing. Most of the time when I blog about something I know a lot about it. Sometimes too much. But this is not one of those times. There are plenty of resources on the internet if you want to learn about diffusions models from an expert. Oodles. But where else but here can you read the barely proof-read writing of a man who read a couple of papers yesterday?\nAnd who doesn’t want5 that?"
  },
  {
    "objectID": "posts/2023-01-30-diffusion/diffusion.html#a-prelude-measure-transport-for-sampling-from-arbitrary-distributions",
    "href": "posts/2023-01-30-diffusion/diffusion.html#a-prelude-measure-transport-for-sampling-from-arbitrary-distributions",
    "title": "Diffusion models; or Yet another way to sample from an arbitrary distribution",
    "section": "A prelude: Measure transport for sampling from arbitrary distributions",
    "text": "A prelude: Measure transport for sampling from arbitrary distributions\nOne of the fundamental tasks in computational statistics is to sample from a probability distribution. There are millions of ways of doing this, but the most popular generic method is Markov chain Monte Carlo. But this is not the post about MCMC methods. I’ve already made a post about MCMC methods.\nInstead, let’s focus on stranger ways to do it. In particular, let’s think about methods that, create a mapping \\(T: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\) that may depend on some properties of the target distribution such that the following procedure constructs a sample \\(x \\sim p(x)\\):\n\nSample \\(u \\sim p(u)\\) for some known distribution \\(q(u)\\)\nSet \\(x = T(u)\\)\n\nThe general problem of starting with a distribution \\(q(\\cdot)\\) and mapping it to another distribution \\(p(\\cdot)\\) is an example of a problem known as measure transport. Transport problems have been studied by mathematicians for yonks. It turns out that there are an infinite number of mappings \\(T\\) that will do the job, so it’s up to us to choose a good one.\nProbably the most famous6 transport problem is the optimal transport problem that was first studied by Monge and Kantorovich that tries to find a mapping \\(T\\) that minimises \\[\n\\mathbb{E}_{x \\sim q}(c(x, T(x)))\n\\] subject to the constraint that \\(T(x) \\sim p\\) whenever \\(x \\sim q\\), where \\(c(x,y)\\) is some sort of cost function. There are canonical choices of cost function, but for the most part we are free to choose something that is convenient.\nThe measure transport concept is underneath the method of normalising flows, but the presentation that I’m most familiar with is due to Youssef Marzouk and his collaborators in 2011 and predates the big sexy normalising flow papers by a few years.\n\nContinuous distributions in 1D\nIf \\(p\\) and \\(q\\) are both continuous, univariate distributions, it is pretty easy to construct a transport map. In particular, if \\(F_p\\) is the cumulative distribution function of \\(p\\), then \\[\nT(x) = F_p^{-1}(F_q(x))\n\\] is a transport map. This works because, if \\(x \\sim q\\), then \\(F_q(x) \\sim \\text{Unif}(0,1)\\). From this, we can use everyone’s favourite result that you can sample from a continuous univariate random variable \\(p\\) by evaluating the quantile function at a uniform random value.\nThere are, of course, two problems with this: it only works in one dimension and we usually don’t know \\(F^{-1}\\) explicitly.\nThe second of these isn’t really a problem if we are willing to do something splendifferously dumb. And I am. Because I’m gay and frivolous7.\nIf I write \\(Q(t) = F^{-1}(t)\\) then I can differentiate this to get \\[\n\\frac{dQ}{dt} = \\frac{1}{p(Q)},\\qquad Q(0) = -\\infty.\n\\] This is a very non-linear differential equation. We can make it even more non-linear differential equation by repeating the procedure to get \\[\n\\frac{d^2Q}{dt^2} = \\frac{1}{p(Q)^2} p'(Q)\\frac{dQ}{dt}.\n\\] Noting that \\(Q' = 1/p(Q)\\) we get \\[\n\\frac{d^2 Q}{dt^2} = \\frac{p'(Q)}{p(Q)} \\left(\\frac{dQ}{dt}\\right)^2.\n\\] This is a rubbish differential equation, but it has the singular advantage that it doesn’t depend8 on the normalising constant for \\(p\\), which can be useful. The downside is that the boundary conditions are infinite on both ends.\nRegardless of that particular challenge, we could use this to build a generic algorithm.\n\nSample \\(u \\sim \\text{Unif}(0,1)\\)\nUse a numerical differential equation solver to solve the equation with boundary conditions \\[\nq(0) = -M, \\quad q(1) = M\n\\] for some sufficiently large number \\(M\\) and return \\(x = q(u)\\)\n\nThis will sample from \\(p(x)\\) truncated to \\([-M, M]\\).\nI was going to write some python code to do this, but honestly it hurts my soul. So I shan’t.\n\n\nTransport maps: A less terrible method that works on general densities\nOutside of one dimension, there is (to the best of my knowledge) no direct solution to the transport problem. That means that we need to solve our own. Thankfully, the glorious Youssef Marzouk and a bunch of his collaborators have spent some quality time mapping out this idea. A really nice survey of their results can be found in this paper.\nEssentially the idea is that we can try to find the most convenient transport map available to us. In particular, it’s useful to minimise the Kullback-Leibler divergence between \\(q\\) and its transport. After a little bit9 of maths, this is equivalent to minimising \\[\n\\mathbb{E}_{x \\sim q}\\left(\\log p(T(x)) + \\log \\det \\nabla T(x)\\right),\n\\] where \\(\\nabla T(x)\\) is the Jacobian of \\(T\\). To finish the specification of the optimisation problem, it’s enough to consider triangular maps10 \\[\nT(x) = \\begin{pmatrix} T_1(x_1) \\\\ T_2(x_1,x_2,) \\\\ \\vdots \\\\ T_d(x_1, \\ldots, x_d) \\end{pmatrix}\n\\] with the additional constraint that their Jacobians have positive determinants. Using a triangular map has two distinct advantages: it’s parsimonious and it makes the positive determinant constraint much easier to deal with. Triangular maps are also sufficient for the problem (my man Bogachev showed it in 2005).\nThat said, this can be a somewhat tricky optimisation problem. Youssef and his friends have spilt a lot of ink on this topic. And if you’re the sort of person who just fucking loves a weird optimisation problem, I’m sure you’ve got thoughts. With and without the triangular constraint, this can be parameterised as the composition of a sequence of simple functions, in which case you turn three times and scream neural net and a normalising11 flow appears.\n\n\nWhat if we only have samples from the target density\nAll of that is very lovely. And quite nice in its context. But what happens if you don’t actually have access of the (unnormalised) log density of the target? What if you only have samples?\nThe good news is that you’re not shit out of luck. But it’s a bit tricky. And once again, that lovely review paper by Youssef and friends will tell us how to do it.\nIn particular, they noticed that if you swap the direction of the KL divergence, you get the optimisation problem for the inverse mapping \\(S(x) = T^{-1}(x)\\) that aims to minimise \\[\n\\mathbb{E}_{x \\sim p}\\left(\\log(q(S(x)) + \\log \\det \\nabla S(x)\\right)\n\\] where \\(S\\) is once again a triangular map subject to the monotonicity constraints \\[\n\\frac{\\partial S_k}{\\partial x_k} &gt; 0.\n\\] Because we have the freedom to choose the reference density \\(q(x)\\), we can choose it to be iid standard normals, in which case we get the optimisation problem \\[\\begin{align*}\n&\\min_S \\mathbb{E}_{x \\sim p}\\left[\\sum_{k = 1}^d \\frac{1}{2}\\left(S_k(z_1, \\ldots, s_k)\\right)^2 -  \\log \\frac{\\partial S_k}{\\partial x_k} \\right]\\\\\n&\\text{s.t.}& \\\\\n&\\quad \\frac{\\partial S_k}{\\partial x_k} &gt;0 \\\\\n&\\quad S \\text{ is triangular},\n\\end{align*}\\] which is a convex, separable optimisation problem that can be solved12 using, for instance, a stochastic gradient method. This can be turned into an unconstrained optimisation problem by explicitly parameterising the monotonicity constraint.\nThe monotonicity of \\(S\\) makes the resulting nonlinear solve to compute \\(T = S^{-1}\\) relatively straightforward. In fact, if \\(d\\) isn’t too big you can solve this sequentially dimension-by-dimension. But, of course, when you’ve got a lot of parameters this is a poor method and it would make more sense13 to attack it with some sort of gradient descent method. It might even be worth taking the time to learn the inverse function \\(T = S^{-1}\\) so that can be applied for, essentially, free.\n\n\nSo does it work?\nTo some extent, the answer is yes. This is very much normalising flows in its most embryonic form. They work to some extent. And this presentation makes some of the problems fairly obvious:\n\nThere’s no real guarantee that \\(T\\) is going to be a nice smooth map, which means that we may have problems moving beyond the training sample.\nThe most natural way to organise the computations are naturally sequential involving sweeps across the \\(d\\) parameters. This can be difficult to parallelise efficiently on modern architectures.\nThe complexity of the triangular map is going to depend on the order of variables. This is fine if you’re processing something that is inherently sequential, but if you’re working with image data, this can be challenging.\n\nOf course, there are a pile of ways that these problems can be overcome in whole or in part. I’d point you to the last five years of ML conference papers. You’re welcome."
  },
  {
    "objectID": "posts/2023-01-30-diffusion/diffusion.html#continuous-normalising-flows-making-the-problem-easier-by-making-it-harder",
    "href": "posts/2023-01-30-diffusion/diffusion.html#continuous-normalising-flows-making-the-problem-easier-by-making-it-harder",
    "title": "Diffusion models; or Yet another way to sample from an arbitrary distribution",
    "section": "Continuous normalising flows: Making the problem easier by making it harder",
    "text": "Continuous normalising flows: Making the problem easier by making it harder\nA really clever idea, which is related to normalising flows, is to ask what if, instead of looking for a single14 map \\(S(x) = T^{-1}(x)\\), we tried to find a sequence of maps \\(S(x,t)\\) that smoothly move from the identity map to to the transport map.\nThis seems like it would be a harder problem. And it is. You need to make an infinite number of maps. But the saving grace is that as \\(t\\) changes slightly, the map \\(S(\\cdot, t)\\) is also only going to change slightly. This means that we can parameterise the change relatively simply.\nTo this end, we write \\[\n\\frac{\\partial S}{\\partial t} = f(S, t),\n\\] for some relatively simple function \\(f\\) that models the infinitesimal change in the transport map as we move along the path. The hope is that learning the vector field \\(f\\) will be easier than learning \\(S\\) directly. To finish the specification, we require that \\[\nS(x,0) = x.\n\\]\nThe question is _can we learn the function \\(f\\) from data? If we can, it will be (relatively) easy to evaluate the transport map for any sample by just solving15 the differential equation.\nIt turns out that the map \\(S\\) is most useful for training the normalising flow, while \\(T\\) is useful for generating samples from the trained model. If we were using the methods in the previous section, we would have had to commit to either modelling \\(S\\) or \\(T\\). One of the real advantages of the continuous formulation is that we can just as easily solve the equation with the terminal condition16 \\[\nS(x,1) = u\n\\] and solve the equation backwards in time to calculate \\(T(u) = S(x, 0)\\)! The dynamics of both equations are driven by the vector field \\(f\\)!\n\nA very quick introduction to inverse problems\nIt turns out that learning parameters of differential equation (and other physical models) has a long and storied history in applied mathematics under the name of inverse problems. If that sounds like statistics, you’d be right. It’s statistics, except with no interest in measurement or, classically, uncertainty.\nThe classic inverse problem framing involves a forward map \\(\\mathcal{F}(f)(t, x)\\) that takes as its input some parameters (often a function) and returns the full state of a system (often another function). For instance, the forwards map could be the solution of a partial differential equation like \\[\n  \\frac{\\partial S}{\\partial t} = f(S, t), \\qquad S(0) = x.\n\\] The thing that you should notice about this is that the forward map is a) possibly expensive to compute, b) not explicitly known, and c) extremely17 non-linear.\nThe problem is specified with \\(n\\) data points \\((t_1, x_1), \\ldots, (t_n, x_n)\\) and the aim is to find the value of \\(f\\) that best fits the data. The traditional choice is to minimise the mean-square error \\[\n  \\theta = \\arg \\min_\\theta \\sum_{i=1}^n \\left(y_i - \\mathcal{F}(f)(t_i,x_i)\\right)^2.\n\\]\nNow every single one of you will know immediately that this question is both vague and ill-posed. There are many functions \\(f\\) that will fit the data. This means that we need to enforce18 some sort of complexity penalty on \\(f\\). This leads to the method known as Tikhonov regularisation19 \\[\n  \\theta = \\arg \\min_{\\theta \\in B} \\sum_{i=1}^n \\left(y_i - \\mathcal{F}(f)(t_i,x_i)\\right)^2 + \\lambda\\|f\\|_B^2,\n  \\] where \\(B\\) is some Banach space and \\(\\lambda&gt;0\\) is some tuning parameter.\nAs you can imagine, there’s a lot of maths under this about when there is a unique minimum, how the reconstruction behaves as \\(n\\rightarrow \\infty\\) and \\(\\lambda \\rightarrow 0\\), and how the choice of \\(B\\) effects the estimation of \\(\\theta\\). There is also quite a lot of work20 looking at how to actually solve these sorts of optimisation problems.\nEventually, the field evolved and people started to realise that it’s actually fairly important to quantify the uncertainty in the estimate. This is … tricky under the Tikhonov regularlisation framework, which became a big motivation for Bayesian inverse problems.\nAs with all Bayesianifications, we just need to turn the above into a likelihood and a prior. Easy. Well, the likelihood part, at least, is easy. If we want to line up with Tikhonov regularisation, we can choose a Gaussian likelihood \\[\ny_i \\mid f, x_i, t_i, \\sigma \\sim N(\\mathcal{F}(f)(t_i,x_i), \\sigma^2).\n\\]\nThis is familiar to statisticians, the forward model is essentially working as a non-standard link function in a generalised linear model. There are two big practical differences. The first one is that \\(\\mathcal{F}\\) is very non-linear and almost certainly not monotone. The second problem is that evaluations of \\(\\mathcal{F}\\) are typically very21 expensive. For instance, you may need to solve a system of differential equations. This means that any computational method22 is going to need to minimise the number of likelihood evaluations.\nThe choice of prior on \\(f\\) can, however, be a bit tricky. The problem is that in most traditional inverse problems \\(f\\) is a function23 and so we need to put a carefully specified prior on it. And there is a lot of really interesting work on what this means in a Bayesian setting. This is really the topic for another blogpost, but it’s certainly an area where you need to be aware of the limitations of different high-dimensional priors and how they perform in various contexts. For instance, if the function you are trying to reconstruct is likely to have a lot of sharp boundaries24 then you need to make sure that your prior can support functions with sharp boundaries. My little soldier bois25 don’t, so you need to get more26 creative.\n\n\nThe likelihood for a normalising flow\nOur aim now is to cast the normalising flow idea into the inverse problems framework. To do this, we remember that we begin our flow from a sample from \\(p(x)\\) and we then deform it until it becomes a sample from \\(q(u)\\) at some known time (which I’m going to choose as \\(t=1\\)). This means that if \\(x_i \\sim p\\), then \\[\nS(x_i, 1) \\sim q.\n\\]\nWe can now derive a relationship between \\(p\\) and \\(q\\) using the change of variables formula. In particular, \\[\np(x \\mid f) = q(S(x,1))\\left|\\det \\left( \\frac{d S(x,1)}{dx }\\right)\\right|,\n\\] which means that our log likelihood will be \\[\n\\log p(x \\mid f) = \\log q(S(x,1)) + \\log \\left|\\det \\left( \\frac{d S(x,1)}{dx }\\right)\\right|.\n\\]\nThe log-determinant term looks like it might cause some trouble. If \\(S\\) is parameterised as a triangular map it can be written explicitly, but there is, of course, another route.\nFor notational ease, let’s consider \\(z_t = S(x, t)\\), for some \\(t &lt;1\\). Then \\[\n\\log p(z_t \\mid f) = \\log q(S(x,1)) + \\log \\left|\\det \\left( \\frac{d S(x,t)}{dx }\\right)\\right|.\n\\] We can differentiate this with respect to \\(t\\) to get 27 to get \\[\n\\frac{\\partial \\log p(z_t \\mid f)}{\\partial t} = \\operatorname{tr}\\left(\\frac{df}{dx}(z_t,t)\\right),\n\\] where I used one of those magical vector calculus identities to get that trace. Remembering that \\(S(x,0) = x\\), the log-determinant of the Jacobian at zero is zero and so we get the initial condition \\[\n\\log p(z_t \\mid f) = \\log q(S(x,1)).\n\\]\nThe likelihood can be evaluated28 by solving the system of differential equations \\[\\begin{align*}\n\\frac{d z_t}{dt} &= f(z_t, t) \\\\\n\\frac{d \\ell}{dt} &=\\operatorname{tr}\\left(\\frac{df}{dx}(z_t,t)\\right) \\\\\nz_0 &= x \\\\\n\\ell(0) &= 0,\n\\end{align*}\\] and the log likelihood is evaluated as \\[\n\\log p(x \\mid f) = \\log q(z_1) + \\ell(1).\n\\]\nIt turns out that you can take gradients of the log-likelihood efficiently by solving an augmented system of differential equations that’s twice the size of the original. This allows for all kinds of gradient-driven inferential shenanigans.\n\n\nBut oh that complexity\nOne big problem with normalising flows as written is that we only have two pieces of information about the entire trajectory \\(z_t\\):\n\nwe know that \\(z(1) \\sim q\\), and\nwe know that \\(z(0) \\sim p\\).\n\nWe know absolutely nothing about \\(z_t\\) outside of those boundary conditions. This means that our model for \\(f\\) basically gets to freestyle in those areas.\nWe can avoid this to some extent by choosing appropriate neural network architectures and/or appropriate penalties in the classical case or priors in the Bayesian case. There’s a whole mini-literature on choosing appropriate penalties.\nJust to show how complex it is, let me quickly sketch what Finlay etc suggest as a way to keep the dynamics as boring as possible in the information desert. They lean into the literature on optimal transport theory to come up with the double penalty \\[\n\\min_f \\sum_{i=1}^n \\left(-\\log p(x_i) + \\lambda_1 \\int_0^T \\|f(S(x_i,s),s)\\|_2^2\\,ds + \\lambda_2 \\int_0^T\\left\\|\\frac{d f(S(x_i,s))}{ds}\\right\\|_F^2\\,ds\\right),\n\\] where the first term minimises the kinetic energy and, essentially, finds the least exciting path from \\(p\\) to \\(q\\), while the second term ensures that the Jacobian of \\(f\\) doesn’t get too big29, which means that the mapping doesn’t have many sharp changes. Both of these penalty terms are designed to both aid generalisation and to make sure the differential equation isn’t unnecessarily difficult for a ODE solver.\nA slightly odd feature of these penalties is that they are both data dependent. That suggests that a prior would, probably, require an amount of work. This is work that I don’t feel like doing today. Especially because this blog post isn’t about bloody normalising flows."
  },
  {
    "objectID": "posts/2023-01-30-diffusion/diffusion.html#diffusion-models",
    "href": "posts/2023-01-30-diffusion/diffusion.html#diffusion-models",
    "title": "Diffusion models; or Yet another way to sample from an arbitrary distribution",
    "section": "Diffusion models",
    "text": "Diffusion models\nOk, so normalising flows are cool, but there are a couple of places where they could potentially be improved. There is a long literature on diffusion models, but the one I’m mostly stealing from is this one by Song et al. (2021)\nFirstly, the vector field \\(f\\) directly effects how easy the differential equations are to solve. This means that if \\(f\\) is too complicated, it can take a long time to both train the model and generate samples from the trained model. To get around this you need to put fairly strict penalties30 and/or structural assumptions on \\(f\\).\nSecondly, we only have information31 at two ends of the flow. The problem would become a lot easier if we could somehow get information about intermediate states. In the inverse problems literature, there’s a concept of value of information that talks about how useful sampling a particular time point can be in terms of reducing model uncertainty. In general this, or other criteria, can be used to design a set of useful sampling times. I don’t particularly feel like working any of this out but one thing I am fairly certain of is that no optimal design would only have information at \\(t=0\\) and \\(t=1\\)!\nDiffusion models fix these two aspects of normalising flows at the cost of both a more complex mathematical formulation and some inexactness32 around the base distribution \\(q\\) when generating new samples.\n\nDiffusions and stochastic differential equations\nDiffusions are to applied mathematicians what gaffer tape is to33 a roadie. They are a ubiquitous, convenient, and they hold down the fort when nothing else works.\nThere are a number of diffusions that are familiar in statistics and machine learning. The most famous one is probably the Langevin diffusion \\[\ndX_t = \\frac{1}{2}\\nabla \\log p(x) dt + \\sigma dW_t,\n\\] which is asymptotically distributed according to \\(p\\). This forms the basis of a bunch of MCMC methods as well as some faster, less adjusted methods.\nBut that is not the only diffusion. Today’s friend is the Ornstein-Uhlenbeck (OU) process, which is a Gaussian process that \\[\ndX_t = - \\frac{1}{2} X_t \\,dt + \\sigma dW_t.\n\\] The OU process can be thought of as a mean-reverting Brownian motion. As such, it has continuous but nowhere differentiable sample paths\nThe stationary distribution of \\(X_t\\) is \\(X_\\infty \\sim N(0, \\sigma^2I)\\), where \\(I\\) is the identity matrix. In fact, if we start the diffusion at stationarity by setting \\[\nX_0 \\sim N(0, \\sigma^2I),\n\\] then X_t is a stationary Gaussian process with covariance function \\[\nc(t, t') = \\sigma^2e^{-\\frac{1}{2} |t-t'|}I.\n\\]\nMore interestingly in our context, however, is what happens if we start the diffusion from a fixed point \\(x\\), that will eventually be a sample from \\(p(x)\\). In that case, we can solve the linear stochastic differential equation exactly to get \\[\nX_t = xe^{-\\frac{1}{2}t} + \\sigma \\int_0^t e^{\\frac{1}{2}(s-t)}\\,dW_s,\n\\] where the integral on the right hand side can be interpreted34 as a white noise integral and so \\[\nX_t \\sim N\\left(xe^{-t}, \\sigma^2\\int_0^t e^{s-t}\\,dt\\right),\n\\] and the variance is \\[\n\\sigma^2\\int_0^t e^{s-t}\\,dt = \\sigma^2 e^{-t}\\frac{1}{2}\\left(e^{t} - 1\\right) = \\sigma^2(1-e^{-t}).\n\\] From these equations, we see that the mean of the diffusion hurtles exponentially fast towards zero and the variance moves at the same speed towards \\(\\sigma^2\\).\nMore importantly, this means that, given a starting point \\(X_0 = x\\), we can generate data from any part of the diffusion \\(X_t\\)! If we want a sequence of observations from the same trajectory, we can generate them sequentially using the fact that and OU process is a Markov35 process. This means that we are no longer limited to information at just two points along the trajectory.\n\n\nReversing the diffusion\nSo far, there is nothing to learn here. The OU process has a known drift and variance, so everything is splendid. It’s even easy to simulate from. The challenge pops up when we try to reverse the diffusion, that is, when we try to remove noise from a sample rather than add noise to it.\nIn some sense, this shouldn’t be too disgusting. A diffusion is a Markov process and, if we run the Markov process back in time, we still get a Markov process. In fact, we are going to get another diffusion process.\nThe twist is that the new diffusion process is going to be quite a bit more complex than the original one. The problem is that unless \\(X_0\\) comes from a Gaussian distribution, this process will be non-Gaussian, and thus somewhat tricky to find the reverse trajectory of.\nTo see this, consider \\(s&gt;t\\) and recall that \\[\np(X_0, X_t, X_s) = p(X_s \\mid X_t)p(X_t \\mid X_0)p(X_0)\n\\] and \\[\np(X_t, X_s) = \\int_{\\mathbb{R}^d} p(X_s \\mid X_t) p(X_t \\mid X_0) p(X_0)\\,dX_0.\n\\] The first two terms in that integrand are Gaussian densities and thus their product is a bivariate Gaussian density \\[\nX_t, X_s \\mid X_0 \\sim N\\left(X_0\\begin{pmatrix}e^{-\\frac{t}{2}}\\\\e^{-\\frac{s}{2}}\\end{pmatrix}, \\sigma^2 \\begin{pmatrix} 1 & e^{-\\frac{s-t}{2}} - e^{-\\frac{s+t}{2}} \\\\ e^{-\\frac{s-t}{2}} - e^{-\\frac{s+t}{2}} & 1\\end{pmatrix}\\right).\n\\] Unfortunately, as \\(X_0\\) is not Gaussian, the marginal distribution will be non-Gaussian. This means that our reverse time transition density \\[\np(X_t \\mid X_s) = \\frac{ \\int_{\\mathbb{R}^d} p(X_t,X_s \\mid X_0) p(X_0)\\,dX_0}{ \\int_{\\mathbb{R}^d} p(X_s \\mid X_t)  p(X_0)\\,dX_0}\n\\] is also going to be very non-linear.\nIn order to work out a stochastic differential equation that runs backwards in time and generates the same trajectory, we need a little bit of theory on how the unconditional density \\(p(X_t)\\) and the transition density \\(p(X_t \\mid X_s)\\) evolves in time \\(t\\) (here and everywhere st). These are related through the Kolmogorov equations.\nTo introduce these, we need to briefly consider the more general diffusion \\[\ndX_t = f(X_t, t)dt + g(X_t,t)dW_t\n\\] for nice36 vector/matrix-valued functions \\(f\\) and \\(g\\). Kolmogorov showed that the unconditional density \\(p(X_t) = p(x,t)\\) evolves according the the partial differential equation \\[\n\\frac{\\partial p(x,t)}{\\partial t} = - \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i}\\left(f_i(x,t)p(x,t)\\right) + \\frac{1}{2}\\sum_{i,j,k = 1}^d\\frac{\\partial^2}{\\partial x_j}\\left( g_{ik}(x,t)g_{jk}(x,t)p(x,t)\\right)\n\\] subject to the initial condition \\[\np(x,0) =p(x).\n\\] This is known as Kolmogorov’s forward equation or the Fokker-Planck equation.\nThe other key result is about the density of \\(X_t\\) conditioned on some future value \\(X_s = y\\), \\(s \\geq t\\). We write this density as \\(p(X_s =y\\mid X_t =x) =p(x,t; u,s)\\) and it satisfies the partial differential equation \\[\n\\frac{\\partial q(x,t;u,s)}{\\partial t} = -\\sum_{i=1}^d f_i(x,t)\\frac{\\partial q(x,t;u,s)}{\\partial x_i} - \\frac{1}{2}\\sum_{i,j,k=1}^d g_{ik}(x,t)g_{jk}(x,t)\\frac{\\partial^2 q(x,t;u,s)}{\\partial x_i\\partial x_j}\n\\] subject to the terminal condition \\[\np(x,s;u,s) = p(u,s).\n\\] This is known as the Kolmogorov backward equation. Great names. Beautiful names.\nLet’s consider a differential equation for the joint density \\[\np(X_t = x, X_s= y) = p(x,t,u,s) = q(x,t;u,s)p(x,t).\n\\] Going ham with the product rule gives \\[\n\\begin{align*}\n\\frac{\\partial p(x,t,u,s)}{\\partial t} &= p(x, t)\\frac{\\partial q(x,t;u,s)}{\\partial t} + q(x,t;u,s) \\frac{\\partial p(x,t)}{\\partial t} \\\\\n&=-\\sum_{i=1}^d p(x,t)f_i(x,t)\\frac{\\partial q(x,t;u,s)}{\\partial x_i} - \\frac{1}{2}\\sum_{ijk} p(x,t)g_{ik}(x,t)g_{jk}(x,t) \\frac{\\partial^2 q(x,t;u,s)}{\\partial x_i \\partial x_j} \\\\ &\\qquad-\\sum_{i=1}^dq(x,t;u,s)\\frac{\\partial}{\\partial x_i}(p(x,t)f(x,t)) + \\frac{1}{2} \\sum_{ijk}q(x,t;u,s)\\frac{\\partial^2}{\\partial x_i \\partial x_j}(g_{ik}(x,t)g_{jk}(x,t)p(x,t)) .\n\\end{align*}\n\\tag{1}\\] The first-order derivatives simplify, using the product rule, to \\[\n-\\sum_{i=1}^d\\frac{\\partial}{\\partial x_i}(p(x,t,u,s)f(x,t))\n\\]\nStaring at this for a moment, we notice that this looks has the same structure as the first-order term on the forward equation. In that case, the second-order term would be \\[\n\\begin{align*}\n&\\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial^2}{\\partial x_i x_j}[p(x,t,u,s) g_{ik}(x,t)g_{jk}(x,t)] = \\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial^2}{\\partial x_i x_j}[q(x,t;u,s) (p(x,t)g_{ik}(x,t)g_{jk}(x,t))] \\\\\n&\\qquad\\qquad=\\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial}{\\partial x_i}\\left[ q(x,t;u,s)\\frac{\\partial }{\\partial x_j}\\left(p(x,t)g_{ik}(x,t)g_{jk}(x,t)\\right) + p(x,t)g_{ik}(x,t)g_{jk}(x,t) \\frac{\\partial q(x,t;u,s)}{\\partial x_j}\\right]\n\\end{align*}\n\\]\nIf we notice that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial x_i}\\left[p(x,t)g_{ik}(x,t)g_{jk}(x,t) \\frac{\\partial q(x,t;u,s)}{\\partial x_j}\\right] =&  p(x,t)g_{ik}(x,t)g_{jk}(x,t) \\frac{\\partial^2 q(x,t;u,s)}{\\partial x_i \\partial x_j} \\\\\n&\\quad+ \\frac{\\partial}{\\partial x_i} \\left[p(x,t)g_{ik}(x,t)g_{jk}(x,t)\\right]\\left[ \\frac{\\partial q(x,t;u,s)}{ \\partial x_j}\\right]\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial x_i}\\left[ q(x,t;u,s)\\frac{\\partial }{\\partial x_j}\\left(p(x,t)g_{ik}(x,t)g_{jk}(x,t)\\right)\\right] =&  q(x,t;u,s)\\frac{\\partial^2 }{\\partial x_i \\partial x_j} p(x,t)g_{ik}(x,t)g_{jk}(x,t) \\\\\n&\\quad+ \\frac{\\partial}{\\partial x_i} \\left[p(x,t)g_{ik}(x,t)g_{jk}(x,t)\\right]\\left[ \\frac{\\partial q(x,t;u,s)}{ \\partial x_j}\\right]\n\\end{align*}\n\\] we can re-write the second-order derivative terms in Equation 1 as \\[\n\\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial}{\\partial x_i}\\left[ q(x,t;u,s)\\frac{\\partial }{\\partial x_j}\\left(p(x,t)g_{ik}(x,t)g_{jk}(x,t)\\right) - p(x,t)g_{ik}(x,t)g_{jk}(x,t) \\frac{\\partial q(x,t;u,s)}{\\partial x_j}\\right]\n\\]\nThis is almost, but not quite, what we want. We are a single minus sign away. Remembering that \\(q(x,t;u,s) = p(x,t,u,s)/p(x,t)\\) we probably don’t want it to turn up in any derivatives37. To this end, let’s make the substitution \\[\n\\begin{align*}\n\\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial}{\\partial x_i}\\left[ p(x,t)g_{ik}(x,t)g_{jk}(x,t) \\frac{\\partial q(x,t;u,s)}{\\partial x_j}\\right]\n=& \\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial^2}{\\partial x_i\\partial x_j}[p(x,t,u,s) g_{ik}(x,t)g_{jk}(x,t)]\\\\\n& -\\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial}{\\partial x_i}\\left[ q(x,t;u,s)\\frac{\\partial }{\\partial x_j}\\left(p(x,t)g_{ik}(x,t)g_{jk}(x,t)\\right) \\right].\n\\end{align*}\n\\] With this substitution the second order terms are \\[\n\\sum_{i=1}^d\\frac{\\partial}{\\partial x_i}\\left[ p(x,t,u,s) h(x,t)\\right] - \\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial^2}{\\partial x_i\\partial x_j}[p(x,t,u,s) g_{ik}(x,t)g_{jk}(x,t)],\n\\] where \\[\nh(x,t) = \\frac{1}{p(x,t)}\\sum_{j,k=1}^d\\frac{\\partial}{\\partial x_j}\\left[p(x,t)g_{ik}(x,t)g_{jk}(x,t))\\right].\n\\]\nIf we write \\[\n[\\bar{f}(x,t)]_i = f(x,t) - h(x,t) = f(x,t) -  \\frac{1}{p(x,t)}\\sum_{j,k=1}^d\\frac{\\partial}{\\partial x_j}\\left[p(x,t)g_{ik}(x,t)g_{jk}(x,t))\\right],\n\\] we get the joint PDE \\[\n\\frac{\\partial p(x,t,u,s)}{\\partial t} = -\\sum_{i=1}^d\\frac{\\partial}{\\partial x_i}[p(x,t,u,s)\\bar{f}(x,t)] - \\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial^2}{\\partial x_i\\partial x_j}[p(x,t,u,s) g_{ik}(x,t)g_{jk}(x,t)].\n\\tag{2}\\]\nIn order to identify the reverse time diffusion, we are going to find the reverse time backward equation, which confusingly, is for \\[\nq(u,s; x,t) =\\frac{p(X_t = x, X_s =y))}{p(X_s =y)} =\\frac{p(x,t,s,y)}{p(u,s)}.\n\\] As \\(p(u,s)\\) is a constant in both \\(x\\) and \\(t\\), we can divide both sides of Equation 2 by it to get \\[\n\\frac{\\partial q(x,t;u,s)}{\\partial t} = -\\sum_{i=1}^d\\frac{\\partial}{\\partial x_i}[q(x,t;u,s)\\bar{f}(x,t)] - \\frac{1}{2}\\sum_{i,j,k=1}^d\\frac{\\partial^2}{\\partial x_i\\partial x_j}[q(x,t;u,s) g_{ik}(x,t)g_{jk}(x,t)].\n\\] where again \\(s&gt;t\\) and \\(s\\) and \\(y\\) are known.\nThis is the forward Kolmogorov equation for the time-reversed38 diffusion \\[\ndX_t = \\bar{f}(X_t, t)dt + g(X_t, t)d\\tilde{W}_t, \\qquad X_s = u,\n\\] where \\(d \\tilde{W}_t\\) is another white nose. Anderson (1982) shows how to connect the white noise \\(dW_t\\) that’s driving the forward dynamics with the white noise that’s driving the reverse dynamics \\(d\\tilde{W}_t\\), but that’s overkill for our present situation.\nIn the context of an OU process, we get the reverse equation \\[\ndX_t= -\\left[\\frac{1}{2} X_t + \\sigma^2 \\nabla  \\log p(X_t, t)\\right]\\,dt + \\sigma\\, dW_t,\n\\] where time runs backwards and I’ve used the formula for the logarithmic derivative.\nUnlike the forward process, the reverse process is the solution to a non-linear stochastic differential equation. In general, this cannot be solved in closed form and we need to use a numerical SDE solver to generate a sample.\nIt’s worth noting that the OU process is an overly simple cartoon of a diffusion model. In practice, \\(\\sigma = \\sigma_t\\) is usually an increasing function of time so the system injects more noise as the diffusion moves along. This changes some of the exact equations slightly, but you can still sample \\(X_t \\mid X_0\\) analytically for any \\(t\\) (as long as you choose a fairly simple function for \\(\\sigma_t\\)). There is a large literature on these choices and, to be honest, I can’t be bothered going through them here. But obviously if you want to implement a diffusion model yourself you should look this stuff up.\n\n\nEstimating the score\nThe reverse dynamics are driven by the score function \\[\ns_t(x) = \\nabla \\log(p(x,t)).\n\\] Typically, we do not know the density \\(p(x,t) = p(X_t= x \\mid X_0 = x_0)\\) and while we could solve the forward equation in order to estimate it, that is wildly inefficient in high dimensions.\nIf we can assume that for each \\(t\\), \\(X_t \\mid X_0=x_0\\) is approximately \\(N(\\mu_t, \\Sigma_t)\\), then the resulting reverse diffusion is linear \\[\ndX_t = \\left[\\Sigma_t^{-1}\\mu_t -\\left(\\frac{1}{2} I + \\sigma^2\\Sigma_t^{-1} \\right)X_t\\right]dt + \\sigma dW_t, \\qquad X_T = u.\n\\] In this case \\(X_t \\mid X_T = u\\) is Gaussian with a mean and covariance that has closed form solution in terms of \\(\\Sigma_t\\) and \\(\\mu_t\\) (perhaps after some numerical quadrature and matrix exponentials).\nUnfortunately, as discussed above this is not true. A better approximation would be a mixture of Gaussians but, in general, we can use any method to approximate \\[\ns_t(x,t).\n\\] There are no particular constraints on it, except we expect it to be fairly smooth39 in both \\(t\\) and \\(x\\). Hence, we can just learn the score.\nAs we are going to solve the SDE numerically, we only need to estimate the score at a finite set of locations. In every application that I’ve seen, these are pre-specified, however it would also be possible to use a basis function expansion to interpolate to arbitrary time points. But, to be honest, I think every single example I’ve seen just uses a regularly spaced grid.\nSo how do we estimate \\(s_t\\)? Well just like every other situation, we need to define a likelihood (or, I guess, an optimisation criterion). One way to think about this would be to note that you’ll never perfectly recover the initial signal. This is because we need to solve a non-linear stochastic partial differential equation and there will, inherently, be noise in that solution. So instead, assume that we have an initial sample \\(x_0 \\sim p(X_0)\\) and that after solving the backward equation we have an unbiased estimator of \\(x_0\\) with standard deviation \\(\\tau_N\\), where \\(N\\) is the number of time steps. We know a lot about how the error of SDE solvers scale with \\(N\\) and so we can use that to set an appropriate scale for \\(\\tau_N\\). For instance, if you’re using the Euler–Maruyama method, then it has strong order \\(1/2\\) and \\(\\tau_N = \\mathcal{O}(N^{-1/2})\\) would likely be an appropriate scaling.\nThis strongly suggests a likelihood that looks like \\[\n\\hat{X}_0(x_0, t) \\mid s_t, x_0, t \\sim N(x_0, \\tau_N^2),\n\\] where \\(\\hat{X}_0(x_0,t)\\) is the estimate of \\(X_0\\) you get by running the reverse diffusion conditioned on \\(\\hat{X}_t = X_t(x_0)\\), where \\(X_t(x_0)\\) is an exact sample at time \\(t\\) from the forward diffusion started at \\(X_0 = x_0\\).\nThis is the key to the success of diffusion models: given our training sample \\(\\{x_0^{(i)}\\}_{i=1}^n\\), we generate new data \\(x_t(x_0)\\) and we can generate as much of that data as we want. Furthermore, we can choose any set of \\(t\\)s we want. We can sample a single \\((t, x_0)\\) pair multiple times or we can look at a diversity of sampling data.\nWe can even try to recover an intermediate state \\(\\hat{X}_{t_1}(x_0,t_2)\\) from information about a future state \\(X_{t_2}(x_0)\\), \\(t_2 &gt;t_1 \\geq 0\\). This gives us quite the opportunity to target our learning to areas of the \\((t,x)\\) space where we have relatively poor estimates of the score function.\nOf course, that’s not what people do. They do stochastic gradient descent to minimise \\[\n\\min_{s_t}\\mathbb{E}_{x_0 \\sim p(X_0), t \\sim \\text{Unif}[0,1]}\\left(\\|x_0 - \\hat{X}_0(x_0,t)\\|^2\\right)\n\\] possibly subject to some penalties on \\(s_t\\). In fact, the distribution on \\(t\\) is usually a discrete uniform. As with any sufficiently complex task, there is a lot of detailed work on exactly how to best parameterise, solve, and evaluate this optimisation procedure.\n\n\nGenerating samples\nOnce the model is trained and we have an estimate \\(\\hat{s}_t\\) of the score function, we can generate new samples by first sampling \\(u \\sim N(0, \\sigma^2)\\) and running the reverse diffusion starting from \\(X_t = u\\) for some sufficiently large \\(t\\). One of the advantages of using a variant of the OU process with a non-constant \\(\\sigma\\) is that we can choose \\(t\\) to be smaller. Nevertheless, there will always be a little bit of error introduced by the fact that \\(X_t\\) is only approximately \\(N(0, \\sigma^2)\\). But really, in the context of all of the other errors, this one is pretty small.\nAnyway, run the diffusion backwards and if you’ve estiamted \\(s_t(x)\\) well for the entire trajectory, you will get something that looks a lot like a new sample from \\(p(X_0)\\)."
  },
  {
    "objectID": "posts/2023-01-30-diffusion/diffusion.html#some-closing-thoughts",
    "href": "posts/2023-01-30-diffusion/diffusion.html#some-closing-thoughts",
    "title": "Diffusion models; or Yet another way to sample from an arbitrary distribution",
    "section": "Some closing thoughts",
    "text": "Some closing thoughts\nSo there you have it, a very high-level mathematical introduction to diffusion models. Along the way, I accidentally put them in some sort of historical context, which hopefully helped make some things clearer.\nObviously there are a lot of cool things that can happen. The ability to, essentially, design our training trajectories should definitely be utilised. To do that, we would need some measure of uncertainty in the recovery of \\(s_t\\). A possible way to do this would be to insert a probabilistic layer into neural net architecture. If this isn’t the final layer in the network, it should be possible to clean up any artifacts it introduces with further layers, but the uncertainty estimates from this hidden layer would still be indicative of the uncertainty in the recovery of the scores. Assuming, of course, that this is successful, it would be possible to target the training at improving the uncertainty.\nBeyond the possibility of using a non-uniform distribution for \\(t\\), these uncertainty estimates might also help indicate the reliability of the generated sample. If the reverse diffusion spends too much time in areas with highly uncertain scores, it is unlikely that the generated data will be a good sample.\nI am also somewhat curious about whether or not this type of system could be a reasonable alternative to bootstrap resampling in some contexts. I mean image creation is cool, but it’s not the only time people want to sample from a distribution that we only know empirically."
  },
  {
    "objectID": "posts/2023-01-30-diffusion/diffusion.html#footnotes",
    "href": "posts/2023-01-30-diffusion/diffusion.html#footnotes",
    "title": "Diffusion models; or Yet another way to sample from an arbitrary distribution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMaybe my favourite running gag was Ronny Chieng refusing to use the American pronunciation of Megan. ↩︎\nI mean, my last post was recounting literature on the Markov property from the 70s and 80s. My only desire for this blog is for it to be very difficult to guess the topic of the next post.↩︎\nI can’t stress enough that I made that tomato and feta tiktok pasta for dinner. Because that’s exactly how on trend I am.↩︎\nI am very much managing expectations here↩︎\nI cannot stress enough that this post will not help you implement a diffusion model. It might help you understand what is being implemented, but it also might not.↩︎\nReally fucking relative.↩︎\nFind a lesbian and follow her blog. Then you’ll get the good shit. There are tonnes of queer women in statistics. If you don’t know any it’s because they probably hate you.↩︎\nThe wokerati among you will notice that the quotient is the derivative of \\(\\log p(Q)\\).↩︎\nLook. I love you all. But I don’t want to introduce measure push-forwards. So if you want the maths read the damn paper.↩︎\nThis is the Knothe-Rosenblatt rearrangement of the optimal transport problem if you’re curious. And let’s face it, you’re not curious.↩︎\nThe normalising flow literature also has a lot of nice chats about how to model the \\(T_j\\)s using masked versions of the same neural net.↩︎\nIf you don’t have too much data, you could just replace that expectation with its empirical approximation. But when there is a lot of data, that will be expensive and stochastic gradient methods will perform better.↩︎\nAnd be more likely to appropriately use your computational resources↩︎\nWe will see later that it doesn’t matter if we model \\(T\\) or \\(S\\), but the likelihood calculations come out nicer if we map from \\(p(x)\\) to \\(q(u)\\) rather than the other way around↩︎\nThere is a tonne of excellent software for efficiently solving differential equations!↩︎\nMy notation here is a bit awkward. The \\(x\\) in \\(S(x,t)\\) is keeping track of the initial condition, which in this case we do not know. But hey. Whatever.↩︎\nPotentially even multi-modal↩︎\nClassically this is done with a penalty, but you could also do it with things like early stopping and specific representations of the function. Which is nice because the continuous nomalising flow people use neural nets↩︎\nThe square on the norm isn’t always there↩︎\nThis was a big-sexy area in optimisation.↩︎\nor at least a lot more expensive than, say, evaluating an exponential!↩︎\nIf you’re familiar with scalable ML methods, you might think well we have solved this problem. But I promise that it is not solved. The problem is that there’s no convenient analogue to subsampling the data. You can’t be half pregnant and you can’t half evaluate the forward map. There are, however, a pile of fabulous techniques that do their best to use multiple resolutions to get something that resembles a sensible MCMC scheme.↩︎\nIn our context, it’s a vector-valued function↩︎\nExamples abound, but they include image reconstruction, tomographic inversion, and really anything where you’re estimating diffusivity↩︎\nGaussian processes↩︎\nBut not necessarily too creative. Not every transformation of a penalty makes a sensible prior. I’m looking at you lasso on increments.↩︎\nUsing the “well known” fact that the derivative of the log-determinant is the trace ↩︎\nThere are some complexities in practice around computing that trace. A straightforward implementation would require \\(d\\) autodiff sweeps, which would make the model totally impractical. There are basically two options: massively simplify \\(f\\) to be something like \\(f(x) = h(Ax + b)\\) for a smooth function \\(h\\) or use a stochastic trace estimator.↩︎\nMeasured in the Frobenius norm, of course↩︎\nor priors↩︎\ndata + distributional assumptions = information↩︎\n\\(q\\) will be the asymptotic distribution of the diffusion, but it isn’t achieved at finite time.↩︎\nArguably, gradient descent is to machine learners what arse crack is to roadies. It’s always present, but with just enough variation to make it interesting.↩︎\nTechnically it’s an Ito integral, but because the integrand is deterministic it reduces to a white noise integral↩︎\nThe Markov property implies that \\(p(X_{t_1}, X_{t_2}\\mid X_0 = x) = p(X_{t_1}\\mid X_0 = x)p(X_{t_2} \\mid X_{t_1})\\). ↩︎\nLipschitz and bounded↩︎\nI hate the quotient rule↩︎\nThis is why the signs don’t seem to match the forwards equation from before, but you can convince yourself if you do the change of variables \\(\\tau = s - t\\), the new variable \\(\\tau\\) runs forward in time and \\(\\bar{f}\\) switches signs, which gives the right forwards equations (with different signs on the first and second order terms) in \\((\\tau,x)\\).↩︎\nIf the \\(p(X_0)\\) is very rough, then, for very small \\(t\\), \\(p(x,t)\\) will also be quite rough but it will quickly become infinitely differentiable. It turns out that mathematicians know quite a lot about parabolic equations!↩︎"
  },
  {
    "objectID": "posts/2021-10-15-priors3/priors3.html",
    "href": "posts/2021-10-15-priors3/priors3.html",
    "title": "Priors: Fire With Fire (Track 3)",
    "section": "",
    "text": "It is Friday night, I am in lockdown, and I have had a few drinks. So let’s talk about objective priors.\nThe first and most obvious thing is that they are not fucking objective. It is bad/unethical marketing from the 90s that has stuck. I dislike it. I think it’s unethical (and, personally, immoral) to proclaim a statistical method objective in any context, let alone one in which all you did was compute some derivatives and maybe sent something that isn’t going to fucking infinity to infinity. It’s fucking trash and I hate it.\nBut let’s talk about some objective priors."
  },
  {
    "objectID": "posts/2021-10-15-priors3/priors3.html#ok-lets-try-again.",
    "href": "posts/2021-10-15-priors3/priors3.html#ok-lets-try-again.",
    "title": "Priors: Fire With Fire (Track 3)",
    "section": "Ok let’s try again.",
    "text": "Ok let’s try again.\nNo I do not think I will.\nI am willing to talk about finite dimensional priors that add minimal information or are otherwise related to MLEs.\nLater, I guess because I’m mathematically interested in it, I’ll talk about the infinite dimensional case. But not today. Because I’m pissed off.\nAnyway.\n\nWhat is an objective prior\nHonestly, still a pretty vague and stupid concept. It is difficult to define for interesting (aka not univariate) cases, but maybe the most practical definition is priors that come from rules.\nBut that’s not a … great definition. Many priors that I will talk about over the next little while could probably be shoved under the objective banner. But in this post I’m going to talk about the OG concept of an objective prior: the type of priors that try to add minimal information beyond the data.\nNominally, the aim of these priors is to let the data speak for itself. And I’ve been doing this a while, and no matter how long I’ve listened to my .csv file, it has never said a word. But if it weren’t for silly justifications, we wouldn’t have silly concepts.\nThere are, essentially, three main types of priors that fall into this traditionally objective category:\n\nJeffreys priors\nReference priors\nMatching priors\n\nJefferys priors argue, for a bunch of very sensible and solid geometrical reasons, that the value of the parameter \\(\\theta\\) is less important than the way that moving from \\(\\theta\\) to \\(\\theta + d\\theta\\) will change the likelihood \\(p(y \\mid \\theta)\\). This push back against the Arianist notion that the prior can be separated from its context is welcome!\nThe actual prior itself comes from, I guess, the idea that the prior should be invariant to reparameterisations and after some maths you get \\[\np(\\theta ) \\propto |I(\\theta)|^{1/2},\n\\] where \\(|I(\\theta)|\\) is the determinant of the Fisher1 information matrix \\[\nI(\\theta)_{ij} = \\frac{\\partial^2}{\\partial \\theta_i \\theta_j} \\log p(y \\mid \\theta).\n\\]\nThis immediately turns out to be a terrible idea for general models. When \\(\\theta\\) has more than one component, the Jeffreys prior tends to concentrate in silly places in the parameter space.\nBut when \\(\\theta\\) is one dimensional, it works fine. In fact, if you use it you will get the sampling distribution Maximum Likelihood estimator (or withing \\(\\mathcal{O}_p(n^{-1})\\) of it). So there’s very little purpose pursing this line of reasoning.\nThere’s actually a bit of a theme that develops here: for models with a single parameter a lot of things work perfectly. Sadly the intersection of one-dimensional statistical models that are regular enough for all this maths to work and interesting statistical problems is not exactly hefty.\nReference priors are an attempt to extend Jeffreys priors to multiple parameters while avoiding some of the more egregious problems of multivariate Jeffreys priors. They were also the topic of the most boring talk I have ever seen at a conference2. It was 45 minutes going through all of the different reference priors you can make for inferring a bivariate normal (you see, to construct a reference prior you need to order your parameters and this ordering matters). If I didn’t already think that reference priors were an impractical waste of time, that certainly convinced me. A lot of people seem to mention reference priors, but it is rarer to see them in use.\nMatching priors try to spin off Jeffreys priors in a different direction. They are a mathematically very interesting idea asking if there is a prior that will produce a posterior uncertainty interval that is exactly the same as (or very close to) the sampling distribution of the MLE. It turns out that for one parameter models you can totally do this (the Jeffreys prior does it! And you can get even closer). But when there are nuisance parameters (aka parameters that aren’t of direct inferential interest but are important to modelling the data), the resulting prior tends to be data-dependent. A really nice example of the literature is Reid, Mukerjee, and Fraser’s 2003 paper. To some extent the matching priors literature is asking “should we even Bayes?”, which is not the worst question to ask3.\nThese three ideas have a number of weird bastard children. Most of these are not recommended by anyone, but used prominently. These are the vague priors. The \\(N(0,100^2)\\) priors. The \\(\\text{Inverse-Gamma}(\\epsilon, \\epsilon)\\) priors. The Uniform over large interval priors. The misinformed concept behind these priors is that wider prior = less information. This is, of course, bullshit. As many4 many5 many6 examples show.\nThe one big thing that I haven’t mentioned so far is that most of the time the priors produced using these methods are not proper, which is to say that you can’t integrate them. That isn’t a big deal mathematically as long as \\(\\int_\\Theta p(y \\mid \\theta)p(\\theta)\\,d\\theta\\) is finite for all7 data sets \\(y\\). This is a fairly difficult thing to check for most models and if you want to really upset a grad student at a Bayesian conference spend some time staring at their poster and then grimace and ask “are you sure that posterior is proper?”8 The frequent impropriety of these classes of means you can’t simulate from them, can’t really consider them a representation of prior information, and can’t easily transfer them from one problem to another without at least a little bit of fear that the whole house of cards is gonna come tumbling down.\n\n\nWho uses objective priors\nFrequentists. People who are obsessed with statistical bias of their estimators (the Venn diagram here isn’t a circle, but it’s also not the poster child for diversity of thought or modernity). People who read boring textbooks. People who write boring textbook. People who believe that it’s the choice of prior and somehow not the choice of the likelihood or, you know, their choice of data that will somehow lead to incorrect inferences9. People who tell you, without being asked10, that they went to Duke.\n\n\nShould I use objective priors\nIf you’ve more parameters than a clumsy butcher has fingers on their non-dominant hand, you probably shouldn’t use objective priors. In these cases, you almost always need to inject some form of regularisation, prior information, or just plain hope into your model to make it behave sensibly11.\nBut if you have less, I mean, live your life I guess. But why go to the effort. Just compute a maximum likelihood if you’re looking for something that is very very similar to a maximum likelihood estimate. It’s faster, it’s cleaner, and it’s not pretending to be something it isn’t.\nI actually think you can usually make stronger, more explicitly justified choices using other things we can talk about later. But I’m not the boss of statistics so you don’t have to listen to me."
  },
  {
    "objectID": "posts/2021-10-15-priors3/priors3.html#footnotes",
    "href": "posts/2021-10-15-priors3/priors3.html#footnotes",
    "title": "Priors: Fire With Fire (Track 3)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\newwwwwww↩︎\nThis is a very high bar. I have seen (and given) a lot of very very very dull talks. But this is the one that sticks in my mind.↩︎\nIf it seems like I like matching priors more than the other two, I do.↩︎\nhttps://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-3/Prior-distributions-for-variance-parameters-in-hierarchical-models-comment-on/10.1214/06-BA117A.full↩︎\nPut a wide normal prior on the logit mean, simulate and back transform. Explain how that is uninformative.↩︎\nSome people love a wide uniform distribution \\(\\text{Unif}(0,U)\\) on the degrees of freedom of a student-t distribution. As \\(U\\) increases, you are putting more and more prior mass on the \\(t\\) distribution being very close to a normal distribution. Oops.↩︎\nor all after some minimal restrictions↩︎\nAllegedly, Jim Berger’s wife, who is not a statistician but was frequently at conferences, used to do this.↩︎\nLike seriously. I don’t want to repeat that old canard that the choice of likelihood is as subjective as the choice of prior because a) Arianism and b) the choice of likelihood is a waaaaaaaaaay more important subjective modelling choice than the choice of prior in all but the most outre circumstances!↩︎\nI promise I have never asked and I will never ask.↩︎\nThere are ideas of objective priors in these cases, which we will talk about later, but these usually take the form of priors that guarantee optimal frequentist behaviour. And again, there is often another way to get that.↩︎"
  },
  {
    "objectID": "posts/2021-10-14-priors2/priors2.html",
    "href": "posts/2021-10-14-priors2/priors2.html",
    "title": "Priors: Whole New Way (Track 2)",
    "section": "",
    "text": "If we’re going to talk about priors, let’s talk about priors. And let’s talk about the most prior-y priors in the whole damn prior universe. Let’s talk about conjugate priors."
  },
  {
    "objectID": "posts/2021-10-14-priors2/priors2.html#ok.-maybe-we-should-try-again",
    "href": "posts/2021-10-14-priors2/priors2.html#ok.-maybe-we-should-try-again",
    "title": "Priors: Whole New Way (Track 2)",
    "section": "Ok. Maybe we should try again",
    "text": "Ok. Maybe we should try again\nDeep breaths. You soul is an island of positivity.\n\nWhat is a conjugate prior?\nConjugate priors are wild and fabulous beasts. They roam the strange, mathematic plains and live forever in our dreams of a better future.\nToo much?\nOK.\nConjugate priors are a mathematical curiosity that occasional turn out to be slightly useful.\nA prior distribution \\(p(\\theta)\\) is conjugate to the likelihood1 \\(p(y \\mid \\theta)\\) if the posterior distribution \\(p(\\theta \\mid y)\\) is in the same distributional family as the prior.\nMoreover, there is an rule to update the parameters in the prior to get the parameters in the posterior based on some simple summaries of the data. This means that you can simply write the posterior down as a specific distribution that you can2 easily sample from and get on with your life.\nReally, it seems like a pretty good thing. But there is, unsurprisingly, a hitch: almost no likelihoods have conjugate priors. And if you happen to have a model with a nice3 conjugate prior then good for you, but if you modify your model even slightly, you will no longer have one.\nThat is, the restriction to conjugate priors is a massive restriction on your entire model.\n\n\nWho uses conjugate priors?\nConjugate priors are primarily used by two types of people:\n\nPeople who need to write exam questions for undergraduate Bayesian statistics courses4,\nPeople who need to implement a Gibbs sampler and don’t want to live through the nightmare5 that is Metropolis-within-Gibbs.\n\nFor the most part, we can ignore the first group of people as a drain on society.\nThe second group is made up of:\n\npeople who are using software that forces it on them. And like we don’t all have time to learn new software6. Leave, as hero7 Chris Crocker, Britney alone.\npeople who are writing their own Gibbs sampler. Annie Lennox said it best: Why-y-y-y-y-y-y-y-y-y-y? For a very large variety of problems, you do not have to do this8. The exception is when you have a discrete parameter in your model that you can’t marginalise out9, like an exponential random graph model or something equally hideous. Thankfully, a lot of work in machine learning has expanded the options for Bayesian and pseudo-semi-kinda Bayesian10 estimation of these types of models. Anyway. Discrete parameters are disgusting. I am tremendously indiscrete.\n\nThe third type are the odd ducks who insist that because the posterior and the prior being in the same family means that the prior can be interpreted as the outcome of Bayesian analysis on a previous experiment. Instead of the much more realistic way of arriving at a conjugate prior where you find yourself waking up alone in a bathtub full of ice and using an \\(\\text{Inverse-Gamma}(1/2, 0.0005)\\) prior on the variance (which is conjugate for a Gaussian likelihood) because some paper from 199511 told you it was a good choice.\n\n\nShould I use conjugate priors?\nThere is actually one situation where they can be pretty useful. If your parameter space breaks down into \\(\\theta = (\\eta, \\phi)\\), where \\(\\eta\\) is a high-dimensional variable, then if \\(p(y \\mid \\theta) = p(y \\mid \\eta)\\) and \\(p(\\eta \\mid \\phi)\\) is conjugate for \\(p(y \\mid \\eta)\\), then a magical thing happens: you can compute \\(p(\\eta \\mid y, \\phi)\\) explicitly (using the conjugate property) and then you can greatly simplify the posterior as \\(p(\\theta\\mid y ) = p(\\eta \\mid y, \\phi) p(\\phi \\mid y)\\), where12 \\[\np(\\phi \\mid y) = \\frac{p(y \\mid \\eta)p(\\eta \\mid \\phi)p(\\phi)}{p(y) p(\\eta \\mid y, \\phi)} \\propto \\left.\\frac{p(y \\mid \\eta)p(\\eta \\mid \\phi)p(\\phi)}{p(\\eta \\mid y, \\phi)}\\right|_{\\eta = \\text{anything}},\n\\] where every term on the right hand side is able to be calculated13. Even if this doesn’t have a known distribution form, it is much much lower-dimensional than the original problem and much more amenable to MCMC or possibly deterministic integration methods.\nThis really does feel a bit abstract, so I will give you the one case where I know it’s used very commonly.This is the case where \\(y \\sim N(A\\eta, R)\\) and14 \\(\\eta \\mid \\phi \\sim N(0, \\Sigma(\\phi))\\), where \\(\\Sigma(\\phi)\\) is a covariance matrix and \\(A\\) is a matrix (the dimension of \\(\\eta\\) is often higher than the dimension of \\(y\\)).\nThis is an example of a class of models that occur constantly in statistics: Håvard Rue15 calls them Latent Gaussian models. They basically extend16 (geostatistical)? linear|additive (mixed)? models. So for all of these models, we can explicitly integrate out the high-dimensional Gaussian component, which makes inference a breeze17.\nIt gets slightly better than that because if you combine this observation with a clever asymptotic approximation, you get an approximately conjugate model and can produce Laplace approximations, nested Laplace approximations18, and Integrated Nested Laplace approximations19, depending on how hard you are willing to work.\n\n\nA conclusion, such as it is\nYes we have drifted somewhat from the topic, but that’s because the topic is boring.\nConjugate priors are mostly a mathematical curiosity and their role in Bayesian statistics is inexplicably inflated20 to make them seem like a core topic. If you never learn about conjugate priors your Bayesian education will not be lacking anything. It will not meaningfully impact your practice. But even stopped clocks are right 2-3 times a day21"
  },
  {
    "objectID": "posts/2021-10-14-priors2/priors2.html#footnotes",
    "href": "posts/2021-10-14-priors2/priors2.html#footnotes",
    "title": "Priors: Whole New Way (Track 2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLike all areas of Bayesian statistics, conjugate priors push back against the notion of Arianism.↩︎\noften, but not always↩︎\nChristian Robert’s book The Bayesian Choice has an example where a model has a conjugate prior but it doesn’t normalise easily.↩︎\nOr, in my case, it’s explicitly listed on the syllabus.↩︎\nNot a nightmare.↩︎\nI’m equally likely to learn Julia and Stata. Which is is to say I’m tremendously unlikely to put the effort in to either. I wish them both well. Live your life and let me live mine.↩︎\nI have not fact checked this recently, and we all know white gays sometimes go bad. But he started from a good place, so I’m sure it’s fine.↩︎\nThere is pedagogical value in learning how MCMC methods work by implementing them yourself. But girl it is 2021. Go fuck with a bouncy particle sampler or something. Live your live out loud! Young Bayesians run free↩︎\nOften, like with mixture models or hidden markov models, you can eg https://mc-stan.org/docs/2_22/stan-users-guide/latent-discrete-chapter.html↩︎\nThe difference between these things is pretty slight in the usual situation where your MCMC scheme doesn’t explore the space particularly well. I’m not of the opinion that you either explore the full posterior or you don’t use the model. Most of the time you do perfectly fine with approximate exploration or, at least, you do as well as anything else will.↩︎\nBERNARDINELLI, L., CLAYTON, D. and MONTOMOLI, C. (1995). Bayesian estimates of disease maps: How important are priors? Stat. Med. 14 2411–2431.↩︎\nMultiply both sides of the first equation by the denominator and it’s equivalent to \\(p(y, \\eta, \\phi) = p(y, \\eta, \\phi)\\), which is tautologically true.↩︎\nThe constant of proportionality does not depend on \\(\\eta\\). All of the \\(\\eta\\) parts cancel!↩︎\nThe mean doesn’t have to be zero but you can usually make it zero using … magic.↩︎\nFamous for INLA↩︎\napologies for the regexp.↩︎\nSee also Rasmussen and Williams doing marginal inference with GPs. Exactly the same process.↩︎\nhttps://arxiv.org/abs/2004.12550↩︎\nhttps://www.r-inla.org↩︎\nI assume this is so people don’t need to update their lecture notes.↩︎\ndaylight savings time fades the curtains and wreaks havoc with metaphors.↩︎"
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "",
    "text": "Eliza knows a little something about monkeys. This will become relevant in a moment.\nIn about 2016, Almeling et al. published a paper that suggested aged Barbary macaques maintained interest in members of their own species while losing interest in novel non-social stimuli (eg toys or puzzles with food inside).\nThis is where Eliza—who knows a little something about monkeys—comes into frame: this did not gel with her experiences at all.\nSo Eliza (and Mark1 2, who also knows a little something about monkeys) decided to look into it."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#what-are-the-stakes-according-to-the-papers-not-according-to-me-who-knows-exactly-nothing-about-this-type-of-work",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#what-are-the-stakes-according-to-the-papers-not-according-to-me-who-knows-exactly-nothing-about-this-type-of-work",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "What are the stake?s (According to the papers, not according to me, who knows exactly nothing3 about this type of work)",
    "text": "What are the stake?s (According to the papers, not according to me, who knows exactly nothing3 about this type of work)\nA big motivation for studying macaques and other non-human primates is that they’re good models of humans. This means that if there was solid evidence of macaques becoming less interested in novel stimuli as they age (while maintaining interest in people), this could suggest an evolutionary reason from this (commonly observed) behaviour in humans.\nSo if this result is true, it could help us understand the psychology of humans as they age (and in particular, the learned vs evolved trade off they are making)."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#so-what-did-eliza-and-mark-do",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#so-what-did-eliza-and-mark-do",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "So what did Eliza and Mark do?",
    "text": "So what did Eliza and Mark do?\nThere are a few things you can do when confronted with a result that contradicts your experience: you can complain about it on the Internet, you can mobilize a direct replication effort, or you can conduct your own experiments. Eliza and Mark opted for the third option, designing a conceptual replication.\nDirect replications tell you more about the specific experiment that was conducted, but not necessarily more about the phenomenon under investigation. In a study involving aged monkeys4, it’s difficult to imagine how a direct replication could take place.\nOn the other hand, a conceptual replication has a lot more flexibility. It allows you to probe the question in a more targeted manner, appropriate for incremental science. In this case, Eliza and Mark opted to study only the claim that the monkeys lose interest in novel stimuli as they age (paper here). They did not look into the social claim. They also used a slightly different species of macaque (M. mulatta rather than M. butterfly). This is reasonable insofar as understanding macaques as a model for human behaviour."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#what-does-the-data-look-like",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#what-does-the-data-look-like",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "What does the data look like?",
    "text": "What does the data look like?\nThe experiment used 2435 monkeys aged between 4 and 30 and gave them a novel puzzle task (opening a fancy tube with food in it) for twenty minutes over two days. The puzzle was fitted with an activity tracker. Each monkey had two tries at the puzzle over two days. Monkeys had access to the puzzle for around6 20 minutes.\nIn order to match the original study’s analysis, Eliza and Mark divided the first two minutes into 15 second intervals and counted the number of intervals where the monkey interacted with the puzzle. They also measured the same thing over 20 minutes in order to see if there was a difference between short-term curiosity and more sustained exploration.\nFor each monkey, we have the following information:\n\nMonkey ID\nAge (4-30)\nDay (one or two)\nNumber of active intervals in the first two minutes (0-8)\nNumber of active intervals in the first twenty minutes (0-80)\n\nThe data and their analysis are freely7 available here.\n\nlibrary(tidyverse)\nacti_data &lt;- read_csv(\"activity_data.csv\") \nactivity_2mins &lt;- acti_data |&gt;\n  filter(obs&lt;9) |&gt; group_by(subj_id, Day) |&gt;\n  summarize(total=sum(Activity), \n            active_bins = sum(Activity &gt; 0), \n            age = min(age)) |&gt;\n  rename(monkey = subj_id, day = Day) |&gt;\n  ungroup()\n\nactivity_20minms80 &lt;- acti_data |&gt; filter(obs&lt;81) |&gt;\n  group_by(subj_id, Day) |&gt;\n  summarize(total=sum(Activity), \n            active_bins = sum(Activity &gt; 0), \n            age = min(age)) |&gt;\n  rename(monkey = subj_id, day = Day) |&gt;\n  ungroup()\n\nglimpse(activity_20minms80)\n\nRows: 485\nColumns: 5\n$ monkey      &lt;dbl&gt; 0, 0, 88, 88, 636, 636, 760, 760, 1257, 1257, 1607, 1607, …\n$ day         &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2…\n$ total       &lt;dbl&gt; 9881, 6356, 15833, 4988, 572, 308, 1097, 2916, 4884, 2366,…\n$ active_bins &lt;int&gt; 42, 34, 43, 19, 10, 4, 12, 23, 50, 33, 9, 11, 13, 7, 30, 3…\n$ age         &lt;dbl&gt; 29, 29, 29, 29, 28, 28, 30, 30, 27, 27, 27, 27, 27, 27, 26…"
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#ok-mary-how-are-we-going-to-analyze-this-data",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#ok-mary-how-are-we-going-to-analyze-this-data",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "Ok Mary, how are we going to analyze this data?",
    "text": "Ok Mary, how are we going to analyze this data?\nEliza and Mark’s monkey data is an example of a fairly common type of experimental data, where the same subject is measured multiple times. It is useful to break the covariates down into three types: grouping variables, group-level covariates, and individual-level covariates.\nGrouping variables indicate what group each observation is in. We will see a lot of different ways of defining groups as we go on, but a core idea is that observations within a group should conceptually more similar to each other than observations in different groups. For Eliza and Mark, their grouping variable is monkey. This encodes the idea that different monkeys might have very different levels of curiosity, but the same monkey across two different days would probably have fairly similar levels of curiosity.\nGroup-level covariates are covariates that describe a feature of the group rather than the observation. In this example, age is a group-level covariate, because the monkeys are the same age at each observation.\nIndividual-level covariates are covariates that describe a feature that is specific to an observation. (The nomenclature here can be a bit confusing: the “individual” refers to individual observations, not to individual monkeys. All good naming conventions go to shit eventually.) The individual-level covariate is experiment day. This can be a bit harder to see than the other designations, but it’s a little clearer if you think of it as an indicator of whether this is the first time the monkey has seen the task or the second time. Viewed this way, it is very clearly a measurement of an property of an observation rather than of a group.\nEliza and Mark’s monkey data is an example of a fairly general type of experimental data where subjects (our groups) are given the same task under different experimental conditions (described through individual-level covariates). As we will see, it’s not uncommon to have much more complex group definitions (that involve several grouping covariates) and larger sets of both group-level and individual-level covariates.\nSo how do we fit a model to this data.\n\nThere are just too many monkeys; or Why can’t we just analyse this with regression?\nThe temptation with this sort of data is to fit a linear regression to it as a first model. In this case, we are using grouping, group-level, and individual-level covariates in the same way. Let’s suck it and see.\n\nlibrary(broom)\nfit_lm &lt;- lm(active_bins ~ age*factor(day) + factor(monkey), data = activity_2mins)\n\ntidy(fit_lm) \n\n\n  \n\n\n\nSo the first thing you will notice is that that is a lot of regression coefficients! There are 243 monkeys and 2 days, but only 485 observations. This isn’t enough data to reliably estimate all of these parameters. (Look at the standard errors for the monkey-related coefficients. They are huge!)\nSo what are we to do?\nThe problem is the monkeys. If we use monkey as a factor variable, we only have (at most) two observations of each factor level. This is simply not enough observations per to estimate a different intercept for each monkey!\nThis type of model is often described as having no pooling, which indicates that there is no explicit dependence between the intercepts for each group (monkey). (There is some dependence between groups due to the group-level covariate age.)\n\n\nIf we ignore the monkeys, will they go away? or Another attempt at regression\nOur first attempt at a regression model didn’t work particularly well, but that doesn’t mean we should give up8. A second option is that we can assume that there is, fundamentally, no difference between monkeys. If all monkeys of the same age have similar amounts of interest in new puzzles, this would be a reasonable assumption. The best case scenario is that not accounting for differences between individual monkeys would still lead to approximately normal residuals, albeit with probably a larger residual variance.\nThis type of modelling assumption is called complete pooling as it pools the information between groups by treating them all as the same.\nLet’s see what happens in this case!\n\nfit_lm_pool &lt;- lm(active_bins ~ age*factor(day), data = activity_2mins)\nsummary(fit_lm_pool)\n\n\nCall:\nlm(formula = active_bins ~ age * factor(day), data = activity_2mins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5249 -1.5532  0.1415  1.6731  4.1884 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.789718   0.344466  11.002   &lt;2e-16 ***\nage              0.003126   0.021696   0.144    0.885    \nfactor(day)2     0.056112   0.488818   0.115    0.909    \nage:factor(day)2 0.025170   0.030759   0.818    0.414    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.103 on 481 degrees of freedom\nMultiple R-squared:  0.01365,   Adjusted R-squared:  0.0075 \nF-statistic: 2.219 on 3 and 481 DF,  p-value: 0.0851\n\n\nOn the up side, the regression runs and doesn’t have too many parameters!\nThe brave and the bold might even try to interpret the coefficients and say something like there doesn’t seem to be a strong effect of age. But there’s real danger in trying to interpret regression coefficients in the presence of a potential confounder (in this case, the monkey ID). And it’s particularly bad form to do this without ever looking at any sort of regression diagnostics. Linear regression is not a magic eight ball.\nLet’s look at the diagnostic plots.\n\nlibrary(broom)\naugment(fit_lm_pool) |&gt; \n  ggplot(aes(x = .fitted, y = active_bins - .fitted)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_classic()\n\n\n\n\n\n\n\naugment(fit_lm_pool) |&gt; ggplot(aes(sample = .std.resid)) + \n  stat_qq() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + \n  theme_classic()\n\n\n\n\n\n\n\n\nThere are certainly some patterns in those residuals (and some suggestion that the error need a heavier tail for this model to make sense)."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#what-is-between-no-pooling-and-complete-pooling-multilevel-models-thats-what",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#what-is-between-no-pooling-and-complete-pooling-multilevel-models-thats-what",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "What is between no pooling and complete pooling? Multilevel models, that’s what",
    "text": "What is between no pooling and complete pooling? Multilevel models, that’s what\nWe are in a Goldilocks situation: no pooling results in a model that has too many independent parameters for the amount of data that we’ve got, while complete pooling has too few parameters to correctly account for the differences between the monkeys. So what is our perfectly tempered porridge9?\nThe answer is to assume that each monkey has its own intercept, but that it’s intercept can only be so far from the overall intercept (that we would’ve gotten from complete pooling). There are a bunch of ways to realize this concept, but the classical method is to use a normal distribution.\nIn particular, if the \\(j\\)th monkey has observations \\(y_{ij}\\), \\(i=1,2\\), then we can write our model as \\[\ny_{ij}  \\sim N(\\mu_j + \\beta_\\text{age}\\, \\text{age}_j + \\beta_\\text{day}\\, \\text{day}_{ij} + \\beta_\\text{age,day}\\, \\text{[age*day]}_{ij}, \\sigma^2).\n\\]\nThe effects of age and day and the data standard deviation (\\(\\sigma\\)) are just like they’d be in an ordinary linear regression model. Our modification comes in how we treat the \\(\\mu_j\\).\nIn a classical linear regression model, we would fit the \\(\\mu_j\\)s independently, perhaps with some weakly informative prior distribution. But we’ve already discussed that that won’t work.\nInstead we will make the \\(\\mu_j\\) exchangeable rather than independent. Exchangeability is a relaxation of the independence assumption to say instead encode that we have no idea which of the intercepts will do what. That is, if we switch around the labels of our intercepts the prior should not change. There is a long and storied history of exchangeable models in statistics, but the short version that is more than sufficient for our purposes is that they usually10 take the form \\[\\begin{align*}\n\\mu_j \\mid \\tau \\stackrel{\\text{iid}}{\\sim} &p(\\mu_j \\mid \\tau), \\qquad i = 1,\\ldots, J \\\\\n\\tau \\sim & p(\\tau).\n\\end{align*}\\]\nIn a regression context, we typically assume that \\[\n\\mu_j \\mid \\tau \\sim N(\\mu, \\tau^2)\n\\] for some \\(\\mu\\) and \\(\\tau\\) that will need their own priors.\nWe can explore this difference mathematically. The regression model, which assumes independence of the \\(\\mu_j\\), uses \\[\np(\\mu_1, \\ldots, \\mu_J) = \\prod_{j=1}^J N(\\mu, \\tau_\\text{fixed}^2)\n\\] as the joint prior on \\(\\mu_1,\\ldots,\\mu_J\\). On the other hand, the exchangeable model, which forms the basis of multilevel models, assumes the joint prior \\[\np(\\mu_1, \\ldots, \\mu_J) = \\int_0^\\infty \\left(\\prod_{j=1}^J N(\\mu, \\tau^2)\\right)p(\\tau)\\,d\\tau,\n\\] for some prior on \\(p(\\tau)\\) on \\(\\tau\\).\nThis might not seem like much of a change, but it can be quite profound. In both cases, the prior is saying that each \\(\\mu_j\\) is, with high probability, at most \\(3\\tau\\) away from the overall mean \\(\\mu\\). The difference is that while the classical least squares formulation uses a fixed value of \\(\\tau\\) that needs to be specified by the modeller, while the exchangeable model lets \\(\\tau\\) adapt to the data.\nThis data adaptation is really nifty! It means that if the groups have similar means, they can borrow information from the other groups (via the narrowing of \\(\\tau\\)) in order to improve their precision over an unpooled estimate. On the other hand, if there is a meaningful difference between the groups11, this model can still represent that, unlike the unpooled model.\nIn our context, however, we need a tiny bit more. We have a group-level covariate (specifically age) that we think is going to effect the group mean. So the model we want is \\[\\begin{align*}\ny_{ij}  \\mid \\mu_j,\\beta, \\sigma &\\sim N(\\mu_j + \\beta_\\text{day}\\, \\text{day}_{ij} + \\beta_\\text{age,day}\\, \\text{[age*day]}_{ij} , \\sigma^2) \\\\\n\\mu_j\\mid \\tau, \\mu,\\beta &\\sim N(\\mu +  \\beta_\\text{age}\\, \\text{age}_j, \\tau^2) \\\\\n\\mu &\\sim p(\\mu)\\\\\n\\beta &\\sim p(\\beta)\\\\\n\\tau & \\sim p(\\tau) \\\\\n\\sigma &\\sim p(\\sigma).\n\\end{align*}\\]\nIn order to fully specify the model we need to set the four prior distributions.\nThis is an example of a multilevel12 model. The name comes from the data having multiple levels (in this case two: the observation level and the group level). Both levels have an appropriate model for their mean.\nThis mathematical representation does a good job in separating out the two different levels. However, there are a lot of other ways of writing multilevel models. An important example is the extended formula notation created13 by R’s lme4 package. In their notation, we would write this model as\n\nformula &lt;- active_bins_scaled ~ age_centred*day + (1 | monkey)\n\nThe first bit of this formula is the same as the formula used in linear regression. The interesting bit is is the (1 | monkey). This is the way to tell R that the intercept (aka 1 in formula notation) is going to be grouped by monkey and we are going to put an exchangeable normal prior on it. For more complex models there are more complex variations on this theme, but for the moment we won’t go any further."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#reasoning-out-some-prior-distributions",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#reasoning-out-some-prior-distributions",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "Reasoning out some prior distributions",
    "text": "Reasoning out some prior distributions\nWe need to set priors. The canny amongst you may have noticed that I did not set priors in the previous two examples. There are two reasons for this: firstly I didn’t feel like it, and secondly none but the most terrible prior distributions would have meaningfully changed the conclusions. This is, it turns out, one of the great truths when it comes to prior distributions: they do not matter until they do14.\nIn particular, if you have a parameter that directly sees the data (eg it’s in the likelihood) and there is nothing weird going on15, then the prior distribution will usually not do much as any prior will be quickly overwhelmed by the data.\nThe problem is that we have one parameter in our model (\\(\\tau\\)) that does not directly see the data. Instead of directly telling us about an observation, it tells us about how different the groups of observations are. There is usually less information in the data about this type of parameter and, consequently, the prior distribution will be more important. This is especially true when you have more than one grouping variable, or when a variable only has a small number of groups.\nSo let’s pay some proper attention to the priors.\nTo begin with, let’s set priors on \\(\\mu\\), \\(\\beta\\), and \\(\\sigma\\) (aka the data-level parameters). This is a considerably easier task if the data is scaled. Otherwise, you need to encode information about the usual scale16 of the data into your priors. Sometimes this is a sensible and easy thing to do, but usually it’s easier to simply scale the data. (A lot of software will simply scale your data for you, but it is always better to do it yourself!)\nSo let’s scale our data. We have three variables that need scaling: age (aka the covariate that isn’t categorical) and active_bins (aka the response). For age, we are going to want to measure it as either years from the youngest monkey or years from the average monkey. I think, in this situation, the first version could make a lot of sense, but we are going with the second. This allows us to interpret \\(\\mu\\) as the over-all mean. Otherwise, \\(\\mu\\) would tell us about the overall average activity of 4 year old monkeys and we will use \\(\\beta(\\text{age}_j - 4)\\) to estimate how much the activity changes, on average keeping all other aspects constant, as the monkey ages.\nOn the other hand, we have no sensible baseline for activity, so deviation from the average seems like a sensible scaling. I also don’t know, a priori, how variable activity is going to be, so I might want to scale17 it by its standard deviation. In this case, I’m not going to do that because we have a sensible fixed18 upper limit (8), which I can scale by.\nOne important thing here is that if we scale the data by data-dependent quantities (like the minimum, the mean, or the standard deviation) we must keep track of this information. This is because any future data we try to predict with this model will need to be transformed the same way using the same19 numbers! This particularly has implication when you are doing things like test/training set validation or cross validation: in the first case, the test set needs to be scaled in the same way the training set was; while in the second case each cross validation training set needs to be scaled independently and that scaling needs to be used on the corresponding left-out data20.\n\nage_centre &lt;- mean(activity_2mins$age)\nage_scale &lt;- diff(range(activity_2mins$age))/2\nactive_bins_centre &lt;- 4\n\nactivity_2mins_scaled &lt;- activity_2mins |&gt;\n  mutate(monkey = factor(monkey),\n         day = factor(day),\n         age_centred = (age - age_centre)/age_scale,\n         active_bins_scaled = (active_bins - active_bins_centre)/4)\nglimpse(activity_2mins_scaled)\n\nRows: 485\nColumns: 7\n$ monkey             &lt;fct&gt; 0, 0, 88, 88, 636, 636, 760, 760, 1257, 1257, 1607,…\n$ day                &lt;fct&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, …\n$ total              &lt;dbl&gt; 495, 1003, 2642, 524, 199, 282, 363, 445, 96, 495, …\n$ active_bins        &lt;int&gt; 6, 6, 8, 6, 2, 3, 3, 4, 3, 8, 6, 5, 3, 3, 6, 5, 8, …\n$ age                &lt;dbl&gt; 29, 29, 29, 29, 28, 28, 30, 30, 27, 27, 27, 27, 27,…\n$ age_centred        &lt;dbl&gt; 1.1054718, 1.1054718, 1.1054718, 1.1054718, 1.02854…\n$ active_bins_scaled &lt;dbl&gt; 0.50, 0.50, 1.00, 0.50, -0.50, -0.25, -0.25, 0.00, …\n\n\nWith our scaling completed, we can now start thinking about prior distributions. The trick with priors is to make them wide enough to cover all plausible values of a parameter without making them so wide that they put a whole bunch of weight on essentially silly values.\nWe know, for instance, that our unscaled activity will go between 0 and 8. That means that it’s unlikely for the mean of the scaled process to be much bigger than 3 or 4. These considerations, along with the fact that we have centred the data so the mean should be closer to zero, suggest that a \\(N(0,1)\\) prior should be appropriate for \\(\\mu\\).\nAs we normalised our age data relative to the smallest age, we should think more carefully about the scaling of \\(\\beta\\). Macaques live for 20-3021 years, so we need to think about, for instance, an ordinary aged macaque that would be 15 years older than the baseline. Thanks to our scaling, the largest change that we can have is around 1, which strongly suggests that if \\(\\beta\\) was too much larger than \\(1/8\\) we are going to be in unreasonable territory. So let’s put a \\(N(0,0.2^2)\\) prior22 on \\(\\beta_\\text{age}\\) and \\(\\beta_\\text{age,day}\\). For \\(\\beta_\\text{day}\\) we can use a \\(N(0,1)\\) prior.\nSimilarly, the scaling of activity_bins suggests that a \\(N(0,1)\\) prior would be sufficient for the data-level standard deviation \\(\\sigma\\).\nThat just leaves us with our choice of prior for the standard deviation of the intercept23 \\(\\mu_j\\), \\(\\tau\\). Thankfully, we considered this case in detail in the previous blog post. There I argued that a sensible prior for \\(\\tau\\) would be an exponential prior. To be quite honest with you, a half-normal or a half-t also would be fine. But I’m going to stick to my guns. For the scaling, again, it would be a touch surprising (given the scaling of the data) if the group means were more than 3 apart, so choosing \\(\\lambda=1\\) in the exponential distribution should give a relatively weak prior without being so wide that we are putting prior mass on a bunch of values that we would never actually want to put prior mass on.\nWe can then fit the model with brms. In this case, I’m using the cmdstanr back end, because it’s fast and I like it.\nTo specify the model, we use the lme4-style formula notation discussed above.\nTo set the priors, we will use brms. Now, if you are Paul you might be able to remember how to set priors in brms without having to look it up, but I am sadly not Paul24, so every time I need to set priors in brms I write the formula and use the convenient get_prior function\n\nlibrary(cmdstanr)\nlibrary(brms)\nget_prior(formula, activity_2mins_scaled)\n\n\n  \n\n\n\nFrom this, we can see that the default prior on \\(\\beta\\) is an improper flat prior, the default prior on the intercept is a Student-t with 3 degrees of freedom centred at zero with standard deviation 2.5. The same prior (restricted to positive numbers) is put on all of the standard deviation parameters. These default prior distributions are, to be honest, probably fine in this context25, but it is good practice to always set your prior.\nWe do this as follows. (Note that brms uses Stan, which parameterises the normal distribution by its mean and standard deviation!)\n\npriors &lt;- prior(normal(0, 0.2), coef = \"age_centred\") + \n  prior(normal(0,0.2), coef = \"age_centred:day2\") +\n  prior(normal(0, 1), coef = \"day2\") +\n  prior(normal(0,1), class = \"sigma\") +\n  prior(exponential(1), class = sd) + # tau\n  prior(normal(0,1), class = \"Intercept\")\npriors"
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#pre-experiment-prophylaxis",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#pre-experiment-prophylaxis",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "Pre-experiment prophylaxis",
    "text": "Pre-experiment prophylaxis\nSo we have specified some priors using the power of our thoughts. But we should probably check to see if they are broadly sensible. A great thing about Bayesian modelling is that we are explicitly specifying our a priori (or pre-data) assumptions about the data generating process. That means that we can do a fast validation of our priors by simulating from them and checking that they’re not too wild.\nThere are lots of ways to do this, but the easiest26 way to do this is to use the sample_prior = \"only\" option in the brm() function.\n\nprior_draws &lt;- brm(formula, \n           data = activity_2mins_scaled,\n           prior = priors,\n           sample_prior = \"only\",\n           backend = \"cmdstanr\",\n           cores = 4,\n           refresh = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.7 seconds.\nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.7 seconds.\nChain 4 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 0.9 seconds.\n\n\nNow that we have samples from the prior distribution, we can assemble them to work out what our prior tells us we would, pre-data, predict for the number of active bins for a single monkey (in this a single monkey27 that is 10 years older than the baseline).\n\npred_data &lt;- data.frame(age_centred = 10, day = 1, monkey = \"88\") \ntibble(pred = brms::posterior_predict(prior_draws, \n                                      newdata = pred_data )) |&gt;\n  ggplot(aes(pred)) +\n  geom_histogram(aes(y = after_stat(density)), fill = \"lightgrey\") +\n  geom_vline(xintercept = -1, linetype = \"dashed\") + \n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  xlim(c(-20,20)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nThe vertical lines are (approximately) the minimum and maximum of the data. This28 suggests that the implied priors are definitely wider than our observed data, but they are not several orders of magnitude too wide. This is a good situation to be in: it gives enough room in the priors that we might be wrong with our specification while also not allowing for truly wild values of the parameters (and implied predictive distribution). One could even go so far as to say that the prior is weakly informative.\nLet’s compare this to the default priors on the standard deviation parameters. (The default priors on the regression parameters are improper so we can’t simulate from them. So I replaced the improper prior with a much narrower \\(N(0,10^2)\\) prior. If you make the prior on the \\(\\beta\\) wider the prior predictive distribution also gets wider.)\n\npriors_default &lt;- prior(normal(0,10), class = \"b\")\nprior_draws_default &lt;- brm(formula, \n           data = activity_2mins_scaled,\n           prior = priors_default,\n           sample_prior = \"only\",\n           backend = \"cmdstanr\",\n           cores = 4,\n           refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.6 seconds.\nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n\ntibble(pred = brms::posterior_predict(prior_draws_default, \n                                      newdata = pred_data )) |&gt;\n  ggplot(aes(pred)) +\n  geom_histogram(aes(y = after_stat(density)), fill = \"lightgrey\") +\n  geom_vline(xintercept = -1, linetype = \"dashed\") + \n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis is considerably wider."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#fitting-the-data-or-do-my-monkeys-get-less-interesting-as-they-age",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#fitting-the-data-or-do-my-monkeys-get-less-interesting-as-they-age",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "Fitting the data; or do my monkeys get less interesting as they age",
    "text": "Fitting the data; or do my monkeys get less interesting as they age\nWith all of that in hand, we can now fit the data. Hooray. This is done with the same command (minus the sample_prior bit).\n\nposterior_draws &lt;- brm(formula, \n           data = activity_2mins_scaled,\n           prior = priors,\n           backend = \"cmdstanr\",\n           cores = 4,\n           refresh = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.7 seconds.\nChain 3 finished in 1.8 seconds.\nChain 2 finished in 1.8 seconds.\nChain 4 finished in 1.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.8 seconds.\nTotal execution time: 2.0 seconds.\n\nposterior_draws\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: active_bins_scaled ~ age_centred * day + (1 | monkey) \n   Data: activity_2mins_scaled (Number of observations: 485) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~monkey (Number of levels: 243) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.31      0.03     0.25     0.37 1.00     1070     1766\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           -0.04      0.03    -0.11     0.02 1.00     4222     3171\nage_centred          0.02      0.07    -0.11     0.14 1.00     3671     3150\nday2                 0.10      0.04     0.03     0.18 1.00     8022     2911\nage_centred:day2     0.07      0.07    -0.08     0.22 1.00     6170     2584\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.43      0.02     0.39     0.47 1.00     1613     2430\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere doesn’t seem to be much of an effect of age in this data.\nIf you’re curious, this matches well29 with the output of lme4, which is a nice sense check for simple models. Generally speaking, if they’re the same then they’re both fine. If they are different30, then you’ve got to look deeper.\n\nlibrary(lme4)\nfit_lme4 &lt;- lmer(formula, activity_2mins_scaled)\nfit_lme4\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: active_bins_scaled ~ age_centred * day + (1 | monkey)\n   Data: activity_2mins_scaled\nREML criterion at convergence: 734.9096\nRandom effects:\n Groups   Name        Std.Dev.\n monkey   (Intercept) 0.3091  \n Residual             0.4253  \nNumber of obs: 485, groups:  monkey, 243\nFixed Effects:\n     (Intercept)       age_centred              day2  age_centred:day2  \n        -0.04114           0.01016           0.10507           0.08507  \n\n\nWe can also compare the fit using leave-one-out cross validation. This is similar to AIC, but more directly interpretable. It is the average of \\[\n\\log p_\\text{posterior predictive}(y_{ij} \\mid y_{-ij}) = \\log \\left(\\int_\\theta p(y_{ij} \\mid \\theta)p(\\theta \\mid y_{-ij})\\, d\\theta\\right),\n\\] where \\(\\theta\\) is a vector of all of the parameters in the model. The notation \\(y_{-ij}\\) is the data without the \\(ij\\)th observation. This average is sometimes called the expected log predictive density or elpd.\nTo compare it with the two linear regression models, I need to fit them in brms. I will use a \\(N(0,1)\\) prior for the monkey intercepts and the same priors as the previous model for the other parameters.\n\npriors_lm &lt;-  prior(normal(0,1), class = \"b\") +\n  prior(normal(0, 0.2), coef = \"age_centred\") + \n  prior(normal(0,0.2), coef = \"age_centred:day2\") +\n  prior(normal(0, 1), coef = \"day2\") +\n  prior(normal(0,1), class = \"Intercept\") +\n  prior(normal(0,1), class = \"sigma\")\n\nposterior_nopool &lt;- brm(\n  active_bins_scaled ~ age_centred * day + monkey, \n  data = activity_2mins_scaled,\n  prior = priors_lm,\n  backend = \"cmdstanr\",\n  cores = 4,\n  refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 4.5 seconds.\nChain 3 finished in 4.5 seconds.\nChain 2 finished in 4.5 seconds.\nChain 4 finished in 4.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.5 seconds.\nTotal execution time: 4.7 seconds.\n\nposterior_pool &lt;- brm(\n  active_bins_scaled ~ age_centred * day, \n  data = activity_2mins_scaled,\n  prior = priors_lm,\n  backend = \"cmdstanr\",\n  cores = 4,\n  refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.3 seconds.\n\n\nWe an now use the loo_compare function to compare the models. By default, the best model is listed first and the other models are listed below it with the difference in elpd values given. To do this, we need to tell brms to compute the loo criterion using the add_criterion function.\n\nposterior_draws &lt;- add_criterion(posterior_draws, \"loo\")\n\nWarning: Found 2 observations with a pareto_k &gt; 0.7 in model 'posterior_draws'.\nIt is recommended to set 'moment_match = TRUE' in order to perform moment\nmatching for problematic observations.\n\nposterior_nopool &lt;- add_criterion(posterior_nopool, \"loo\")\n\nWarning: Found 63 observations with a pareto_k &gt; 0.7 in model\n'posterior_nopool'. It is recommended to set 'moment_match = TRUE' in order to\nperform moment matching for problematic observations.\n\nposterior_pool &lt;- add_criterion(posterior_pool, \"loo\")\nloo_compare(posterior_draws, posterior_nopool, posterior_pool)\n\n                 elpd_diff se_diff\nposterior_draws    0.0       0.0  \nposterior_pool   -29.0       7.4  \nposterior_nopool -53.3       9.0  \n\n\nThere are some warnings there suggesting that we could recompute these using a slower method, but for the purposes of today I’m not going to do that and I shall declare that the multilevel model performs far better than the other two models."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#post-experiment-prophylaxis",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#post-experiment-prophylaxis",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "Post-experiment prophylaxis",
    "text": "Post-experiment prophylaxis\nOf course, we would be fools to just assume that because we fit a model and compared it to some other models, the model is a good representation of the data. To do that, we need to look at some posterior checks.\nThe easiest thing to look at is the predictions themselves.\n\nfitted &lt;- activity_2mins_scaled |&gt;\n  cbind(t(posterior_predict(posterior_draws,ndraws = 200))) |&gt;\n  pivot_longer(8:207, names_to = \"draw\", values_to = \"fitted\")\n\nday_labs &lt;- c(\"Day 1\", \"Day 2\")\nnames(day_labs) &lt;- c(\"1\", \"2\")\n\nviolin_plot &lt;- fitted |&gt; \n  ggplot(aes( x=age, y = 4*fitted + active_bins_centre, group = age)) + \n  geom_violin(colour = \"lightgrey\") +\n  geom_point(aes(y = active_bins), colour = \"red\") +\n  facet_wrap(~day, labeller = labeller(day = day_labs)) +\n  theme_bw() \nviolin_plot\n\n\n\n\n\n\n\n\nThat appears to be a reasonably good fit, although it’s possible that the prediction intervals are a bit wide. We can also look at the plot of the posterior residuals vs the fitted values. Here the fitted values are the mean of the posterior predictive distribution.\nNext, let’s check for evidence of non-linearity in age.\n\nplot_data &lt;- activity_2mins_scaled |&gt;\n  mutate(fitted_mean = colMeans(posterior_epred(posterior_draws,ndraws = 200)))\n\nage_plot &lt;- plot_data |&gt; \n  ggplot(aes(x = age, y = active_bins_scaled - fitted_mean)) +\n  geom_point() +\n  theme_bw()\nage_plot\n\n\n\n\n\n\n\n\nThere doesn’t seem to be any obvious evidence of non-linearity in the residuals, which suggests the linear model for age was sufficient.\nWe can also check the distributional assumption31 that the residuals \\[\nr_{ij} = y_{ij} - \\mu_j\n\\] have a Gaussian distribution. We can check this with a qq-plot. Here we are using the posterior mean to define our residuals.\nWe can look at the qq-plot to see how we’re doing with normality.\n\ndistribution_plot &lt;- plot_data |&gt; ggplot(aes(sample = (active_bins_scaled - fitted_mean)/sd(active_bins_scaled - fitted_mean))) + \n  stat_qq() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + \n  theme_classic()\ndistribution_plot\n\n\n\n\n\n\n\n\nThat’s not too bad. A bit of a deviation from normality in the tails but nothing that would make me weep. It could well be an artifact of how I defined and normalised the residuals.\nWe can also look at the so-called k-hat plot, which can be useful for finding high-leverage observations in general models.\n\nloo_posterior &lt;- LOO(posterior_draws) #warnings suppressed\nloo_posterior\n\n\nComputed from 4000 by 485 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -349.8 12.4\np_loo       117.8  5.2\nlooic       699.7 24.7\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     418   86.2%   902       \n (0.5, 0.7]   (ok)        65   13.4%   443       \n   (0.7, 1]   (bad)        1    0.2%   272       \n   (1, Inf)   (very bad)   1    0.2%   59        \nSee help('pareto-k-diagnostic') for details.\n\nplot(loo_posterior)\n\n\n\n\n\n\n\n\nThis suggests that observations 393, 394 are potentially high leverage and we should check them more carefully. I won’t be doing that today.\nFinally, let’s look at the residuals vs the fitted values. This is a commonly used diagnostic plot in linear regression and it can be very useful for visually detecting non-linear patterns and heteroskedasticity in the residuals. So let’s make the plot32.\n\nproblem_plot &lt;- plot_data |&gt; \n  ggplot(aes(x = fitted_mean, y = active_bins_scaled - fitted_mean)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\", colour = \"blue\")+\n  facet_wrap(~day) +\n  theme_bw() +  theme(legend.position=\"none\") +\n  xlim(c(-1,1)) +\n  ylim(c(-1,1))\nproblem_plot\n\n\n\n\n\n\n\n\nHmmmm. That’s not excellent. The stripes are related to the 8 distinct values the response can take, but there is definitely a trend in the residuals. In particular, we are under-predicting small values and over-predicting large values. There is something here and we will look into it!"
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#understanding-diagnostic-plots-from-multilevel-models",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#understanding-diagnostic-plots-from-multilevel-models",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "Understanding diagnostic plots from multilevel models",
    "text": "Understanding diagnostic plots from multilevel models\nThe thing is, multilevel models are notorious for having patterns that are essentially a product of the data design and not of any type of statistical misspecification. In a really great paper that you should all read, Adam Loy, Heike Hofmann, and Di Cook talk extensively about the challenges with interpreting diagnostic plots for linear mixed effects models33.\nI’m not going to fully follow their recommendations, mostly because I’m too lazy34 to write a for loop, but I am going to appropriate the guts of their idea.\nThey note that strange patterns can occur in diagnostic plots even for correctly specified models. Moreover, we simply do not know what these patters will be. It’s too complex a function of the design, the structure, the data, and the potential misspecification. That sounds bad, but they note that we don’t need to know what pattern to expect. Why not? Because we can simulate it!\nSo this is the idea: Let’s simulate some fake35 data from a correctly specified model that otherwise matches with our data. We can then compare the diagnostic plots from the fake data with diagnostic plots from the real data and see if the patterns are meaningfully different.\nIn order to do this, we should have a method to construct multiple fake data sets. Why? Well a plot is nothing but another test statistic and we must take this variability into account.\n(That said, do what I say, not what I do. This is a blog. I’m not going to code well enough to make this clean and straightforward, so I’m just going to do one.)\nThere is an entire theory of visual inference that uses these lineups of diagnostic plots, where one uses the real data and the rest use realisations of the null data, that is really quite interesting and well beyond the scope of this post. But if you want to know more, read the Low, Hoffman, and Cook paper!\n\nMaking new data\nThe first thing that we need to do is to work out how to simulate fake data from a correctly specified model with the same structure. Following the Low etc paper, I’m going to do a simple parameteric bootstrap, where I take the posterior medians of the fitted distribution and simulate data from them.\nThat said, there are a bunch of other options. Specifically, we have a whole bag of samples from our posterior distribution and it would be possible to use that to select values of36 \\((\\mu, \\beta, \\tau, \\sigma)\\) for our simulation.\nSo let’s make some fake data and fit the model to it!\n\nmonkey_effect &lt;- tibble(monkey = unique(activity_2mins_scaled$monkey), \n                        monkey_effect = rnorm(243,0,0.31))\ndata_fake &lt;- activity_2mins_scaled |&gt;\n  left_join(monkey_effect, by = \"monkey\")  |&gt;\n  mutate(active_bins_scaled = rnorm(length(age_centred),\n            mean = -0.04 +0.01 * age_centred + \n              monkey_effect + if_else(day == \"2\", 0.1 + 0.085 *age_centred, 0.0), \n            sd = 0.43))\n                                              \nposterior_draws_fake &lt;- brm(formula, \n           data = data_fake,\n           prior = priors,\n           backend = \"cmdstanr\",\n           cores = 4,\n           refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.6 seconds.\nChain 2 finished in 1.6 seconds.\nChain 3 finished in 1.6 seconds.\nChain 4 finished in 1.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.6 seconds.\nTotal execution time: 1.8 seconds.\n\n\n\n\nThe good plots\nFirst up, let’s look at the violin plot.\n\nlibrary(cowplot)\nfitted_fake &lt;- data_fake |&gt;\n  cbind(t(posterior_predict(posterior_draws_fake,ndraws = 200))) |&gt;\n  pivot_longer(8:207, names_to = \"draw\", values_to = \"fitted\")\n\nday_labs &lt;- c(\"Day 1\", \"Day 2\")\nnames(day_labs) &lt;- c(\"1\", \"2\")\n\nviolin_fake &lt;- fitted_fake |&gt; \n  ggplot(aes( x=age, y = 4*fitted + active_bins_centre, group = age)) + \n  geom_violin(colour = \"lightgrey\") +\n  geom_point(aes(y = active_bins), colour = \"red\") +\n  facet_wrap(~day, labeller = labeller(day = day_labs)) +\n  theme_bw() \n  \nplot_grid(violin_plot, violin_fake, labels = c(\"Real\", \"Fake\"))\n\n\n\n\n\n\n\n\nThat’s very similar to our data plot.\nNext up, we will look at the residuals ordered by age\n\nplot_data_fake &lt;- data_fake |&gt;\n  mutate(fitted_mean = colMeans(posterior_epred(posterior_draws_fake,ndraws = 200)))\n\nage_fake &lt;- plot_data_fake |&gt; \n  ggplot(aes(x = age, y = active_bins_scaled - fitted_mean)) +\n  geom_point() +\n  theme_bw()\nplot_grid(age_plot, age_fake, labels = c(\"Real\", \"Fake\"))\n\n\n\n\n\n\n\n\nFabulous!\nNow let’s check the distributional assumption on the residuals!\n\ndistribution_fake &lt;- plot_data_fake |&gt;\n  ggplot(aes(sample = (active_bins_scaled - fitted_mean)/sd(active_bins_scaled - fitted_mean))) + \n  stat_qq() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + \n  theme_classic()\n\nplot_grid(distribution_plot, distribution_fake, labels = c(\"Real\", \"Fake\"))\n\n\n\n\n\n\n\n\nExcellent!\nFinally, we can look at the k-hat plot. Because I’m lazy, I’m not going to put them side by side. You can scroll.\n\nloo_fake &lt;- LOO(posterior_draws_fake)\n\nWarning: Found 4 observations with a pareto_k &gt; 0.7 in model\n'posterior_draws_fake'. It is recommended to set 'moment_match = TRUE' in order\nto perform moment matching for problematic observations.\n\nloo_fake\n\n\nComputed from 4000 by 485 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -372.1 14.9\np_loo       115.4  6.1\nlooic       744.2 29.7\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     422   87.0%   579       \n (0.5, 0.7]   (ok)        59   12.2%   220       \n   (0.7, 1]   (bad)        4    0.8%   118       \n   (1, Inf)   (very bad)   0    0.0%   &lt;NA&gt;      \nSee help('pareto-k-diagnostic') for details.\n\nplot(loo_fake)\n\n\n\n\n\n\n\n\nAnd look: we get some extreme values. (Depending on the run we get more or less). This suggests that while it would be useful to look at the data points flagged by the k-hat statistic, it may just be sampling variation.;\nAll of this suggests our model assumptions are not being grossly violated. All except for that residual vs fitted values plot…\n\n\nThe haunted residual vs fitted plot\nNow let’s look at our residual vs fitted plot.\n\nproblem_fake &lt;- plot_data_fake |&gt; \n  ggplot(aes(x = fitted_mean, y = active_bins_scaled - fitted_mean)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\", colour = \"blue\")+\n  facet_wrap(~day) +\n  theme_bw() +  theme(legend.position=\"none\") +\n  xlim(c(-1,1)) +\n  ylim(c(-1,1))\nplot_grid(problem_plot, problem_fake, labels = c(\"Real\", \"Fake\"))\n\n\n\n\n\n\n\n\nAnd what do you know! They look the same. (Well, minus the discretisation artefacts.)\n\n\nSo what the hell is going on?\nGreat question! It turns out that this is one of those cases where our intuition from linear models does not transfer over to multilevel models.\nWe can actually reason this out by thinking about a model where we have no covariates.\nIf we have no pooling then the observations for every monkey are, essentially, averaged to get our estimate of \\(\\mu_j\\). If we repeat this, we will find that our \\(\\mu_j\\) are basically37 unbiased and the corresponding residual \\[\nr_{ij} = y_{ij} - \\mu_j\n\\] will have mean zero.\nBut that’s not what happens when we have partial pooling. When we have partial pooling we are combining our naive average38 \\(\\bar y_j\\) with the global average \\(\\mu\\) in a way that accounts for the size of group \\(j\\) relative to other groups as well as the within-group variability relative to the between-group variability.\n\n\nExpand for maths. Just a little\n\nThere is, in fact, a formula for it. Just in case you’re a formula sort of person. The posterior estimate for a Gaussian multilevel model with an intercept but no covariates is \\[\n\\frac{1}{1 +\\frac{\\sigma^2/n}{\\tau^2}}\\left(\\bar{y}_j + \\frac{\\sigma^2/n}{\\tau^2} \\mu\\right).\n\\] When \\(\\sigma/\\sqrt{n}\\) is small, which happens when the sampling standard deviation of \\(\\bar y_j\\) is small relative to the between group variation \\(\\tau\\), this is almost equal to \\(\\bar{y}_j\\) and there is almost no pooling. On the other hand, when \\(\\sigma/\\sqrt{n}\\) is large relative to \\(\\tau\\), then the estimate of \\(\\mu_j\\) will be very close to the overall mean \\(\\mu\\).\n\nThe short version is that there is some magical number \\(\\alpha\\), which depends on \\(\\tau\\), \\(\\sigma\\), and \\(n_j\\) such that \\[\n\\hat \\mu_j = \\alpha \\bar{y}_j + (1-\\alpha) \\mu.\n\\] Because of this, the residuals \\[\nr_{ij} = y_j - \\alpha \\bar{y_j} - (1-\\alpha)\\mu\n\\] are suddenly not going to have mean zero.\nIn fact, if we think about it a bit more, we will realise that the model will drag extreme groups to the centre, which accounts for the positive slope in the residuals vs the fitted values.\nThe slope in this example is quite extreme because the groups are very small (only one or two individuals). But it is a general phenomenon and it’s discussed extensively in Chapter 7 of Jim Hodges’ excellent book. His suggestion is that there isn’t really a good, general way to remove the trend. But that doesn’t mean the plot is useless. It is still able to pinpoint outliers and heteroskedasticity. You’ve just got to tilt your head.\nBut for the purposes of today we can notice that there don’t seem to be any extreme outliers so everything is probably ok."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#conclusion",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#conclusion",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "Conclusion",
    "text": "Conclusion\nSo what have we done? Well we’ve gone through the process of fitting and scruitinising a simple Bayesian multilevel model. We’ve talked about some of the challenges associated with graphical diagnostics for structured data. And we’ve all39 learnt something about the residual-vs-fitted plot for a multilevel model.\nMost importantly, we’ve all learnt the value of using fake data simulated from the posterior model to help us understand our diagnostics.\nThere is more to the scientific story here. It turns out that while there is no effect over 2 minutes, there is a slight effect over 20 minutes. So the conceptual replication failed, but still found some interesting things.\nOf course, I’ve ignored one big elephant in the room: That data was discrete. In the end, our distributional diagnostics didn’t throw up any massive red flags, but nevertheless it could be an interesting exercise to see what happens if we use a more problem-adapted likelihood.\nLast, and certainly not least, I barely scratched the surface40 of the Loy, Hoffman, and Cook paper. Anyone who is interested in fitting Gaussian multilevel models should definitely give it a read."
  },
  {
    "objectID": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#footnotes",
    "href": "posts/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey/everybodys-got-something-to-hide-except-me-and-my-monkey.html#footnotes",
    "title": "A first look at multilevel regression; or Everybody’s got something to hide except me and my macaques",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMark insisted that I like to his google scholar rather than his website. He’s cute that way.↩︎\nMark wants me to tell you that he’s not vain he’s just moving. Sure Jan.↩︎\nI know that marmosets suffer from lesbian bed death, but I’m told that a marmoset is not a macaque, which in turn is not a macaw. Ecology is fascinating.↩︎\nA real problem in the world is that there aren’t enough monkeys for animal research at the best of times. Once you need aged monkeys, it’s an even smaller population. Non-human primate research is hard.↩︎\nActually 244, but one of them turned out to be blind. Animal research is a journey.↩︎\nIt turns out that some of the monkeys didn’t want to give up the puzzle after 20 minutes. One held out for 72 minutes before the data collection ended. Cheeky monkeys.↩︎\nDid Mark make me do unspeakable, degrading, borderline immoral things to get the data? No. It’s open source. Truly the first time I’ve been disappointed that something was open source.↩︎\nIf statisticians abandoned linear regression we would have nothing left. We would be desiccated husks propping up the bar at 3am talking about how we used to do loads of lines in the 80s.↩︎\nOur perfect amount of pool? I don’t know how metaphors work↩︎\nThey always take this form if there is a countable collection of exchangeable random variables. For a finite set there are a few more options. But no one talks about those.↩︎\nmonkeys↩︎\nAlso known as a mixed effects or a linear mixed effects model.↩︎\nThere are many other ways to represnt Gaussian multilevel models. My former colleague Emi Tanaka and Francis Hui wrote a great paper on this topic.↩︎\nSome particularly bold and foolish people take this to mean that priors aren’t important. They usually get their arse handed to them the moment they try to fit an even mildly complex model.↩︎\nA non-exhaustive set of weird things: categorical regressors with a rare category, tail parameters, mixture models↩︎\nThere are situations where this is not true. For instance if you have a log or logit link function you can put reasonable bounds on your coefficients regardless of the scaling of your data. That said, the computational procedures always appreciate a bit of scaling. If there’s one thing that computers hate more that big numbers it’s small numbers.↩︎\nOf course, we know that the there are only 8 fifteen second intervals in two minutes, so we could use this information to make a data-independent scaling. To be brutally francis with you, that’s what you should probably do in this situation, but I’m trying to be pedagogical so let’s at least think about scaling it by the standard deviation.↩︎\nFixed scaling is always easier than data-dependent scaling↩︎\nA real trick for young players is scaling new data by the mean and standard deviation of the new data rather than the old data. That’s a very subtle bug that can be very hard to squash.↩︎\nThe tidymodels package in R is a great example of an ecosystem that does this properly. Max and Julia’s book on using tidymodels is very excellent and well worth a read.↩︎\nOf all of the things in this post, this has been the most aggressively fact checked one↩︎\nIn prior width and on grindr, you should always expect that he’s rounding up.↩︎\nIn some places, we would call this a random effect.↩︎\nHe is very lovely. Many people would prefer that I was him.↩︎\nIt’s possible the the prior on \\(\\tau\\) might be too wide. If we were doing a logistic regression, these priors would definitely be too wide. And if we had a lot of different random terms (eg if we had lots of different species or lots of different labs) then they would also probably be too wide. But they are better than not having priors.↩︎\nNot the most computationally efficient, but the easiest. Also because it’s the same code we will later use to fit the model, we are evaluating the priors that are actually used and not the ones that we think we’re using.↩︎\n It’s number 88, but because our prior is exchangeable it does not matter which monkey we do this for!↩︎\nI also checked different values of age as well as looking at the posterior mean (via posterior_epred) and the conclusions stay the same.↩︎\nThe numbers will never be exactly equal, but they are of similar orders of magnitude.↩︎\nOr if you get some sort of error or warning from lme4↩︎\nSo there’s a wrinkle here. Technically, all of the residuals have different variances, which is annoying. You typically studentise them using the leverage scores, but this is a touch trickier for multilevel models. Chapter 7 of Jim Hodges’s excellent book contains a really good discussion.↩︎\nOnce again, we are not studentizing the residuals. I’m sorry.↩︎\nAnother name for a multilevel model with a Gaussian response↩︎\nAlso because all of my data plots are gonna be stripey as hell, and that kinda destroys the point of visual inference.↩︎\nThey call it null data.↩︎\nNote that I am not using values of \\(\\mu_j\\)! I will simulate those from the normal distribution to ensure correct model specification. For the same reason, I am not using a residual bootstrap. The aim here is not to assess uncertainty so much as it is to ↩︎\nThis is a bit more complex when you’re Bayesian, but the intuition still holds. The difference is that now it is asymptotic↩︎\nThis is the average of all observations in group j. \\[\n\\bar y_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} y_{ij}.\n\\]↩︎\nI mean, some of us knew this. Personally, I only remembered after I saw it and swore a bit.↩︎\nIn particular, they have an interesting discussion on assessing the distributional assumption for \\(\\mu_j\\).↩︎"
  },
  {
    "objectID": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane.html",
    "href": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane.html",
    "title": "\\((n-1)\\)-sane in the membrane",
    "section": "",
    "text": "I’ve been teaching a lot lately. That’s no huge surprise. It is my job. Maybe the one slight oddity this year is that I shifted jobs and, in switching hemispheres, I landed 3 consecutive teaching semesters. So. I’ve been teaching a lot lately.\nAnd when you’re in a period of heavy teaching, every-fucking-thing is about teaching.\nSo this blogpost is about teaching.\nRight now, I’m coming to the end of a second year class called Statistical Thinking. It’s been fun to work out how to teach the material. It’s standard fare: sampling variation, tests, bootstraps1, regression, and just a hint of Bayes in the last 2 weeks that you incentivize by promising a bastard of an exam question. So you know, (arms up even though I’m Catholic) tradition!"
  },
  {
    "objectID": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane.html#footnotes",
    "href": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane.html#footnotes",
    "title": "\\((n-1)\\)-sane in the membrane",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo paraphrase Jimmy Somerville, tell me whyyyyyyyyy about 90% of the bootstrap material on the web is … misguided. And why tidymodels only has the shit bootstrap in it?↩︎\nOk. Straight up, “[Intro to statistics] is a commentary on the various and continuing incapabilities of men” would’ve also worked.↩︎\nThis course stands on the shoulders of giants: Di Cook and Catherine Forbes gave me a great base. And of course every single textbook (shout out to the OpenIntro crew!), blog post, weird subsection of some other book, paper from 1987 on some weird bootstrap, etc that I have used to make a course!↩︎\nYes. I used the word on purpose.↩︎\n\\(n\\) is the size of the sample.↩︎\nI spend most of my time doing Bayes shit, and we play this game somewhat differently. But the gist is the same.↩︎\nNot thrilling.↩︎"
  },
  {
    "objectID": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html",
    "href": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html",
    "title": "Why won’t you cheat with me? (Repost)",
    "section": "",
    "text": "But I got some ground rules  I’ve found to be sound rules\nand you’re not the one I’m exempting.\nNonetheless, I confess it’s tempting.\n– Jenny Toomey sings Franklin Bruno\nIt turns out that I did something a little controversial in last week’s1 post. As these things always go, it wasn’t the thing I was expecting to get push back from, but rather what I thought was a fairly innocuous scaling of the prior. One commenter (and a few other people on other communication channels) pointed out that the dependence of the prior on the design didn’t seem kosher. Of course, we (Andrew, Mike and I) wrote a paper that was sort of about this a few months ago2, but it’s one of those really interesting topics that we can probably all deal with thinking more about.\nSo in this post, I’m going to go into a couple of situations where it makes sense to scale the prior based on fixed information about the experiment. (The emerging theme for these posts is “things I think are interesting and useful but are probably not publishable” interspersed with “weird digressions into musical theatre / the personal mythology of Patti LuPone”.)\nIf you haven’t clicked yet, this particular post is going to be drier than Eve Arden in Mildred Pierce. If you’d rather be entertained, I’d recommend Tempting: Jenny Toomey sings the songs of Franklin Bruno. (Franklin Bruno is today’s stand in for Patti, because I’m still sad that War Paint closed3. I only got to see it twice.)\n(Jenny Toomey was one of the most exciting American indie musicians in the 90s both through her bands [Tsunami was the notable one, but there were others] and her work with Simple Machines, the label she co-founded. These days she’s working in musician advocacy and hasn’t released an album since the early 2000s. Bruno’s current band is called The Human Hearts. He has had a long solo career and was also in an excellent powerpop band called Nothing Painted Blue, who had an album called The Monte Carlo Method. And, now4 that I live in Canada, I should say that that album has a fabulous cover of Mark Szabo’s I Should Be With You. To be honest, the only reason I work with Andrew and the Stan crew is that I figure if I’m in New York often enough I’ll eventually coincide with a Human Hearts concert5.)"
  },
  {
    "objectID": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#sparsity",
    "href": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#sparsity",
    "title": "Why won’t you cheat with me? (Repost)",
    "section": "Sparsity",
    "text": "Sparsity\n\nWhy won’t you cheat with me? You and I both know you’ve done it before. – Jenny Toomey sings Franklin Bruno\n\nThe first object of our affliction are priors that promote sparsity in high-dimensional models. There has been a lot of work on this topic, but the cheaters guide is basically this:\n\nWhile spike-and-slab models can exactly represent sparsity and have excellent theoretical properties, they are basically useless from a computational point of view. So we use scale-mixture of normal priors (also known as local-global priors) to achieve approximate sparsity, and then use some sort of decision rule to take our approximately sparse signal and make it exactly sparse.\n\nWhat is a scale-mixture of normals? Well it has the general form \\[\n\\beta_j \\sim N(0, \\tau^2 \\psi^2_j),\n\\] where \\(\\tau\\) is a global standard deviation parameter, controlling how large the \\(\\beta_j\\) parameters are in general6, while the local standard deviation parameters \\(\\psi_j\\) control how big \\(\\beta_j\\) is relative to the other \\(\\beta\\)s.\nThe priors for \\(\\tau\\) and the \\(\\psi_j\\) are typically set to be independent. A lot of theoretical work just treats \\(\\tau\\) as fixed (or as otherwise less important than the local parameters), but this is wrong.\nPedant’s corner: Andrew likes define mathematical statisticians as those who use \\(x\\) for their data rather than \\(y\\). I prefer to characterise them by those who think it’s a good idea to put a prior on variance (an un-elicitable quantity) rather than standard deviation (which is easy to have opinions about). Please people just stop doing this. You’re not helping yourselves!\nActually, maybe that last point isn’t for Pedant’s Corner after all. Because if you parameterise by standard deviation it’s pretty easy to work out what the marginal prior on \\(\\beta_j\\) (with \\(\\tau\\) fixed) is.\nThis is quite useful because, with the notable exception of the “Bayesian” “Lasso” which-does-not-work-but-will-never-die-because-it-was-inexplicably-published-in-the leading-stats-journal-by-prominent-statisticians-and-has-the-word-Lasso-in-the-title-even-though-a-back-of-the-envelope-calculation-or-I-don’t-know-a-fairly-straightforward-simulation-by-the-reviewers-should-have-nixed-it (to use its married name), we can’t compute the marginal prior for most scale-mixtures of normals.\nThe following result, which was killed by reviewers at some point during the PC prior papers long review process, but lives forever in the arXiv’d first version, tells you everything you need to know. It’s a picture because frankly I’ve had a glass of wine and I’m not bloody typing it all again7.\n\nTheorem 1 Let \\(\\pi_d(r)\\) be a prior on the standard deviation of \\(v \\sim\n{\\mathcal N}(0,r^2)\\). The induced prior \\[\n\\pi(v) = \\int_0^\\infty\n\\frac{1}{2\\pi r}\\exp\\left({-\\frac{v^2}{2r^2}}\\right)\\pi_d(r)\\,dr\n\\] has the following properties. Fix \\(\\delta&gt; 0\\).\n\nIf \\(\\pi_d(r) \\leq Cr^t\\) for all \\(r \\in [0,\\delta]\\) and for some \\(C,t &gt;0\\), then \\(\\pi(v)\\) is finite at \\(v=0\\).\nIf \\(\\pi_d(r) \\in (0,\\infty)\\) for every \\(r \\in\n[0,\\delta]\\), then \\(\\pi(v)\\) has a weak logarithmic spike at zero, that is \\[\n\\pi(v) = \\mathcal{O}\\left[\\log\\left(1 +\n                                   \\frac{\\delta^2}{v^2}\\right)\\right], \\qquad v \\rightarrow 0.\n\\]\nIf \\(\\int_0^\\delta \\frac{1}{2\\pi\n  r}\\exp\\left({-\\frac{v^2}{2r^2}}\\right)\\pi_d(r)\\,dr &lt;\n  \\infty\\), then \\[\n\\pi(v) \\geq\n\\mathcal{O}\\left(v^{-2}\\exp\\left(-\\frac{v^2}{2\\delta^2}\\right)\\right),\n\\qquad |v| \\rightarrow \\infty.\n\\]\nIf \\(\\pi_d(r) {\\leq}({\\geq}) Cr^{-t}\\) for all \\(r \\in [0,\\delta]\\) and for some \\(C,t &gt;0\\), then \\[\n\\pi(v)\n{\\leq}({\\geq}) \\mathcal{O}(|v|^{-t}),\\qquad v \\rightarrow 0.\n\\]\nIf \\(\\pi_d(r) {\\leq}({\\geq}) Cr^{-t}\\) for all \\(r &gt;\\delta\\) and for some \\(C,t &gt;0\\), then \\[\n\\pi(v)\n{\\leq}({\\geq}) \\mathcal{O}(|v|^{-t}),\\qquad |v| \\rightarrow\n\\infty.\n\\]\n\n\n\n\nThe proof is here.\n\nFor any \\(\\delta &gt; 0\\), \\[\n\\pi(v) =\n\\int_0^\\delta\\frac{1}{2\\pi r}\n\\exp\\left({-\\frac{v^2}{2r^2}}\\right)\n\\pi_d(r)\\,dr +\n\\int_\\delta^\\infty\\frac{1}{2\\pi\nr}\\exp\\left({-\\frac{v^2}{2r^2}}\\right)\n\\pi_d(r)\\,dr = I_1 + I_2.\n\\] Examining this splitting, we note that \\(I_1\\) will control the behaviour of \\(\\pi(v)\\) near zero, while \\(I_2\\) will control the tails.\nAssuming that \\(\\int_\\delta^\\infty r^{-1}\\pi_d(r)\\,dr &lt; \\infty\\), we can bound \\(I_2\\) as \\[\n\\frac{1}{2\\pi }\\exp\\left({-\\frac{v^2}{2\\delta^2}}\\right)\n\\int_\\delta^\\infty r^{-1}\\pi_d(r)\\,dr \\leq I_2 \\leq \\frac{1}{2\\pi}\n\\int_\\delta^\\infty r^{-1}\\pi_d(r)\\,dr.\n\\]\nTo prove part 1, let \\(\\pi_d(r) \\leq Cr^t\\), \\(r \\in\n[0,\\delta]\\) for some \\(t&gt;0\\). Substituting this into \\(I_1\\) and computing the resulting integral using Maple8, we get \\[\\begin{align*}\nI_1 &\\leq - \\frac{C}{2\\pi t}\\left( {2}^{-1/2\\,t}{|v|}^{t}\\Gamma\n\\left( 1-1/2\\,t,1/2\\,{\\frac {v^2}{{\\delta}^{2}}} \\right) -{{\\rm\ne}^{-1/2\\,{\\frac {v^2}{{\\delta}^{2}}}} }{\\delta}^{t}\n\\right) = \\mathcal{O}(1),\n\\end{align*}\\] where \\(\\Gamma(a,x) = \\int_x^\\infty\n\\exp\\left({-t}\\right)t^{a-1}\\,dt\\) is the incomplete Gamma function.\nTo prove parts 2 and 3, we bound \\(I_1\\) as follows. \\[\\begin{align*}\n\\left(\\inf_{r\\in[0,\\delta]} \\pi_d(r)\n\\right)\\int_0^\\delta\\frac{1}{2\\pi\nr}\\exp\\left({-\\frac{v^2}{2r^2}}\\right) \\,dr &\\leq I_1 \\leq\n\\left(\\sup_{r\\in[0,\\delta]} \\pi_d(r)\n\\right)\\int_0^\\delta\\frac{1}{2\\pi r}\\exp\\left({-\\frac{v^2}{2r^2}}\\right) \\\\\n\\frac{1}{4\\pi}\\left(\\inf_{r\\in[0,\\delta]} \\pi_d(r)\\right)\n\\text{E}_1\\left(\\frac{v^2}{2\\delta^2}\\right) & \\leq I_1 \\leq\n\\frac{1}{4\\pi}\\left(\\sup_{r\\in[0,\\delta]} \\pi_d(r)\\right)\n\\text{E}_1\\left(\\frac{v^2}{2\\delta^2}\\right) \\\\\n\\frac{1}{8\\pi}\\left(\\inf_{r\\in[0,\\delta]} \\pi_d(r)\\right)\n\\exp\\left({-\\frac{v^2}{2\\delta^2}}\\right)\\log\\left( 1 +\n\\frac{4\\delta^2}{v^2}\\right) &\\leq I_1\n\\leq\\frac{1}{4\\pi}\\left(\\sup_{r\\in[0,\\delta]} \\pi_d(r)\\right)\n\\exp\\left({-\\frac{v^2}{2\\delta^2}}\\right)\\log\\left( 1 +\n\\frac{2\\delta^2}{v^2}\\right),\n\\end{align*}\\] where \\(\\text{E}_1(x) = \\int_1^\\infty t^{-1}\\exp\\left({-tx}\\right)\\,dt\\) and the third line of inequalities follows using standard bounds in the exponential integral9.\nCombining the lower and upper bounds, it follows that if \\(0\n&lt;\\inf_{r\\in[0,\\delta]} \\pi_d(r) \\leq \\sup_{r\\in[0,\\delta]}\n\\pi_d(r) &lt; \\infty\\), then \\(\\pi(v)\\) has a logarithmic spike near zero. Similarly, the lower bounds show that \\(\\pi(v) \\geq C\nv^{-2}\\exp\\left(-\\frac{v^2}{2\\delta^2}\\right)\\) as \\(v\\rightarrow \\infty\\).\nPart 4 follows by considering let \\(\\pi_d(r) = Cr^{-t}\\), \\(r \\in [0,\\delta]\\) for some \\(t&gt;0\\). Substituting this into \\(I_1\\) and computing the resulting integral using Maple, we get \\[\\begin{align*}\nI_1 & = \\frac{C}{2\\pi t}\\left( {|v|}^{-t}\\Gamma \\left(\n1+1/2\\,t,1/2\\,{\\frac {v^2}{{\\delta}^{2}}} \\right)\n{2}^{t/2}-{\\delta}^{-t}{{\\rm e}^{-1/2\\,{\\frac\n{v^2}{{\\delta}^{2}}}}} \\right) \\sim\n\\mathcal{O}(v^{-t})\n\\end{align*}\\] as \\(v \\rightarrow 0\\). We note that \\(I_1 =\n\\mathcal{O}\\left(\\exp\\left(-v^2/(2\\delta^2)\\right)\\right)\\) as \\(|v|\n\\rightarrow \\infty\\).\nTo prove part 5, let \\(\\pi_d(r) = Cr^{-t}\\), \\(r \\in\n(\\delta,\\infty)\\) for some \\(t&gt;0\\). Substituting this into \\(I_2\\), we get \\[\\begin{align*}\nI_2 = \\frac{C}{8\\pi^2}\\,{2}^{1/2\\,t}{|v|}^{-t} \\left( \\Gamma\n\\left( 1/2\\,t \\right) - \\Gamma \\left( 1/2\\,t,1/2\\,{\\frac\n{{v}^{2}}{{\\delta}^{2}}} \\right) \\right) =\n\\mathcal{O}(|v|^{-t}),\n\\end{align*}\\] where we used the identity \\[\n\\Gamma \\left( 1/2\\,t \\right) - \\Gamma\n\\left( 1/2\\,t,1/2\\,{\\frac {{v}^{2}}{{\\delta}^{2}}} \\right)\n\\rightarrow \\Gamma\\left( 1/2\\,t \\right)\n\\] as \\(|v|\\rightarrow\n\\infty\\).\nDone.\n\nAll of this basically says the following:\n\nIf the density of the prior on the standard deviation is finite at zero, then the implied prior on \\(\\beta_j\\) has a logarithmic spike at zero.\nIf the density of the prior on the standard has a polynomial tail, then the implied prior on \\(\\beta_j\\) has the same polynomial tail.\nNot in the result, but computed at the time: if the prior on the standard deviation is exponential, the prior on \\(\\beta_j\\) still has Gaussian-ish tails. I couldn’t work out what happened in the hinterland between exponential tails and polynomial tails, but I suspect at some point the tail on the standard deviation does eventually get heavy enough to be seen in the marginal, but I can’t tell you when.)\n\nWith this sort of information, you can compute the equivalent of the bounds that I did on the Laplace prior for the general case (or, actually, for the case that will have at least a little bit of a chance, which is the monotonically decreasing priors on the standard deviation).\nIn particular, if you run the argument from the last post, you see that you need a quite heavy tail on the standard deviation prior to get a reasonable prior on the implied sparsity. In particular, we showed that applying this reasoning to the horseshoe prior, where the prior on the local standard deviation is half-Cauchy, you can see that there is a \\(\\lambda\\) that gives a priori weight on \\(p^{-1}\\)-sparse signals, while also letting you have a few very large \\(\\beta_j\\)s.\n\nThe design-scaling in these priors links directly to an implied decision process\n\nYou’d look better if your shadow didn’t follow you around, but it looks as though you’re tethered to the ground, just like every pound of flesh I’ve ever found. – Franklin Bruno in a sourceless light.\n\nFor a very simple decision process (the deterministic threshold process described in the previous post), you can work out exactly how the threshold needs to interact with the prior. In particular, we can see that if we’re trying to detect a true signal that is exactly zero (no components are active), then we know that \\(latex \\| \\mathbf{X} \\boldsymbol{\\beta} \\| = 0\\). This is not possible for these scale-mixture models, but we can require that in this case all of the components are at most \\(latex \\epsilon\\), in which case \\[\n\\| \\mathbf{X}\\boldsymbol{\\beta} \\| \\leq \\epsilon \\| \\mathbf{X} \\|,\n\\] which suggests we want \\(\\epsilon \\ll \\| \\mathbf{X} \\|_\\infty^{-1}\\). The calculation in the previous post shows that if we want this sort of almost zero signal to have any mass at all under the prior, we need to scale \\(\\lambda\\) using information about \\(\\mathbf{X}\\).\nOf course, this is a very very simple decision process. I have absolutely no idea how to repeat these arguments for actually good decision processes, like the predictive loss minimization favoured by Aki. But I’d still expect that we’d need to make sure there was a priori enough mass in the areas of the parameter space where the decision process is firmly one way or another (as well as mass in the indeterminate region). I doubt that the Bayesian Lasso would magically start to work under these more complex losses."
  },
  {
    "objectID": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#models-specified-through-their-full-conditionals",
    "href": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#models-specified-through-their-full-conditionals",
    "title": "Why won’t you cheat with me? (Repost)",
    "section": "Models specified through their full conditionals",
    "text": "Models specified through their full conditionals\n\nWhy won’t you cheat with me? You and I both know that he’s done the same. – Franklin Bruno\n\nSo we can view the design dependence of sparsity priors as preparation for the forthcoming decision process. (Those of you who just mentally broke into Prepare Ye The Way Of The Lord from Godspell, please come to the front of the class. You are my people.) Now let’s talk about a case where this isn’t true.\nTo do this, we need to cast our minds back to a time when people really did have the original cast recording of Godspell on their mind. In particular, we need to think about Julian Besag (who I’m sure was really into musicals about Jesus. I have no information to the contrary, so I’m just going to assume it’s true.) who wrote a series of important papers, one in 1974 and one in 1975 (and several before and after, but I can’t be arsed linking to them all. We all have google.) about specifying models through conditional independence relations.\nThese models have a special place in time series modelling (where we all know about discrete-time Markovian processes) and in spatial statistics. In particular, generalisations of Besag’s (Gaussian) conditional autoregressive (CAR) models are widely used in spatial epidemiology.\nMathematically, Gaussian CAR models (and more generally Gaussian Markov random fields on graphs) are defined through their precision matrix, that is the inverse of the covariance matrix as \\[\n\\mathbf{x} \\sim N(\\mathbf{0}, \\tau^{-1}\\mathbf{Q}^{-1}).\n\\]\nFor simple models, such as the popular CAR model, we assume \\(\\mathbf{Q}\\) is fixed, known, and sparse (i.e. it has a lot of zeros) and we typically interpret \\(\\tau\\) to be the inverse of the variance of \\(\\mathbf{x}\\).\nThis interpretation of \\(\\tau\\) could not be more wrong.\nWhy? Well, let’s look at the marginal distribution \\[\nx_j \\sim N\\left(0, \\tau^{-1}[Q^{-1}]_{ii}\\right).\n\\]\nTo interpet \\(\\tau\\) and the inverse variance, we need the diagonal elements of \\(\\mathbf{Q}^{-1}\\) to all be around 1. This is never the case.\nA simple, mathematically tractable example is the first order random walk on a one-dimensional lattice, which can be written in terms of the increment process as \\[\nx_{j+1} - x_j \\sim N(0, \\tau^{-1}), \\qquad j = 1, \\ldots J-1.\n\\]\nConditioned on a particular starting point, this process looks a lot like a discrete version of Brownian motion as you move the lattice points closer together. This is a useful model for rough non-linear random effects, such as the baseline hazard rate in a Cox proportional hazard model. A long and detailed (and quite general) discussion of these models can be found in Rue and Held’s book.\nI am bringing this case up because you can actually work out the size of the diagonal of \\(\\mathbf{Q}^{-1}\\). Sørbye and Rue talk about this in detail, but for this model maybe the easiest way to understand it is that if we had a fixed lattice with \\(n\\) points and we’d carefully worked out a sensible prior for \\(\\tau\\). Now imagine that we’ve gotten some new data and instead of only \\(n\\) points in the lattice, we got information at a finer scale, so now the same interval is covered by \\(nk\\) equally spaced nodes. We model this with the new first order random walk prior \\[\nx'_{j+1} - x'_j \\sim N(0,[\\tau']^{-1}).\n\\]\nIt turns out that we can relate the inverse variances of these two increment processes as \\(\\tau' = J \\tau\\).\nThis strongly suggests that we should not use the same prior for \\(\\tau\\) as we should for \\(\\tau'\\), but that the prior should actually know about how many nodes there are on the lattice. Concrete suggestions are in the Sørbye and Rue paper linked above.\n\nDesign dependence for Markov random fields\n\nNot to coin a phrase, but play it as it lays – Franklin Bruno in Nothing Painted Blue\n\nThis type of design dependence is a general problem for multivariate Gaussian models specified through their precision (so-called Gaussian Markov random fields). The critical thing here is that, unlike the sparsity case, the design dependence does not come from some type of decision process. It comes from the gap between the parameterisation (in terms of \\(\\tau\\) and \\(\\mathbf{Q}\\)) and the elicitable quantity (the scale of the random effect).\nThis is kinda a general lesson. When specifying multivariate priors, you must always check the implications of your prior on the one- and two-dimensional quantities of interest. Because weird things happen in multivariate land!"
  },
  {
    "objectID": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#gaussian-process-models",
    "href": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#gaussian-process-models",
    "title": "Why won’t you cheat with me? (Repost)",
    "section": "Gaussian process models",
    "text": "Gaussian process models\n\nAnd it’s not like we’re tearing down a house of more than gingerbread. It’s not like we’re calling down the wrath of heaven on our heads. – Jenny Toomey sings Franklin Bruno\n\nSo the design dependence doesn’t necessarily come in preparation for some kind of decision, it can also be because we have constructed (and therefore parameterised) our process in an inconvenient way. Let’s see if we can knock out another one before my bottle of wine dies.\nGaussian processes, the least exciting tool in the machine learner’s toolbox, are another example where your priors need to be design dependent. It will probably surprise you not a single sausage that in this case the need for design dependence comes from a completely different place.\nFor simplicity let’s consider a Gaussian process \\(f(t)\\) in one dimension with isotropic covariance function \\[\nc(s,t) =\\sigma^2 (\\kappa|s-t|)^\\nu K_\\nu(|\\kappa|s-t|).\n\\]\nThis is the commonly encountered Whittle-Matérn family of covariance functions. The distinguished members are the exponential covariance function when \\(\\nu = 0.5\\) and the squared exponential function \\[\nc(s,t)= \\sigma^2\\exp\\left(\\kappa |s-t|^2 \\right),\n\\]\nwhich is the limit as \\(\\nu \\rightarrow \\infty\\).\nOne of the inconvenient features of Matérn models in 1-3 dimensions is that it is impossible to consistently recover all of the parameters by simply observing more and more of the random effect on a fixed interval. You need to see new replicates in order to properly pin these down10.\nSo one might expect that this non-identifiability would be the source of some problems.\nOne would be wrong.\nThe squared exponential covariance function does not have this pathology, but it’s still very very hard to fit. Why? Well the problem is that you can interpret \\(\\kappa\\) as an inverse-range parameter. Roughly, the interpretation is that if \\[\n|s - t | &gt; \\frac{ \\sqrt{ 8 \\nu } }{\\kappa}\n\\] then the value of \\(u(s)\\) is approximately independent of the value of \\(u(t)\\).\nThis means that a fixed data set provides no information about \\(\\kappa\\) in large parts of the parameter space. In particular if \\(\\kappa^{-1}\\) is bigger than the range of the measurement locations, then the data has almost no information about the parameter.\nSimilarly, if \\(\\kappa^{-1}\\) is smaller than the smallest distance between two data points (or for irregular data, this should be something like “smaller than some low quantile of the set of distances between points”), then the data will have nothing to say about the parameter.\nOf these two scenarios, it turns out that the inference is much less sensitive to the prior on small values of \\(\\kappa\\) (ie ranges longer than the data) than it is on small values of \\(\\kappa\\) (ie ranges shorter than the data).\nCurrently, we have two recommendations: one based around PC priors and a very similar one based around inverse gamma priors. But both of these require you to specify the design-dependent quantity of a “minimum length scale we expect this data set to be informative about”.\n\nDesign for Gaussian processes (I’d say “Designing Women”, but I’m aware of the demographics)\n\nI’m a disaster, you’re a disaster, we’re a disaster area. – Franklin Bruno in The Human Hearts (featuring alto extraordinaire and cabaret god Ms Molly Pope)\n\nSo in this final example we hit our ultimate goal. A case where design dependent priors are needed not because of a hacky decision process, or an awkward multivariate specification, but due to the limits of the data. In this case, priors that do not recognise the limitation of the design of the experiment will lead to poorly behaving posteriors. This manifests as the Gaussian processes severely over-fitting the data.\nThis is the ultimate expression of the point that we tried to make in the Entropy paper: The prior can often only be understood in the context of the likelihood."
  },
  {
    "objectID": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#principles-can-only-get-you-so-far",
    "href": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#principles-can-only-get-you-so-far",
    "title": "Why won’t you cheat with me? (Repost)",
    "section": "Principles can only get you so far",
    "text": "Principles can only get you so far\n\nI’m making scenes, you’re constructing dioramas – Franklin Bruno in Nothing Painted Blue\n\nJust to round this off, I guess I should mention that the strong likelihood principle really does suggest that certain details of the design are not relevant to a fully Bayesian analysis. In particular, if the design only pops up in the normalising constant of the likelihood, it should not be relevant to a Bayesian. This seems at odds with everything I’ve said so far.\nBut it’s not.\nIn each of these cases, the design was only invoked in order to deal with some external information. For sparsity, design was needed to properly infer a sparse signal and came in through the structure of the decision process.\nFor the CAR models, the external information was that the elicitable quantity was the marginal standard deviation, which was a complicated function of the design and the standard parameter.\nFor Gaussian processes, the same thing happened: the implicit decision criterion was that we wanted to make good predictions. The design told us which parts of the parameter space obstructed this goal, and a well specified prior removed the problem.\nThere are also any number of cases in real practice where the decision at hand is stochastically dependent on the data gathering mechanism. This is why things like MRP exist.\nI guess this is the tl;dr version of this post (because apparently I’m too wordy for some people. I suggest they read other things. Of course suggesting this in the final paragraph of such a wordy post is very me.):\nDesign matters even if you’re Bayesian. Especially if you want to do something with your posterior that’s more exciting than just sitting on it.\nEdited from an original blog, posted November 2017."
  },
  {
    "objectID": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#footnotes",
    "href": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/why-wont-you-cheat-with-me-repost.html#footnotes",
    "title": "Why won’t you cheat with me? (Repost)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImagine it’s November 2017.↩︎\nAgain, 2017.↩︎\n2021: I am still sad War Paint closed.↩︎\n2017↩︎\nI eventually did coincide with a Human Hearts concert and, to my extreme joy, Jenny Toomey did two songs with the band! They were supporting Gramercy Arms, who I’d never heard before that night but have several perfect albums.↩︎\nThis is like the standard deviation we’d use in an iid normal prior for a non-sparse model.↩︎\n2021: I did indeed type it all again. And a proof. Because why bother if you’re not going to do it well.↩︎\nYes. No open source for me!↩︎\nAbramowitz, M. and Stegun, I. (1972). Handbook of Mathematical Functions. Formula 5.1.20↩︎\nThere’s a recent paper (2021) in JRSSSB that says that these nuggets are identifiable under infill with a “nugget”, which is equivalent to observing with iid noise that magically stays independent as you observe locations closer and closer together. I will let you judge how relevant this case is to your practice. But regardless, for a finite set of data under any reasonable likelihood, you hit these identifiabiliy problems. And in my personal experience, they persevere even with a decent number of sites.↩︎"
  },
  {
    "objectID": "posts/2022-09-07-priors5/priors5.html",
    "href": "posts/2022-09-07-priors5/priors5.html",
    "title": "Priors for the parameters in a Gaussian process",
    "section": "",
    "text": "Long time readers will know that I bloody love a Gaussian process (GP). I wrote an extremely detailed post on the various ways to define Gaussian processes. And I did not do that because I just love inflicting Hilbert spaces on people. In fact, the only reason that I ever went beyond the standard operational definition of GPs that most people live their whole lives using is that I needed to.\nTwice.\nThe first time was when I needed to understand approximation properties of a certain class of GPs. I wrote a post about it. It’s intense1.\nThe second time that I really needed to dive into their arcana and apocrypha2 was when I foolishly asked the question can we compute Penalised Complexity (PC) priors3 4 for Gaussian processes?.\nThe answer was yes. But it’s a bit tricky.\nSo today I’m going to walk you through the ideas. There’s no real need to read the GP post before reading the first half of this one5, but it would be immensely useful to have at least glanced at the post on PC priors.\nThis post is very long, but that’s mostly because it tries to be reasonably self-contained. In particular, if you only care about the fat stuff, you really only need to read the first part. After that there’s a long introduction to the theory of stationary Gaussian processes. All of this stuff is standard, but it’s hard to find collected in one place all of the things that I need to derive the PC prior. The third part actually derives the PC prior using a great deal of methods from the previous part."
  },
  {
    "objectID": "posts/2022-09-07-priors5/priors5.html#part-1-how-do-you-put-a-prior-on-parameters-of-a-gaussian-process",
    "href": "posts/2022-09-07-priors5/priors5.html#part-1-how-do-you-put-a-prior-on-parameters-of-a-gaussian-process",
    "title": "Priors for the parameters in a Gaussian process",
    "section": "Part 1: How do you put a prior on parameters of a Gaussian process?",
    "text": "Part 1: How do you put a prior on parameters of a Gaussian process?\nWe are in the situation where we have a model that looks something like this6 7 \\[\\begin{align*}\ny_i \\mid \\beta, u, \\theta &\\sim p(y_i \\mid \\beta, u, \\phi) \\\\\nu(\\cdot) \\mid \\theta &\\sim GP(0, c_\\theta) \\\\\n\\beta, \\phi &\\sim p(\\beta,\\phi),\n\\end{align*}\\] where \\(c_\\theta(\\cdot,\\cdot)\\) is a covariance function with parameters \\(\\theta\\) and we need to specify a joint prior on the GP parameters \\(\\theta\\).\nThe simplest case of this would be GP regression, but a key thing here is that, in general, the structure (or functional form) of the priors on \\(\\theta\\) probably shouldn’t be too tightly tied to the specific likelihood. Why do I say that? Well the scaling of a GP should depend on information about the likelihood, but it’s less clear that anything else in the prior needs to know about the likelihood.\nNow this view is predicated on us wanting to make an informative prior. In some very special cases, people with too much time on their hands have derived reference priors for specific models involving GPs. These priors care deeply about which likelihood you use. In fact, if you use them with a different model8, you may not end up with a proper9 posterior. We will talk about those later.\nTo start, let’s look at the simplest way to build a PC prior. We will then talk about why this is not a good idea.\n\nA first crack at a PC prior\nAs always, the best place to start is the simplest possible option. There’s always a hope10 that we won’t need to pull out the big guns.\nSo what is the simplest solution? Well it’s to treat a GP as just a specific multivariate Gaussian distribution \\[\nu \\sim GP(0, \\sigma^2R(\\theta)),\n\\] where \\(R(\\theta)\\) is a correlation matrix.\nThe nice thing about a multivariate Gaussian is that we have a clean expression for its Kullback-Leibler divergence. Wikipedia tells us that for an \\(n\\)-dimensional multivariate Gaussian \\[\n2\\operatorname{KL}(N(0, \\Sigma) || N(0, \\Sigma_0)) = \\operatorname{tr}\\left(\\Sigma_0^{-1}\\Sigma\\right) + \\log \\det \\Sigma_0 - \\log \\det \\Sigma- n.\n\\] To build a PC prior we need to consider a base model. That’s tricky in generality, but as we’ve assumed that the covariance matrix can be decomposed into the variance \\(\\sigma^2\\) and a correlation matrix \\(R(\\theta)\\), we can at least specify an easy base model for \\(\\sigma\\). As always, the simplest model is one with no GP in it, which corresponds to \\(\\sigma_\\text{base} = 0\\). From here, we can follow the usual steps to specify the PC prior \\[\np(\\sigma) = \\lambda e^{-\\lambda \\sigma},\n\\] where we choose \\(\\lambda = \\log(\\alpha)/U\\) for some upper bound \\(U&gt;0\\) and some tail probability \\(0&lt;\\alpha&lt;1\\) so that \\[\n\\Pr(\\theta &gt; U) = \\alpha.\n\\] The specific choice of \\(U\\) will depend on the context. For instance, if it’s logistic regression we probably want something like11 \\(U=1\\). If we have a GP on the log-mean of a Poisson distribution, then we probably want \\(U &lt; 21.5\\) if you want the mean of the Poisson distribution to be less than the maximum integer12 in R. In most data, you’re gonna want13 \\(U\\ll 5\\). If the GP is on the mean of a normal distribution, the choice of \\(U\\) will depend on the context and scaling of the data.\nWithout more assumptions about the form of the covariance function, it is impossible to choose a base model for the other parameters \\(\\theta\\).\nThat said, there is one special case that’s important: the case where \\(\\sigma = \\ell\\) is a single parameter controlling the intrinsic length scale, that is the distance at which the correlation between two points \\(\\ell\\) units apart is approximately zero. The larger \\(\\ell\\) is, the more correlated observations of the GP are and, hence, the less wiggly its realisation is. On the other hand, as \\(\\ell \\rightarrow 0\\), the observations GP often behaves like realisations from an iid Gaussian and the GP becomes14 wilder and wilder.\nThis suggests that a good base model for the length-scale parameter would be \\(\\ell_0 = \\infty\\). We note that if both the base model and the alternative have the same value of \\(\\sigma\\), then it cancels out in the KL-divergence. Under this assumption, we get that \\[\nd(\\ell \\mid \\sigma) = \\text{``}\\lim_{\\ell_0\\rightarrow \\infty}\\text{''} \\sqrt{\\operatorname{tr}\\left(R(\\ell_0)^{-1}R(\\ell)\\right)  - \\log \\det R(\\ell) + \\log \\det R(\\ell_0) - n},\n\\] where I’m being a bit cheeky putting that limit in, as we might need to do some singular model jiggery-pokery of the same type we needed to do for the standard deviation. We will formalise this, I promise.\nAs the model gets more complex as the length scale decreases, we want our prior to control the smallest value \\(\\ell\\) can take. This suggests we want to choose \\(\\lambda\\) to ensure \\[\n\\Pr(\\ell &lt; L) = \\alpha.\n\\] How do we choose the lower bound \\(L\\)? One idea is that our prior should have very little probability of the length scale being smaller than the length-scale of the data. So we can chose \\(L\\) to be the smallest distance between observations (if the data is regularly spaced) or as a low quantile of the distribution of distances between nearest neighbours.\nAll of this will specify a PC prior for a Gaussian process. So let’s now discuss why that prior is a bit shit.\n\n\nWhat’s bad about this?\nThe prior on the standard deviation is fine.\nThe prior on the length scale is more of an issue. There are a couple of bad things about this prior. The first one might seem innocuous at first glance. We decided to treat the GP as a multivariate Gaussian with covariance matrix \\(\\sigma^2 R(\\theta)\\). This is not a neutral choice. In order to do it, we need to commit to a certain set of observation locations15. Why? The matrix \\(R(\\theta)\\) depends entirely on the observation locations and if we use this matrix to define the prior we are tied to those locations.\nThis means that if we change the amount of data in the model we will need to change the prior. This is going to play havoc16 on any sort of cross-validation! It’s worth saying that the other two sources of information (the minimum length scale and the upper bound on \\(\\sigma\\)) are not nearly as sensitive to small changes in the data. This information is, in some sense, fundamental to the problem at hand and, therefore, much more stable ground to build your prior upon.\nThere’s another problem, of course: this prior is expensive to compute. The KL divergence involves computing \\(\\operatorname{tr}(R(\\ell_0)^{-1}R(\\ell))\\) which costs as much as another log-density evaluation for the Gaussian process (which is to say it’s very expensive).\nSo this prior is going to be deeply inconvenient if we have varying amounts of data (through cross-validation or sequential data gathering). It’s also going to be wildly more computationally expensive than you expect a one-dimensional prior to be.\nAll in all, it seems a bit shit.\n\n\nThe Matérn covariance function\nIt won’t be possible to derive a prior for a general Gaussian process, so we are going to need to make some simplifying assumptions. The assumption that we are going to make is that the covariance comes from the Whittle-Matérn17 18 class \\[\nc(s, s') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{8\\nu}\\frac{\\|s-s'\\|}{\\ell}\\right)^\\nu K_\\nu\\left(\\sqrt{8\\nu}\\frac{\\|s-s'\\|}{\\ell}\\right),\n\\] where \\(\\nu\\) is the smoothness parameter, \\(\\ell\\) is the length-scale parameter, \\(\\sigma\\) is the marginal standard deviation, and \\[\nK_\\nu(x) = \\int_0^\\infty e^{-x\\cosh t}\\cosh(\\nu t)\\,dt\n\\] is the modified Bessel19 function of the second kind.\nThis class of covariance function is extremely important in practice. It interpolates between two of the most common covariance functions:\n\nwhen \\(\\nu = 1/2\\), it corresponds to the exponential covariance function,\nwhen \\(\\nu = \\infty\\), it corresponds to the squared exponential covariance.\n\nThere are years of experience suggesting that Matérn covariance functions with finite \\(\\nu\\) will often perform better than the squared exponential covariance.\nCommon practice is to fix20 the value of \\(\\nu\\). There are a few reasons for this. One of the most compelling practical reasons is that we can’t easily evaluate its derivative, which rules out most modern optimisation and MCMC algorithms. It’s also very difficult to think about how you would set a prior on it. The techniques in this post will not help, and as far as I’ve ever been able to tell, nothing else will either. Finally, you could expect there to be horrible confounding between \\(\\nu\\), \\(\\ell\\), and \\(\\sigma\\), which will make inference very hard (both numerically and morally).\nIt turns out that even with \\(\\nu\\) fixed, we will run into a few problems. But to understand those, we are going to need to know a bit more about how inferring parameters in a Gaussian processes actually works.\nJust for future warning, I will occasionally refer to a GP with a Matérn covariance function as a “Matérn field”21.\n\n\nAsymptotic? I barely know her!\nLet’s take a brief detour into classical inference for a moment and ask ourselves when can we recover the parameters of a Gaussian process? For most models we run into in statistics, the answer to that question is when we get enough data. But for Gaussian processes, the story is more complex.\nFirst of all, there is the very real question of what we mean by getting more data. When our observations are iid, this so easy that when asked how she got more data, Kylie just said she “did it again”.\nBut this is more complex once data has dependence. For instance, in a multilevel model you could have the number of groups staying fixed while the number of observations in each group goes to infinity, you could have the number of observations in each group staying fixed while the number of groups go to infinity, or you could have both22 going to infinity.\nFor Gaussian processes it also gets quite complicated. Here is a non-exhaustive list of options:\n\nYou observe the same realisation of the GP at an increasing number of points that eventually cover the whole of \\(\\mathbb{R}^d\\) (this is called the increasing domain or outfill regime); or\nYou observe the same realisation of the GP at an increasing number of points that stay within a fixed domain (this is called the fixed domain or infill regime); or\nYou observe multiple realisations of the same GP at a finite number of points that stay in the same location (this does not have a name, in space-time it’s sometimes called monitoring data); or\nYou observe multiple realisations of the same GP at a (possibly different) finite number of points that can be in different locations for different realisations; or\nYou observe realisations of a process that evolves in space and time (not really a different regime so much as a different problem).\n\nOne of the truly unsettling things about Gaussian processes is that the ability to estimate the parameters depends on which of these regimes you choose!\nOf course, we all know that asymptotic regimes are just polite fantasies that statisticians concoct in order to self-soothe. They are not reflections on reality. They serve approximately the same purpose23 as watching a chain of Law and Order episodes.\nThe point of thinking about what happens when we get more data is to use it as a loose approximation of what happens with the data you have. So the real question is which regime is the most realistic for my data?.\nOne way you can approach this question is to ask yourself what you would do if you had the budget to get more data. My work has mostly been in spatial statistics, in which case the answer is usually24 that you would sample more points in the same area. This suggests that fixed-domain asymptotics is a good fit for my needs. I’d expect that in most GP regression cases, we’re not expecting25 that further observations would be on new parts of the covariate space, which would suggest fixed-domain asymptotics are useful there too.\nThis, it turns out, is awkward.\n\n\nWhen is a parameter not consistently estimatable: an aside that will almost immediately become relevant\nThe problem with a GP with the Matérn covariance function on a fixed domain is that it’s not possible26 to estimate all of its parameters at the same time. This isn’t the case for the other asymptotic regimes, but you’ve got to dance with who you came to the dance with.\nTo make this more concrete, we need to think about a Gaussian process as a realisation of a function rather than as a vector of observations. Why? Because under fixed-domain asymptotics we are seeing values of the function closer and closer together until we essentially see the entire function on that domain.\nOf course, this is why I wrote a long and technical blog post on understanding Gaussian processes as random functions. But don’t worry. You don’t need to have read that part.\nThe key thing is that because a GP is a function, we need to think of it’s probability of being in a set \\(A\\) of functions. There will be a set of function \\(\\operatorname{supp}(u)\\), which we call the support of \\(u(\\cdot)\\), that is the smallest set such that \\[\n\\Pr(u(\\cdot) \\in \\operatorname{supp}(u)) = 1.\n\\] Every GP has an associated support and, while you probably don’t think much about it, GPs are obsessed with their supports. They love them. They hug them. They share them with their friends. They keep them from their enemies. And they are one of the key things that we need to think about in order to understand why it’s hard to estimate parameters in a Matérn covariance function.\nThere is a key theorem that is unique27 to Gaussian processes. It’s usually phrased in terms of Gaussian measures, which are just the probability associated with a GP. For example, if \\(u_1(\\cdot)\\) is a GP then \\[\n\\mu_1(A) = \\Pr(u_1(\\cdot) \\in A)\n\\] is the corresponding Gaussian measure. We can express the support of \\(u(\\cdot)\\) as the smallest set of functions such that \\(\\mu(A)=1\\).\n\nTheorem 1 (Feldman-Hájek theorem) Two Gaussian measures \\(\\mu_1\\) and \\(\\mu_2\\) with corresponding GPs \\(u_1(\\cdot)\\) and \\(u_2(\\cdot)\\) on a locally convex space28 either satisfy, for every29 set \\(A\\),\n\\[\n\\mu_2(A) &gt; 0 \\Rightarrow \\mu_1(A) &gt; 0 \\text{ and } \\mu_1(A) &gt; 0 \\Rightarrow \\mu_2(A) &gt; 0,\n\\] in which case we say that \\(\\mu_1\\) and \\(\\mu_2\\) are equivalent30 (confusingly31 written \\(\\mu_1 \\equiv \\mu_2\\)) and \\(\\operatorname{supp}(u_1) = \\operatorname{supp}(u_2)\\), or \\[\n\\mu_2(A) &gt; 0 \\Rightarrow \\mu_1(A) = 0 \\text{ and } \\mu_1(A) &gt; 0 \\Rightarrow \\mu_2(A) = 0,\n\\] in which case we say \\(\\mu_1\\) and \\(\\mu_2\\) are singular (written \\(\\mu_1 \\perp \\mu_2\\)) and \\(u_1(\\cdot)\\) and \\(u_2(\\cdot)\\) have disjoint supports.\n\nLater on in the post, we will see some precise conditions for when two Gaussian measures are equivalent, but for now it’s worth saying that it is a very delicate property. In fact, if \\(u_2(\\cdot) = \\alpha u_1(\\cdot)\\) for any \\(|\\alpha|\\neq 1\\), then32 \\(\\mu_1 \\perp \\mu_2\\)!\nThis seems like it will cause problems. And it can33. But it’s fabulous for inference.\nTo see this, we can use one of the implications of singularity: \\(\\mu_1 \\perp \\mu_2\\) if and only if \\[\n\\operatorname{KL}(u_1(\\cdot) || u_2(\\cdot)) = \\infty,\n\\] where the the Kullback-Leibler divergence can be interpreted as the expectation of the likelihood ratio of \\(u_1\\) vs \\(u_2\\) under \\(u_1\\). Hence, if \\(u_1(\\cdot)\\) and \\(u_2(\\cdot)\\) are singular, we can (on average) choose the correct one using a likelihood ratio test. This means that we will be able to correctly recover the true34 parameter.\nIt turns out the opposite is also true.\n\nTheorem 2 If \\(\\mu_\\theta\\), \\(\\theta \\in \\Theta\\) is a family of Gaussian measures corresponding to the GPs \\(u_\\theta(\\cdot)\\) and \\(\\mu_\\theta \\equiv \\mu_{\\theta'}\\) for all values of \\(\\theta, \\theta' \\in \\Theta\\), then there is no sequence of estimators \\(\\hat \\theta_n\\) such that, for all \\(\\theta_0 \\in \\Theta\\) \\[\n{\\Pr}_{\\theta_0}(\\hat \\theta_n \\rightarrow \\theta_0) = 1,\n\\] where \\({\\Pr}_{\\theta_0}(\\cdot)\\) is the probability under data drawn with true parameter \\(\\theta_0\\). That is, there is no estimator \\(\\hat \\theta_n\\) that is (strongly) consistent for all \\(\\theta \\in \\Theta\\).\n\n\n\nClick for a surprise (the proof. shit i spoiled the surprise)\n\n\nProof. We are going to do this by contradiction. So assume that there is a sequence such that \\[\n\\Pr{_{\\theta_0}}(\\hat \\theta_n \\rightarrow \\theta_0) = 1.\n\\] For some \\(\\epsilon &gt;0\\), let \\(A_n = \\{\\|\\hat\\theta_n - \\theta_0\\|&gt;\\epsilon\\}\\). Then we can re-state our almost sure convergence as \\[\n\\Pr{_{\\theta_0}}\\left(\\limsup_{n\\rightarrow \\infty}A_n\\right) = 0,\n\\] where the limit superior is defined35 as \\[\n\\limsup_{n\\rightarrow \\infty}A_n = \\bigcap_{n=1}^\\infty \\left(\\bigcup_{m=n}^\\infty A_n\\right).\n\\]\nFor any \\(\\theta' \\neq \\theta_0\\) with \\(\\mu_{\\theta'} \\equiv \\mu_{\\theta_0}\\), the definition of equivalent measures tells us that \\[\n\\Pr{_{\\theta'}}\\left(\\limsup_{n\\rightarrow \\infty}A_n\\right) = 0\n\\] and therefore \\[\n\\Pr{_{\\theta'}}\\left(\\hat \\theta_n \\rightarrow \\theta_0\\right) = 1.\n\\] The problem with this is that is that this data is generated using \\(u_{\\theta'}\\), but the estimator converges to \\(\\theta_0\\) instead of \\(\\theta'\\). Hence, the estimator isn’t uniformly (strongly) consistent.\n\n\nThis seems bad but, you know, it’s a pretty strong version of convergence. And sometimes our brothers and sisters in Christ who are more theoretically minded like to give themselves a treat and consider weaker forms of convergence. It turns out that that’s a disaster too.\n\nTheorem 3 If \\(\\mu_\\theta\\), \\(\\theta \\in \\Theta\\) is a family of Gaussian measures corresponding to the GPs \\(u_\\theta(\\cdot)\\) and \\(\\mu_\\theta \\equiv \\mu_{\\theta'}\\) for all values of \\(\\theta, \\theta' \\in \\Theta\\), then there is no sequence of estimators \\(\\hat \\theta_n\\) such that, for all \\(\\theta_0 \\in \\Theta\\) and all \\(\\epsilon &gt; 0\\) \\[\n\\lim_{n\\rightarrow \\infty}{\\Pr}_{\\theta_0}(\\|\\hat \\theta_n - \\theta_0\\| &gt; \\epsilon) = 0.\n\\] That is there is no estimator \\(\\hat \\theta_n\\) that is (weakly) consistent for all \\(\\theta \\in \\Theta\\).\n\nIf you can’t tell the difference between these two theorems that’s ok. You probably weren’t trying to sublimate some childhood trauma and all of your sexual energy into maths just so you didn’t have to deal with the fact that you might be gay and you were pretty sure that wasn’t an option and anyway it’s not like it’s that important. Like whatever, you don’t need physical or emotional intimacy. You’ve got a pile of books on measure theory next to your bed. You are living your best life. Anyway. It makes almost no practical difference. BUT I WILL PROVE IT ANYWAY.\n\n\nOnce more, into the proof.\n\n\nProof. This proof is based on a kinda advanced fact, which involves every mathematician’s favourite question: what happens along a sub-sequence?\n\n\n\n\n\n\nProbability Fact!\n\n\n\nIf \\(\\hat \\theta_n\\) converges to \\(\\theta\\) in probability, then there exists an infinite sub-sequence \\(\\hat \\theta_{n_k}\\), where \\(n_k \\rightarrow \\infty\\) as \\(k \\rightarrow \\infty\\), such that \\(\\hat \\theta_{n_k}\\) converges to \\(\\theta\\) with probability one (or almost surely).\n\n\nThis basically says that the two modes of convergence are quite similar except convergence in probability is relaxed enough to have some36 values that aren’t doing so good at the whole converging thing.\nWith this in hand, let us build a contradiction. Assume that \\(\\hat \\theta_n\\) is weakly consistent for all \\(\\theta \\in \\Theta\\). Then, if we generate data under \\(\\mu_{\\theta_0}\\), then we get that, along a sub-sequence \\(n_k\\) \\[\n\\Pr{_{\\theta_0}}(\\hat \\theta_{n_k} \\rightarrow \\theta_0) =1.\n\\]\nNow, if \\(\\hat \\theta_n\\) is weakly consistent for all \\(\\theta\\), then so is \\(\\hat \\theta_{n_k}\\). Then, by our assumption, for every \\(\\theta' \\in \\Theta\\) and every \\(\\epsilon&gt;0\\) \\[\n\\lim_{k \\rightarrow \\infty} \\Pr{_{\\theta'}}\\left(\\|\\hat \\theta_{n_k} - \\theta'\\| &gt; \\epsilon\\right) = 0.\n\\]\nOur probability fact tells us that there is a further infinite sub-sub-sequence \\(n_{k_\\ell}\\) such that \\[\n\\Pr{_{\\theta'}}\\left(\\hat \\theta_{n_{k_\\ell}} \\rightarrow \\theta'\\right) = 1.\n\\] But Theorem 2 tells us that \\(\\hat \\theta_{n_k}\\) (and hence \\(\\theta_{n_{k_l}}\\)) satisfies \\[\n\\Pr{_{\\theta'}}\\left(\\hat \\theta_{n_{k_\\ell}} \\rightarrow \\theta_0\\right) = 1.\n\\] This is a contradiction unless \\(\\theta'= \\theta_0\\), which proves the assertion.\n\n\n\n\nMatérn fields under fixed domain asymptotics: the love that dares not speak its name\nAll of that lead up immediately becomes extremely relevant once we learn one thing about Gaussian processes with Matérn covariance functions.\n\nTheorem 4 Let \\(\\mu_{\\nu, \\sigma, \\ell}\\) be the Gaussian measure corresponding to the GP with Matérn covariance function with parameters \\((\\nu, \\sigma, \\ell)\\), let \\(D\\) be any finite domain in \\(\\mathbb{R}^d\\), and let \\(d \\leq 3\\). Then, restricted to \\(D\\), \\[\n\\mu_{\\nu,\\sigma_1, \\ell_1} \\equiv \\mu_{\\nu, \\sigma_2, \\ell_2}\n\\] if and only if \\[\n\\frac{\\sigma_1^2}{\\ell_1^{2\\nu}} = \\frac{\\sigma_2^2}{\\ell_2^{2\\nu}}.\n\\]\n\nI’ll go through the proof of this later, but the techniques require a lot of warm up, so let’s just deal with the consequences for now.\nBasically, Theorem 4 says that we can’t consistently estimate the range and the marginal standard deviation for a one, two, or three dimensional Gaussian process. Hao Zhang noted this and that it remains true37 when dealing with non-Gaussian data.\nThe good news, I guess, is that in more than four38 dimensions the measures are always singular.\nNow, I don’t give one single solitary shit about the existence of consistent estimators. I am doing Bayesian things and this post is supposed to be about setting prior distributions. But it is important. Let’s take a look at some simulations.\nFirst up, let’s look at what happens in 2D when we directly (ie with no noise) observe a zero-mean GP with exponential covariance function (\\(\\nu = 1/2\\)) at points in the unit square. In this case, the log-likelihood is, up to an additive constant, \\[\n\\log p(y \\mid \\theta) = -\\frac{1}{2}\\log |\\Sigma(\\theta)| - \\frac{1}{2}y^T\\Sigma(\\theta)^{-1}y.\n\\]\nThe R code is not pretty but I’m trying to be relatively efficient with my Cholesky factors.\n\nset.seed(24601)\nlibrary(tidyverse)\ncov_fun &lt;- \\(h,sigma, ell) sigma^2 * exp(-h/ell)\n\nlog_lik &lt;- function(sigma, ell, y, h) {\n  V &lt;- cov_fun(h, sigma, ell)\n  R &lt;- chol(V)\n  -sum(log(diag(R))) - 0.5*sum(y * backsolve(R, backsolve(R, y, transpose = TRUE)))\n}\n\nWe can now simulate 500 data points on the unit square, compute their distances, and simulate from the GP.\n\nn &lt;- 500\ndat &lt;- tibble(s1 = runif(n), s2 = runif(n), \n              dist_mat = as.matrix(dist(cbind(s1,s2))),\n              y = MASS::mvrnorm(mu=rep(0,n), \n                      Sigma = cov_fun(dist_mat, 1.0, 0.2)))\n\nWith all of this in hand, let’s look at the likelihood surface along39 the line \\[\n\\frac{\\sigma^2}{\\ell} = c\n\\] for various values of \\(c\\). I’m using some purrr trickery40 here to deal with the fact that sometimes the Cholesky factorisation will throw an error.\n\nm &lt;- 100\nf_direct &lt;- partial(log_lik, y = dat$y, h = dat$dist_mat)\n\npars &lt;- \\(c) tibble(ell = seq(0.05,1, length.out = m),\n                    sigma = sqrt(c * ell), c = rep(c, m))\n\n ll &lt;- map_df(3:8,pars) |&gt;\n  mutate(contour = factor(c), \n         ll = map2_dbl(sigma, ell, \n                       possibly(f_direct, \n                                otherwise = NA_real_)))\n\n\nll |&gt; ggplot(aes(ell, ll, colour = contour)) + \n  geom_line() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can see the same thing in 2D (albeit at a lower resolution for computational reasons). I’m also not computing a bunch of values that I know will just be massively negative.\n\nf_trim &lt;- \\(sigma, ell) ifelse(sigma^2 &lt; 3*ell | sigma^2 &gt; 8*ell,\n                               NA_real_, f_direct(sigma, ell))\nm &lt;- 50\nsurf &lt;- expand_grid(ell = seq(0.05,1,length.out = m),\n                    sigma = seq(0.1, 4, length.out = m)) |&gt;\n  mutate(ll =  map2_dbl(sigma, ell, \n                       possibly(f_trim, otherwise = NA_real_)))\n\nsurf |&gt; filter(ll &gt; 50) |&gt;\n  ggplot(aes(ell, sigma, fill = ll)) + \n  geom_raster() +\n  scale_fill_viridis_c() +\n  theme_bw()\n\n\n\n\n\n\n\n\nClearly there is a ridge in the likelihood surface, which suggests that our posterior is going to be driven by the prior along that ridge.\nFor completeness, let’s run the same experiment again when we have some known observation noise, that is \\(y_i \\sim N(u(s_i), 1)\\). In this case, the log-likelihood is \\[\n\\log p(y\\mid \\sigma, \\ell) = -\\frac{1}{2} \\log \\det(\\Sigma(\\theta) + I) - \\frac{1}{2}y^{T}(\\Sigma(\\theta) + I)^{-1}y.\n\\]\nLet us do the exact same thing again!\n\nn &lt;- 500\ndat &lt;- tibble(s1 = runif(n), s2 = runif(n), \n              dist_mat = as.matrix(dist(cbind(s1,s2))),\n              mu = MASS::mvrnorm(mu=rep(0,n), \n                      Sigma = cov_fun(dist_mat, 1.0, 0.2)),\n              y = rnorm(n, mu, 1))\n\nlog_lik &lt;- function(sigma, ell, y, h) {\n  V &lt;- cov_fun(h, sigma, ell)\n  R &lt;- chol(V + diag(dim(V)[1]))\n  -sum(log(diag(R))) - 0.5*sum(y * backsolve(R, backsolve(R, y, transpose = TRUE)))\n}\n\nm &lt;- 100\nf &lt;- partial(log_lik, y = dat$y, h = dat$dist_mat)\n\npars &lt;- \\(c) tibble(ell = seq(0.05,1, length.out = m),\n                    sigma = sqrt(c * ell), c = rep(c, m))\n\n ll &lt;- map_df(seq(0.1, 10, length.out = 30),pars) |&gt;\n  mutate(contour = factor(c), \n         ll = map2_dbl(sigma, ell, \n                       possibly(f, otherwise = NA_real_)))\n\n\nll |&gt; ggplot(aes(ell, ll, colour = contour)) + \n  geom_line(show.legend = FALSE) +\n  #scale_color_brewer(palette = \"Set1\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nf_trim &lt;- \\(sigma, ell) ifelse(sigma^2 &lt; 0.1*ell | sigma^2 &gt; 10*ell,\n                               NA_real_, f(sigma, ell))\nm &lt;- 20\nsurf &lt;- expand_grid(ell = seq(0.05,1,length.out = m),\n                    sigma = seq(0.1, 4, length.out = m)) |&gt;\n  mutate(ll =  map2_dbl(sigma, ell, \n                       possibly(f_trim, otherwise = NA_real_)))\n\nsurf |&gt; filter(ll &gt; -360) |&gt;\n  ggplot(aes(ell, sigma, fill = ll)) + \n  geom_raster() +\n  scale_fill_viridis_c() +\n  theme_bw()\n\n\n\n\n\n\n\n\nOnce again, we can see that there is going to be a ridge in the likelihood surface! It’s a bit less disastrous this time, but it’s not excellent even with 500 observations (which is a decent number on a unit square). The weird structure of the likelihood is still going to lead to a long, non-elliptical shape in your posterior that your computational engine (and your person interpreting the results) are going to have to come to terms with. In particular, if you only look at the posterior marginal distributions for \\(\\sigma\\) and \\(\\ell\\) you may miss the fact that \\(\\sigma \\ell^{\\nu}\\) is quite well estimated by the data even though the marginals for both \\(\\sigma\\) and \\(\\ell\\) are very wide.\nThis ridge in the likelihood is going to translate somewhat into a ridge in the prior. We will see below that how much of that ridge we see is going to be very dependent on how we specify the prior. The entire purpose of the PC prior is to meaningfully resolve this ridge using sensible prior information.\nBut before we get to the (improved) PC prior, it’s worthwhile to survey some other priors that have been proposed in the literature.\n\n\nSo the prior is important then! What do other people do?\nThat ridge in the likelihood surface does not go away in low dimensions, which essentially means that our inference along that ridge is going to be driven by the prior.\nPossibly the worst choice you could make in this situation is trying to make a minimally informative prior. Of course, that’s what somebody did when they made a reference prior for the problem. In fact it was the first paper41 that looks rigorously at prior distributions on the parameters of GPs. It’s just unfortunate that it’s quite shit. It has still been cited quite a lot. And there are some technical advances to the theory of reference priors, but if you use it you just find yourself mapping out that damn ridge.\nOn top of being, structurally, a bad choice, the reference prior has a few other downsides:\n\nIt is very computationally intensive and quite complex. Not unlike the bad version of the PC prior!\nIt requires strong assumptions about the likelihood. The first version assumed that there was no observation noise. Later papers allowed there to be observation noise. But only if it’s Gaussian.\nIt is derived under the asymptotic regime where an infinite sequence of different independent realisations of the GP are observed at the same finite set of points. This is not the most useful regime for GPs.\n\nAll in all, it’s a bit of a casserole.\nFrom the other end, there’s a very interesting contribution from Aad van der Vaart and Harry van Zanten wrote a very lovely theoretical paper that looked at which priors on \\(\\ell\\) could result in theoretically optimal contraction rates for the posterior of \\(u(\\cdot)\\). They argued that \\(\\ell^{-d}\\) should have a Gamma distribution. Within the Matérn class, their results are only valid for the squared exponential contrivance function.\nOne of the stranger things that I have never fully understood is that the argument I’m going to make below ends up with a gamma distribution on \\(\\ell^{-d/2}\\), which is somewhat different to van der Vaart and van Zanten. If I was to being forced to bullshit some justification I’d probably say something about the Matérn process depending only on the distance between observations makes the \\(d\\)-sphere the natural geometry (the volume of which scales like \\(\\ell^{-d/2}\\)) rather than the \\(d\\)-cube (the volume of which scales lie \\(\\ell^{-d}\\)). But that would be total bullshit. I simply have no idea. They’re proposal comes via the time-honoured tradition of “constant chasing” in some fairly tricky proofs, so I have absolutely no intuition for it.\nWe also found in other contexts that use the KL divergence rather than its square root tended to perform worse. So I’m kinda happy with our scaling and, really, their paper doesn’t cover the covariance functions I’m considering in this post.\nNeither42 of these papers consider that ridge in the likelihood surface.\nThis lack of consideration—as well as their success in everything else we tried them on—was a big part of our push to make a useful version of a PC prior for Gaussian processes.\n\n\nRescuing the PC prior on \\(\\ell\\); or What I recommend you do\nIt has been a long journey, but we are finally where I wanted us to be. So let’s talk about how to fix the PC prior. In particular, I’m going to go through how to derive a prior on the length scale \\(\\ell\\) that has a simple form.\nIn order to solve this problem, we are going to do three things in the rest of this post:\n\nRestrict our attention to the stationary43 GPs\nRestrict our attention to the Matérn class of covariance functions.\nGreatly increase our mathematical44 sophistication.\n\nBut before we do that, I’m going to walk you through the punchline.\nThis work was originally done with the magnificent Geir-Arne Fuglstad, the glorious Finn Lindren, and the resplendent Håvard Rue. If you want to read the original paper, the preprint is here45.\nThe PC prior is derived using the base model \\(\\ell = \\infty\\), which might seem like a slightly weird choice. The intuition behind it is that if there is strong dependence between far away points, the realisations of \\(u(\\cdot)\\) cannot be too wiggly. In some context people talk about \\(\\ell\\) as a “smoothness”46 parameter because realisations with large \\(\\ell\\) “look”47 smoother than realisations with small \\(\\ell\\).\nAnother way to see the same thing is to note that a Matérn field approaches a48 smoothing spline prior, in which case \\(\\sigma^{-2}\\) plays the role of the “smoothing parameter” of the spline. In that case, the natural base model of \\(\\sigma=0\\) interacts with the base model of \\(\\ell = \\infty\\) to shrink towards an increasingly flat surface centred on zero.\nWe still need to choose a quantity of interest in order to encode some explicit information in the prior. In this case, I’m going to use the idea that for any data set, we only have information up to a certain spatial resolution. In that case, we don’t want to put prior mass on the length scale being less than that resolution. Why? Well any inference about \\(\\ell\\) at a smaller scale than the data resolution is going to be driven entirely by unverifiable model assumptions. And that feels a bit awkward. This suggests that we chose a minimum49 length scale \\(L\\) and choose the scaling parameter in the PC prior so that \\[\n\\Pr(\\ell &lt; L) &lt; \\alpha_\\ell.\n\\]\nUnder these assumptions, the PC prior for the length scale in a \\(d\\)-dimensional space is50 a Fréchet distribution51 with shape parameter \\(d/2\\) and scale parameter \\(\\lambda_\\ell^{2/d}\\). That is, \\[\np(\\ell) = \\frac{d\\lambda_\\ell}{2} \\ell^{-(d/2+1)}e^{-\\lambda_{\\ell}\\ell^{-d/2}},\n\\] where we choose \\(\\lambda_\\ell = -\\log(\\alpha_\\ell)L^{d/2}\\) to ensure that \\[\n\\Pr(\\ell &lt; L) = e^{-\\lambda L^{-d/2}} &lt; \\alpha_\\ell.\n\\]\nIn two dimensions, this is an inverse gamma prior, which gives rigorous justification to a commonly used prior in spatial statistics.\n\n\nComparing it with the reference prior\nOk, so let’s actually see how much of a difference using a weakly informative prior makes relative to using the reference prior.\nIn the interest of computational speed, I’m going to use the simplest possible model setup, \\[\ny \\mid \\sigma,\\ell \\sim N(0, \\sigma^2 R(\\ell)),\n\\] and I’m only going to use 25 observations.\nIn this case52 is \\[\np(\\ell, \\sigma) = \\sigma^{-1}\\left(\\operatorname{tr}\\left[\\left(\\frac{\\partial R}{\\partial \\ell}R^{-1}\\right)^2\\right] - \\frac{1}{n}\\operatorname{tr}\\left(\\frac{\\partial R}{\\partial \\ell}R^{-1}\\right)^2\\right)^{1/2}.\n\\]\nEven with this limited setup, it took a lot of work to make Stan sample this posterior. You’ll notice that I did a ridge-aware reparameterisation. I also had to run twice as much warm up as I ordinarily would.\nThe Stan code is under the fold.\n\n\nShow the Stan code!\nfunctions {\n  matrix cov(int N, matrix s,  real ell) {\n    matrix[N,N] R;\n    row_vector[2] s1, s2;\n    for (i in 1:N) {\n      for (j in 1:N){\n        s1 = s[i, 1:2];\n        s2 = s[j, 1:2];\n        R[i,j] = exp(-sqrt(dot_self(s1-s2))/ell);\n      }\n    }\n    return 0.5 * (R + R');\n  }\n  matrix cov_diff(int N, matrix s,  real ell) {\n    // dR /d ell = cov(N, p ,s, sigma2*|x-y|/ell^2, ell)\n    matrix[N,N] R;\n    row_vector[2] s1, s2;\n    for (i in 1:N) {\n      for (j in 1:N){\n        s1 = s[i, 1:2];\n        s2 = s[j, 1:2];\n        R[i,j] =  sqrt(dot_self(s1-s2)) * exp(-sqrt(dot_self(s1-s2))/ell) / ell^2 ;\n      }\n    }\n    return 0.5 * (R + R');\n  }\n\n  real log_prior(int N, matrix s, real sigma2, real ell) {\n    matrix[N,N] R = cov(N, s,  ell);\n    matrix[N,N] W = (cov_diff(N, s, ell)) / R;\n    return 0.5 * log(trace(W * W) - (1.0 / (N)) * (trace(W))^2) - log(sigma2);\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  matrix[N,2] s;\n}\n\nparameters {\n  real&lt;lower=0&gt; sigma2;\n  real&lt;lower=0&gt; ell;\n}\n\nmodel {\n  {\n    matrix[N,N] R = cov(N, s, ell);\n    target += multi_normal_lpdf(y | rep_vector(0.0, N), sigma2 * R);\n  }\n  target += log_prior(N,  s, sigma2, ell);\n}\n\ngenerated quantities {\n  real sigma = sqrt(sigma2);\n}\n\n\nBy comparison, the code for the PC prior is fairly simple.\n\nfunctions {\n  matrix cov(int N, matrix s, real sigma, real ell) {\n    matrix[N,N] R;\n    row_vector[2] s1, s2;\n    real sigma2 = sigma * sigma;\n    for (i in 1:N) {\n      for (j in 1:N){\n        s1 = s[i, 1:2];\n        s2 = s[j, 1:2];\n        R[i,j] = sigma2 * exp(-sqrt(dot_self(s1-s2))/ell);\n      }\n    }\n    return 0.5 * (R + R');\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  matrix[N,2] s;\n  real&lt;lower = 0&gt; lambda_ell;\n  real&lt;lower = 0&gt; lambda_sigma;\n}\n\nparameters {\n  real&lt;lower=0&gt; sigma;\n  real&lt;lower=0&gt; ell;\n}\n\nmodel {\n  matrix[N,N] R = cov(N, s, sigma, ell);\n  y ~ multi_normal(rep_vector(0.0, N), R);\n  sigma ~ exponential(lambda_sigma);\n  ell ~ frechet(1, lambda_ell); // Only in 2D\n}\n\n// generated quantities {\n//   real check = 0.0; // should be the same as lp__\n//   { // I don't want to print R!\n//     matrix[N,N] R = cov(N, s, sigma, ell);\n//     check -= 0.5* dot_product(y,(R\\ y)) + 0.5 * log_determinant(R);\n//     check += log(sigma) - lambda_sigma * sigma;\n//     check += log(ell) - 2.0 * log(ell) - lambda_ell / ell;\n//   }\n// }\n\nThis is a lot easier than the code for the reference prior.\nLet’s compare the results on some simulated data. Here I’m choosing \\(\\alpha_\\ell = \\alpha_\\sigma = 0.05\\), \\(L_\\ell = 0.05\\), and \\(U_\\sigma = 5\\).\n\nlibrary(cmdstanr)\nlibrary(posterior)\nn &lt;- 25\n\ndat &lt;- tibble(s1 = runif(n), s2 = runif(n), \n              dist_mat = as.matrix(dist(cbind(s1,s2))),\n              y = MASS::mvrnorm(mu=rep(0,n), \n                                Sigma = cov_fun(dist_mat, 1.0, 0.2)))\n\nstan_dat &lt;- list(y = dat$y,\n                 s = cbind(dat$s1,dat$s2),\n                 N = n,\n                 lambda_ell = -log(0.05)*sqrt(0.05),\n                 lambda_sigma = -log(0.05)/5)\n\nmod_ref &lt;- cmdstan_model(\"gp_ref_no_mean.stan\")\nmod_pc &lt;- cmdstan_model(\"gp_pc_no_mean.stan\")\n\nFirst off, let’s look at the parameter estimates from the reference prior\n\nfit_ref &lt;- mod_ref$sample(data = stan_dat, \n                          seed = 30127, \n                          parallel_chains = 4, \n                          iter_warmup = 2000,\n                          iter_sampling = 2000,\n                          refresh = 0) \n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 41.6 seconds.\nChain 2 finished in 43.4 seconds.\nChain 4 finished in 44.8 seconds.\nChain 3 finished in 47.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 44.2 seconds.\nTotal execution time: 47.2 seconds.\n\nfit_ref$print(digits = 2)\n\n variable   mean median     sd  mad     q5    q95 rhat ess_bulk ess_tail\n   lp__   -30.95 -30.57   1.24 0.89 -33.46 -29.79 1.00     1397      896\n   sigma2  32.56   1.28 823.19 0.58   0.69   7.19 1.00      979      562\n   ell      9.04   0.26 240.39 0.16   0.11   1.88 1.00      927      542\n   sigma    1.67   1.13   5.46 0.27   0.83   2.68 1.00      979      562\n\n\nIt also took a bloody long time.\nNow let’s check in with the PC prior.\n\nfit_pc &lt;- mod_pc$sample(data = stan_dat, \n                          seed = 30127, \n                          parallel_chains = 4,\n                          iter_sampling = 2000,\n                          refresh = 0) \n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 4.9 seconds.\nChain 4 finished in 5.1 seconds.\nChain 3 finished in 5.4 seconds.\nChain 2 finished in 5.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 5.2 seconds.\nTotal execution time: 5.6 seconds.\n\nfit_pc$print(digits = 2)\n\n variable   mean median   sd  mad     q5   q95 rhat ess_bulk ess_tail\n    lp__  -10.36 -10.05 1.02 0.76 -12.42 -9.36 1.00     2160     3228\n    sigma   1.52   1.36 0.60 0.41   0.92  2.72 1.00     1424     1853\n    ell     0.67   0.45 0.72 0.27   0.19  1.89 1.00     1338     1694\n\n\nYou’ll notice two things there: it did a much better job at sampling and it was much faster.\nFinally, let’s look at some plots. First off, let’s look at some 2D density plots.\n\nlibrary(cowplot)\nsamps_ref &lt;- fit_ref$draws(format = \"draws_df\")\nsamps_pc &lt;- fit_pc$draws(format = \"draws_df\")\n\np1 &lt;- samps_ref |&gt;  ggplot(aes(ell, sigma)) +\n  geom_hex() +\n  scale_color_viridis_c()\np2 &lt;- samps_pc |&gt;  ggplot(aes(ell, sigma)) +\n  geom_hex() +\n  scale_color_viridis_c()\n\nplot_grid(p1,p2)\n\n\n\n\n\n\n\n\nIt would be interesting to look at how different the densities for \\(\\ell\\) are.\n\nsamps_pc |&gt; ggplot(aes(ell)) +\n  geom_density() +\n  geom_density(aes(samps_ref$ell), colour = \"red\") +\n  xlim(0,2)\n\n\n\n\n\n\n\n\nAs expected, the PC prior (black) pulls the posterior towards the base model (\\(\\ell = \\infty\\)), but what is interesting to me is that the posterior for the reference prior (red) has so much mass near zero. That’s the one thing we didn’t want to happen.\nWe can look closer at this by looking at the posterior for \\(\\kappa = 2\\ell^{-1}\\).\n\np3 &lt;- samps_ref |&gt; \n  mutate(kappa = 2/ell) |&gt;\n  ggplot(aes(kappa, sigma)) +\n  geom_hex() +\n  scale_color_viridis_c()\np4 &lt;- samps_pc |&gt;  \n  mutate(kappa = 2/ell) |&gt;\n  ggplot(aes(kappa, sigma)) +\n  geom_hex() +\n  scale_color_viridis_c()\n\nplot_grid(p3, p4)\n\n\n\n\n\n\n\n\nTo be brutally francis with you all, I’m not sure how much I trust that Stan posterior, so I’m going to look at the posterior along the ridge.\n\nlog_prior &lt;- function(sigma, ell) {\n  V &lt;- cov_fun(dat$dist_mat, sigma, ell)\n  dV &lt;- (V * dat$dist_mat)/ell^2\n  U &lt;- t(solve(V, dV))\n  lprior &lt;- 0.5 * log(sum(diag(U %*% U)) - sum(diag(U))^2/n) - log(sigma)\n}\n\nlog_posterior &lt;- \\(sigma, ell) log_prior(sigma, ell) + f_direct(sigma, ell)\n\nm &lt;- 500\npars &lt;- \\(c) tibble(ell = seq(0.001,2, length.out = m),\n                    sigma = sqrt(c * ell), c = rep(c, m))\n\nlpost &lt;- map_df(seq(0.001, 8, length.out = 200),pars) |&gt;\n  mutate(tau = c, \n         log_posterior = map2_dbl(sigma, ell, \n                       possibly(log_posterior, otherwise = NA_real_)))\n\n\nlpost |&gt;\n  filter(log_posterior &gt; -20) |&gt;\n  ggplot(aes(ell, log_posterior, colour = tau, group = tau)) + \n  geom_line() +\n  #scale_color_brewer(palette = \"Set1\") +\n  theme_bw() \n\n\n\n\n\n\n\n\nWe can compare this with the likelihood surface.\n\nllik &lt;- map_df(seq(0.001, 8, length.out = 200),pars) |&gt;\n  mutate(tau = c, \n         log_likelihood = map2_dbl(sigma, ell, \n                       possibly(f_direct, otherwise = NA_real_)))\n\nlprior &lt;- map_df(seq(0.001, 8, length.out = 200),pars) |&gt;\n  mutate(tau = c, \n         log_prior = map2_dbl(sigma, ell, \n                       possibly(log_prior, otherwise = NA_real_)))\n\np1 &lt;- llik |&gt;\n  filter(log_likelihood &gt; -50) |&gt;\n  ggplot(aes(ell, log_likelihood, colour = tau, group = tau)) + \n  geom_line() +\n  #scale_color_brewer(palette = \"Set1\") +\n  theme_bw() \n\np2 &lt;- lprior |&gt;\n  filter(log_prior &gt; -20) |&gt;\n  ggplot(aes(ell, log_prior, colour = tau, group = tau)) + \n  geom_line() +\n  #scale_color_brewer(palette = \"Set1\") +\n  theme_bw() \nplot_grid(p1, p2)\n\n\n\n\n\n\n\n\nYou can see here that the prior is putting a lot of weight at zero relative to the likelihood surface, which is relatively flat.\nIt’s also important to notice that the ridge isn’t as flat with \\(n=25\\) as it is with \\(n=500\\). It would be very interesting to repeat this with larger values of \\(n\\), but frankly I do not have the time.\n\n\nMoving beyond the Matérn\nThere is a lot more to say on this topic. But honestly this blog post is already enormous (you are a bit over halfway if you choose to read the technical guff). So I’m just going to summarise some of the things that I think are important here.\nFirstly, the rigorous construction of the PC prior only makes sense when \\(d \\leq 3\\). This is a bit annoying, but it is what it is. I would argue that this construction is still fairly reasonable in moderate dimensions. (In high dimensions I think we need more research.)\nThere are two ways to see that. Firstly, if you look at the derivation of the distance, it involves an infinite sum that only converges when \\(d &lt; 4\\). But mathematically, if we can show53 that the partial sums can be bounded independently of \\(\\ell\\), then we can just send another thing to infinity when we send the domain size and the base model length scale there.\nA different way is to see this is to note that the PC prior distance is \\(d(\\ell) = \\ell^{-d/2}\\). This is proportional to the inverse of the volume of the \\(d\\)-sphere54 of radius \\(\\ell\\). This doesn’t seem like a massively useful observation, but just wait.\nWhat if we ask ourselves “what is the average variance of \\(u(s)\\) over a ball of radius \\(r\\)?”. If we write \\(c_{\\ell,\\sigma}(h)\\) as the Matérn covariance function, then55 \\[\n\\operatorname{Var}\\left(\\frac{1}{\\operatorname{Vol}(\\mathbb{B}_d(r))}\\int_{\\mathbb{B}_d(r)}u(s)\\,ds\\right) = \\frac{1}{\\operatorname{Vol}(\\mathbb{B}_d(r))} \\int_0^\\infty \\tilde{c}_{\\ell, \\sigma}(t) t^{d-1}\\,dt,\n\\] where \\(\\tilde c_{\\ell, \\sigma}(t) = c_{\\ell, \\sigma}(h)\\) for all \\(\\|h\\| = t\\). If we remember that \\(c_{\\ell, \\sigma}(s) = c_{1, \\sigma}(\\ell s)\\), then we can write this as \\[\n\\frac{1}{\\operatorname{Vol}(\\mathbb{B}_d(r))} \\int_0^\\infty \\tilde{c}_{1, \\sigma}(\\ell t) t^{d-1}\\,dt = \\frac{\\ell^{-d}}{\\operatorname{Vol}(\\mathbb{B}_d(r))} \\int_0^\\infty \\tilde{c}_{1, \\sigma}(v) v^{d-1}\\,dv.\n\\] Hence the PC prior on \\(\\ell\\) is penalising the change in average standard deviation over a ball relative to the unit length scale. With this interpretation, the base model is, once again, zero standard deviation. This reasoning carries over to the length scale parameter in any56 Gaussian process.\nThis post only covers the simplest version of Matérn GPs. One simple extension is to construct a non-stationary GP by replacing the Euclidean distance with the distance on a manifold with volume element \\(R(s)\\,ds\\). This might seem like a weird and abstract thing to do, but it’s an intrinsic specification of the popular deformation method due to Guttorp and Samson. Our paper covers the prior specification in this case.\nThe other common case that I’ve not considered here is the extension where there is a different length scale57 in each dimension. In this case, we could compute a PC prior independently for each dimension (so \\(d=1\\) for each prior). To be completely honest with you, I worry a little bit about that choice in high dimensions58 (products of independent priors being notoriously weird), but I don’t have a better suggestion.\n\n\nWhat’s in the rest of the post?\nSo you might have noticed that even though the previous section is a “conclusion” section, there is quite a bit more blog to go. I shan’t lie: this whole thing up to this point is a tl;dr that got wildly out of control.\nThe rest of the post is the details.\nThere are two parts. The first part covers enough59 of the theory of stationary GPs to allow us to understand the second part, which actually derives the PC prior.\nIt’s going to get a bit hairy and I’m going to assume you’ve at least skimmed through the first 2 definitions in my previous post defining GPs.\nI fully expect that most people will want to stop reading here. But you shouldn’t. Because if I had to suffer you all have to suffer."
  },
  {
    "objectID": "posts/2022-09-07-priors5/priors5.html#part-2-an-invitation-to-the-theory-of-stationary-gaussian-processes",
    "href": "posts/2022-09-07-priors5/priors5.html#part-2-an-invitation-to-the-theory-of-stationary-gaussian-processes",
    "title": "Priors for the parameters in a Gaussian process",
    "section": "Part 2: An invitation to the theory of Stationary Gaussian processes",
    "text": "Part 2: An invitation to the theory of Stationary Gaussian processes\nGaussian processes with the Matérn covariance function are an excellent example of a stationary60 Gaussian process, which are characterised61 62 by have covariance functions of the form \\[\nc(s, s') = c(s- s'),\n\\] where I am abusing notation and using \\(c\\) for both the two parameter and one parameter functions. This assumption means that the correlation structure does not depend on where you are in space, only on the distance between points.\nThe assumption of stationarity massively simplifies GPs. Firstly, the stationarity assumption greatly reduces the number of parameters you need to describe a GP as we don’t need to worry about location-specific parameters. Secondly, it increases the statistical power of the data. If two subsets of the domain are more than \\(2\\ell\\) apart, they are essentially independent replicates of the GP with the same parameters. This means that if the locations \\(s\\) vary across a large enough area (relative to the natural length scale), we get multiple effective replicates63 from the same realisation of the process.\nIn practice, stationarity64 is often a good enough assumption when the mean has been modelled carefully, especially given the limitations of the data. That said, priors on non-stationary processes can be set using the PC prior methodology by using a stationary process as the base model. The supplementary material of our paper gives a simple, but useful, example of this.\n\nStationary covariance functions and Bochner’s theorem\nThe restriction to stationary processes is extremely powerful. It opens us up to using Fourier analysis as a potent tool for understanding GPs. We are going to need this to construct our KL divergence, and so with some trepidation, let’s dive into the moonee ponds of spectral representations.\nThe first thing that we need to do is remember what a Fourier transform is. A Fourier transform of a square integrable function \\(\\phi(s)\\) is65 \\[\n\\hat \\phi(\\omega) = \\mathcal{F}(\\phi)(\\omega) =\\frac{1}{(2\\pi)^d}\\int_{\\mathbb{R}^d} e^{-i\\omega^Ts}\\phi(s) \\,ds.\n\\]\nIf you have bad memories66 of desperately trying to compute Fourier integrals in undergrad, I promise you that we are not doing that today. We are simply affirming their right to exist (and my right to look them up in a table).\nThe reason I care about Fourier67 transforms is that if I have a non-negative measure68 \\(\\nu\\), I can define a function \\[\nc(h) = \\int_{\\mathbb{R}^d}e^{i\\omega^Th}\\,d\\nu(\\omega).\n\\] If measures freak you out, you can—with some loss of generality—assume that there is a function \\(f(\\omega)\\geq 0\\) such that \\[\nc(h) = \\int_{\\mathbb{R}^d}e^{i\\omega^Th}f(\\omega)\\,d\\omega.\n\\] We are going to call \\(\\nu\\) the spectral measure and the corresponding \\(f\\), if it exists, is called the spectral density.\nI put it to you that, defined this way, \\(c(s,s') = c(s - s')\\) is a (complex) positive definite function.\nRecall69 that a function is positive definite if, for every for every \\(k&gt;0\\), every \\(s_1, \\ldots, s_k \\in \\mathbb{R}^d\\), and every \\(a_1, \\ldots, a_k \\in \\mathbb{C}\\) \\[\n\\sum_{i = 1}^k\\sum_{j=1}^k a_i\\bar{a}_j c(s_i, s_j) \\geq 0,\n\\] where \\(\\bar a\\) is the complex conjugate of \\(a\\).\nUsing our assumption about \\(c(\\cdot)\\) we can write the left hand side as \\[\\begin{align*}\n\\sum_{i = 1}^k\\sum_{j=1}^k a_i\\bar{a}_j c(s_i, s_j) &= \\sum_{i = 1}^k\\sum_{j=1}^k a_i\\bar{a}_j c(s_i- s_j) \\\\\n&=\\sum_{i = 1}^k\\sum_{j=1}^k a_i\\bar{a}_j \\int_{\\mathbb{R}^d} e^{i\\omega^T(s_i-s_j)}\\,d\\nu(\\omega) \\\\\n&=\\int_{\\mathbb{R}^d}\\sum_{i = 1}^k\\sum_{j=1}^k a_i\\bar{a}_j e^{i\\omega^T(s_i-s_j)}\\,d\\nu(\\omega) \\\\\n&=\\int_{\\mathbb{R}^d}\\left(\\sum_{i = 1}^k a_i e^{i\\omega^Ts_i}\\right)\\left(\\sum_{j = 1}^k \\bar{a_j} e^{-i\\omega^Ts_j}\\right) \\,d\\nu(\\omega)\\\\\n&=\\int_{\\mathbb{R}^d}\\left(\\sum_{i = 1}^k a_i e^{i\\omega^Ts_i}\\right)\\overline{\\left(\\sum_{j = 1}^k a_j e^{i\\omega^Ts_j}\\right)} \\,d\\nu(\\omega) \\\\\n&=\\int_{\\mathbb{R}^d}\\left|\\sum_{i = 1}^k a_i e^{i\\omega^Ts_i}\\right|^2\\,d\\nu(\\omega) \\geq 0,\n\\end{align*}\\] where \\(|a|^2 = a\\bar{a}\\).\nWe have shown that if \\(c(s,s') = c(s-s') = \\int e^{i\\omega^T(s-s')}\\,d \\nu(\\omega)\\) , then it is a valid covariance function. This is also true, although much harder to prove, in the other direction and the result is known as Bochner’s theorem.\n\nTheorem 5 (Bochner’s theorem) A function \\(c(\\cdot)\\) is positive definite, ie for every \\(k&gt;0\\), every \\(s_1, \\ldots, s_k \\in \\mathbb{R}^d\\), and every \\(a_1, \\ldots, a_k \\in \\mathbb{C}\\) \\[\n\\sum_{i = 1}^k\\sum_{j=1}^k a_i\\bar{a}_j c(s_i- s_j) \\geq 0,\n\\] if and only if there is a non-negative finite measure \\(\\nu\\) such that \\[\nc(h) = \\int_{\\mathbb{R}^d} e^{i\\omega^Th}\\,d\\nu(\\omega).\n\\]\n\nJust as a covariance function70 is enough to completely specify a zero-mean Gaussian process, a spectral measure is enough to completely specify a zero mean stationary Gaussian process.\nOur lives are mathematically much easier when \\(\\nu\\) represents a density \\(f(\\omega)\\) that satisfies \\[\n\\int_{\\mathbb{R}^d}\\phi(\\omega)\\,d\\nu(\\omega) = \\int_{\\mathbb{R}^d}\\phi(\\omega)f(\\omega)\\,d\\omega.\n\\] This function, when it exists, is precisely the Fourier transform of \\(c(h)\\). Unfortunately, this will not exist71 for all possible positive definite functions. But as we drift further and further down this post, we will begin to assume that we’re only dealing with cases where \\(f\\) exists.\nThe case of particular interest to us is the Matérn covariance function. The parameterisation used above is really lovely, but for mathematical convenience, we are going to set72 \\(\\kappa = \\sqrt{8\\nu}\\ell^{-1}\\), which has73 Fourier transform \\[\\begin{align*}\nf(\\omega) &= \\frac{\\Gamma(\\nu+d/2)\\kappa^{2\\nu}\\sigma^2}{4^{d}\\pi^{d/2}\\Gamma(\\nu)}\\frac{1}{(\\kappa^2 + \\|\\omega\\|^2)^{\\nu+d/2}}\\\\\n&= C_\\text{Matérn}(\\nu,d).\\kappa^{2\\nu}\\sigma^2 \\frac{1}{(\\kappa^2 + \\|\\omega\\|^2)^{\\nu+d/2}},\n\\end{align*}\\] where \\(C_\\text{Matérn}(\\nu,d)\\) is defined implicitly above and is a constant (as we are keeping \\(\\nu\\) fixed).\n\n\nSpectral representations (and the simplest of the many many versions of a stochastic integral)\nTo see this, we need a tiny bit of machinery. Specifically, we need the concept of a Gaussian \\(\\nu\\)-noise and its corresponding integral.\n\nDefinition 1 (Complex \\(\\nu\\)-noise) A (complex) \\(\\nu\\)-noise74 is a random measure75 \\(Z_\\nu(\\cdot)\\) such that, for every76 disjoint77 pair of sets \\(A, B\\) satisfies the following properties\n\n\\(Z_\\nu(A)\\) has mean zero and variance \\(\\nu(A)\\),\nIf \\(A\\) and \\(B\\) are disjoint then \\(Z_\\nu(A\\cup B) = Z_\\nu(A) + Z_\\nu(B)\\)\nIf \\(A\\) and \\(B\\) are disjoint then \\(Z_\\nu(A)\\) and \\(Z_\\nu(B)\\) are uncorrelated78, ie \\(\\mathbb{E}(Z_\\nu(A) \\overline{Z_\\nu(B)}) = 0\\).\n\n\nThis definition might not seem like much, but imagine a simple79 piecewise constant function \\[\nf(\\omega) = \\sum_{i=1}^{n} f_i 1_{A_i}(\\omega),\\quad g(\\omega) =  \\sum_{i=1}^{n} g_i 1_{A_i}(\\omega)\n\\] where \\(f_i, g_i\\in \\mathbb{C}\\) and the sets \\(A_i\\) are pairwise disjoint and \\(\\bigcup_{i=1}^n A_i  = \\mathbb{R}^d\\). Then we can define an integral with respect to the \\(\\nu\\)-noise as \\[\n\\int_{\\mathbb{R}^d} f(\\omega)\\,dZ_\\nu(\\omega) = \\sum_{i=1}^n f_i Z_\\nu(A_i),\n\\] which has mean \\(0\\) and variance \\[\n\\mathbb{E}\\left(\\int_{\\mathbb{R}^d} f(\\omega)\\,dZ_\\nu(\\omega)\\right)^2 = \\sum_{i=1}^n f_i^2 \\nu(A_i) = \\int_{\\mathbb{R}^d}f(\\omega)^2\\,d\\nu(\\omega),\n\\] where the first equality comes from noting that \\(\\int_{A_i} \\,dZ_v(\\omega)\\) and \\(\\int_{A_j} \\, dZ_v(\\omega)\\) are uncorrelated and the last equality comes from the definition of an integral of a piecewise constant function.\nMoreover, we get the covariance \\[\\begin{align*}\n\\mathbb{E}\\left(\\int_{\\mathbb{R}^d} f(\\omega)\\,dZ_\\nu(\\omega)\\overline{\\int_{\\mathbb{R}^d} g(\\omega)\\,dZ_\\nu(\\omega)}\\right) &= \\sum_{i=1}^n \\sum_{j=1}^n f_i g_j \\nu(A_i \\cap A_j) \\\\\n&= \\sum_{i=1}^n f_i\\overline{g}_i \\nu(A_i) \\\\\n&= \\int_{\\mathbb{R}^d}f(\\omega)\\overline{g(\\omega)}\\,d\\nu(\\omega).\n\\end{align*}\\]\nA nice thing is that while these piecewise constant functions are quite simple, we can approximate any80 function arbitrarily well by a simple function. This is the same fact we use to build ourselves ordinary81 integrals.\nIn particular, the brave and the bold among you might just say “we can take limits here and define” an integral with respect to the \\(\\nu\\)-noise this way. And, indeed, that works. You get that, for any \\(f\\in L^2(\\nu)\\),\n\\[\n\\mathbb{E}\\left(\\int_{\\mathbb{R}^d} f(\\omega)\\,d Z_\\nu(\\omega)\\right) = 0\n\\] and, for any \\(f,g \\in L^2(\\nu)\\), \\[\n\\mathbb{E}\\left(\\int_{\\mathbb{R}^d} f(\\omega)\\,d Z_\\nu(\\omega)\\overline{\\int_{\\mathbb{R}^d} g(\\omega)\\,d Z_\\nu(\\omega)}\\right) = \\int_{\\mathbb{R}^d} f(\\omega)\\overline{g(\\omega)}\\,d \\nu(\\omega).\n\\]\nIf we define \\[\nu(s) = \\int_{\\mathbb{R}^d}e^{i\\omega^Ts}\\,dZ_\\nu(\\omega),\n\\] then it follows immediately that \\(u(s)\\) is mean zero and has covariance function \\[\n\\mathbb{E}(u(s)\\overline{u(s')}) = \\int_{\\mathbb{R}^d}e^{i\\omega^T(s - s')}\\, d\\nu(\\omega) = c(s-s').\n\\] That is \\(\\nu\\) is the spectral measure associated with the correlation function.\nCombining this with Bochner’s theorem, we have just proved82 the spectral representation theorem for general83 (weakly) stationary84 random fields85.\n\nTheorem 6 (Spectral representation theorem) If \\(\\nu\\) is a finite, non-negative measure on \\(\\mathbb{R}^d\\) and \\(W\\) is a complex \\(\\nu\\)-noise, then the complex-valued process \\[\nu(s) =\\int_{\\mathbb{R}^d}e^{i\\omega^Ts}\\,dZ_\\nu(\\omega)\n\\] has mean zero an covariance \\[\nc(s,s') = \\int_{\\mathbb{R}^d}e^{i\\omega^T(s-s')}\\,d\\nu(\\omega)\n\\] and is therefore weakly stationary. If \\(Z_\\nu(A) \\sim N(0, \\nu(A))\\) then \\(u(s)\\) is a Gaussian process.\nFurthermore, every mean-square continuous mean zero stationary Gaussian process with covariance function \\(c(s,s')= c(s-s')\\) and corresponding spectral measure \\(\\nu\\) has an associated \\(\\nu\\)-noise \\(Z_\\nu(\\cdot)\\) such that \\[\nu(s) =\\int_{\\mathbb{R}^d}e^{i\\omega^Ts}\\,dZ_\\nu(\\omega)\n\\] holds in the mean-square sense for all \\(s \\in \\mathbb{R}^d\\).\n\\(Z_\\nu(\\cdot)\\) is called the spectral process 86 associated with \\(u(\\cdot)\\). When it exists, the density of \\(\\nu\\), denoted by \\(f(\\omega)\\), is called the spectral density or the power spectrum.\n\nAll throughout here I used complex numbers and complex Gaussian processes because, believe it or not, it makes things easier. But you will be pleased to know that \\(u(\\cdot)\\) will be real-valued as long as the spectral density \\(f(\\omega)\\) is symmetric around the origin. And it always is.\n\n\nThe Cameron-Martin87 space of a stationary Gaussian process\nOne particular advantage of stationary processes is that we get a straightforward characterization of the Cameron-Martin space inner product. Recall that the Cameron-Martin space (or reproducing kernel Hilbert space) associated with a Gaussian process is the88 space of all functions of the form \\[\nh(s) = \\sum_{k=1}^K c_k c(s, s_k),\n\\] where \\(K\\) is finite, \\(c_k\\) are real, and \\(s_k\\) are distinct points in \\(\\mathbb{R}^d\\). This is the space that the posterior mean for GP regression lives in.\nThe inner product associated with this space can be written in terms of the spectral density \\(f\\) as89 \\[\n\\langle h, h'\\rangle = \\int_{\\mathbb{R}^d} \\hat h(\\omega) \\overline{\\hat {h'}(\\omega)} \\frac{1}{f(\\omega)}\\,d\\omega.\n\\] In particular, for a Matérn Gaussian process, the corresponding norm is \\[\n\\| h\\|_{H_u} = C_\\text{Matérn}\\kappa^{2\\nu}\\sigma^2 \\int_{\\mathbb{R}^d}|\\hat h(\\omega)|^2 (\\kappa^2 + \\|\\omega\\|^2)^{\\nu+d/2}\\,d\\omega.\n\\] For those of you familiar with function spaces, this is equivalent to the norm on \\(H^{\\nu+d/2}(\\mathbb{R}^d)\\). One way to interpret this is that the set of functions in the Cameron-Martin space for a Matérn GP only depends on \\(\\nu\\), while the norm and inner product (and hence the posterior mean and all that stuff) depend on \\(\\nu\\), \\(\\kappa\\), and \\(\\sigma\\). This observation is going to be important.\n\n\nAnother look at equivalence and singularity\nIt would’ve been a bit of an odd choice to spend all this time talking about spectral representations and never using them. So in this section, I’m going to cover the reason for the season: singularity or absolute continuity of Gaussian measures.\nThe Feldman-Hájek theorem quoted is true on quite general sets of functions. However, if we are willing to restrict ourselves to a separable90 Hilbert91 space there is a much more refined version of the theorem that we can use.\n\nTheorem 7 (Feldman-Hájek theorem (Taylor’s92 version)) Two Gaussian measures \\(\\mu_1\\) (mean \\(m_1\\), covariance operator93 \\(C_1\\)) and \\(\\mu_2\\) (mean \\(m_2\\), covariance operator \\(C_2\\)) on a separable Hilbert space \\(X\\) are absolutely continuous if and only if\n\nThe Cameron-Martin spaces associated with \\(\\mu_1\\) and \\(\\mu_2\\) are the same (considered as sets of functions. They usually will not have the same inner products.),\n\\(m_1 - m_2\\) is in the94 Cameron-Martin space, and\nThe operator \\(T = C_1^{-1/2}C_2C_1^{-1/2} - I\\) is a Hilbert-Schmidt operator, that is it has a countable set of eigenvalues \\(\\delta_k\\) and corresponding eigenfunctions \\(\\phi_k\\) that satisfy \\(\\delta_k &gt; -1\\) and \\[\n\\sum_{k=1}^{\\infty}\\delta_k^2 &lt; \\infty.\n\\]\n\nWhen these three conditions are fulfilled, the Radon-Nikodym derivative is \\[\n\\frac{d\\mu_2}{d\\mu_1} = \\exp\\left(-\\frac{1}{2}\\sum_{k=1}^\\infty \\left(\\frac{\\delta_k}{1 + \\delta_k}\\eta_k^2 - \\log(1+\\delta_k)\\right)\\right],\n\\] where \\(\\eta_k\\) is an sequence of N(0,1) random variables95 96 (under \\(\\mu_1\\)).\nOtherwise, the two measures are singular.\n\nThis version of Feldman-Hájek is considerably more useful than its previous incarnation. The first condition basically says that the posterior means from the two priors will have the same smoothness and is rarely a problem. Typically the second condition is fulfilled in practice (for example, we always set the mean to zero).\nThe third condition is where all of the action is. This is, roughly speaking, a condition that says that \\(C_1\\) and \\(C_2\\) aren’t toooooo different. To understand this, we need to look a little at what the \\(\\delta_k\\) values actually are. It turns out to actually be easier to ask about \\(1+ \\delta_k\\), which are the eigenvalues of \\(C_1^{-1/2}C_2 C_1^{-1/2}\\). In that case, we are trying to find the orthonormal system of functions \\(\\phi_k\\in X\\) such that \\[\\begin{align*}\nC_1^{-1/2}C_2 C_1^{-1/2}\\phi_k &= (1+\\delta_k) \\phi_k \\\\\nC^{-1/2}C_2 \\psi_k &= (1+\\delta_k) C_1^{1/2}\\psi_k \\\\\nC_2\\psi_k &=(1+\\delta_k) C_1\\psi_k,\n\\end{align*}\\] where \\(\\psi_k = C_1^{-1/2}\\phi_k\\).\nHence, we can roughly interpret the \\(\\delta_k\\) as the eigenvalues of \\[\nC_1^{-1}C_2 - I.\n\\] The Hilbert-Schmidt condition is then requiring that \\(C_1^{-1}C_2\\) is not infinitely far from the identity mapping.\nA particularly nice version of this theorem occurs when \\(C_1\\) and \\(C_2\\) have the same eigenvectors. This is a fairly restrictive assumption, but we are going to end up using it later, so it’s worth specialising. In that case, assuming \\(C_j\\) has eigenvalues \\(\\lambda_k^{(j)}\\) and corresponding \\(L^2\\)-orthogonal eigenfunctions \\(\\phi_k(\\cdot)\\), we can write97 \\[\n[C_jh](s) = \\sum_{k=1}^\\infty \\lambda_k^{(j)} \\langle\\phi_k, h\\rangle \\phi_k(s).\n\\] Using the orthogonality of the eigenfunctions, we can show98 that \\[\n[C_j^{\\beta}h](s)=\\sum_{k=1}^\\infty (\\lambda_k^{(j)})^\\beta \\langle\\phi_k, h\\rangle \\phi_k(s).\n\\]\nWith a bit of effort, we can see that \\[\n(C_1^{-1/2}C_2C_1^{-1/2} - I)h = \\sum_{k=1}^\\infty \\frac{\\lambda_k^{(2)} - \\lambda_k^{(1)}}{\\lambda_k^{(1)}} \\langle\\phi_k, h\\rangle \\phi_k\n\\] and so \\[\n\\delta_k = \\frac{\\lambda_k^{(2)} - \\lambda_k^{(1)}}{\\lambda_k^{(1)}}.\n\\] From that, we get99 the KL divergence \\[\\begin{align*}\n\\operatorname{KL}(\\mu_1 || \\mu_2) &= \\mathbb{E}_{\\mu_1}\\log\\left(\\frac{d\\mu_1}{d\\mu_2}\\right) \\\\\n&=-\\frac{1}{2}\\sum_{k=1}^\\infty \\left(\\frac{\\delta_k}{1 + \\delta_k} - \\log(1+\\delta_k)\\right) \\\\\n&= \\frac{1}{2}\\sum_{k=1}^\\infty \\left[\\frac{\\lambda_k^{(1)}}{\\lambda_k^{(2)}} -1+ \\log\\left(\\frac{\\lambda_k^{(1)}}{\\lambda_k^{(2)}}\\right)\\right].\n\\end{align*}\\]\nPossibly unsurprisingly, this is simply the sum of the one dimensional divergences \\[\n\\sum_{k=1}^\\infty\\operatorname{KL}(N(0,\\lambda_k^{(1)}) || N(0,\\lambda_k^{(2)})).\n\\] It’s fun to convince yourself that that \\(\\sum_{k=1}^\\infty \\delta_k^2 &lt; \\infty\\) is sufficient to ensure the sum converges.\n\n\nA convenient suffient condition for absolute continuity, which turns out to be necessary for Matérn fields\nOk. So I lied. I suggested that we’d use all of that spectral stuff in the last section. And we didn’t! Because I’m dastardly. But this time I promise we will!\nIt turns out that even with our fancy version of Feldman-Hájek, it can be difficult100 to work out whether two Gaussian processes are singular or equivalent. One of the big challenges is that the eigenvalues and eigenfunctions depend on the domain \\(D\\) and so we would, in principle, have to check this quite complex condition for every single domain.\nThankfully, there is an easy to parse sufficient condition that we can use that show when two GPs are equivalent on every bounded domain. These conditions are stated in terms of the spectral densities.\n\nTheorem 8 (Sufficent condition for equivalence (Thm 4 of Skorokhod and Yadrenko)) Let \\(u_1(\\cdot)\\) and \\(u_2(\\cdot)\\) be mean-zero Gaussian processes with spectral densities \\(f_j(\\omega)\\), \\(j=1,2\\). Assume that \\(f_1(\\omega)\\|\\omega\\|^\\alpha\\) is bounded away from zero and infinity for some101 \\(\\alpha&gt;0\\) and \\[\n\\int_{\\mathbb{R}^d}\\left(\\frac{f_2(\\omega) - f_1(\\omega)}{f_1(\\omega)}\\right)^2\\,d\\omega &lt; \\infty.\n\\] Then the joint distributions of \\(\\{u_1(s): s \\in D\\}\\) and \\(\\{u_2(s): s \\in D\\}\\) are equivalent measures for every bounded region \\(D\\).\n\nThe proof of this is pretty nifty. Essentially it constructs the operator \\(T+I\\) in a sneaky102 way and then bounds its trace on rectangle containing \\(D\\). That upper bound is finite precisely when the above integral is finite.\nNow that we have a relatively simple condition for equivalence, let’s look at Matérn fields. In particular, we will assume \\(u_j(\\cdot)\\), \\(j=1,2\\) are two Matérn GPs with the same smoothness parameter \\(\\nu\\) and other parameters103 \\((\\kappa_j, \\sigma_j)\\). \\[\n\\int_{\\mathbb{R}^d}\\left(\\frac{f_2(\\omega) - f_1(\\omega)}{f_1(\\omega)}\\right)^2\\,d\\omega  = \\int_{\\mathbb{R}^d}\\left(\\frac{\\kappa_2^{2\\nu}\\sigma_2^2(\\kappa_2^2 + \\|\\omega\\|^2)^{-\\nu - d/2} }{\\kappa_1^{2\\nu}\\sigma_1^2(\\kappa_1^2 + \\|\\omega\\|^2)^{-\\nu - d/2}}-1\\right)^2\\,d\\omega.\n\\] We can save ourselves some trouble by considering two cases separately.\nCase 1: \\(\\kappa_1^{2\\nu}\\sigma_1^2 = \\kappa_2^{2\\nu}\\sigma_2^2\\).\nIn this case, we can make the change to spherical coordinates via the substitution \\(r = \\|\\omega\\|\\) and, again to save my poor fingers, let’s set \\(\\alpha = \\nu + d/2\\). The condition becomes \\[\n\\int_0^\\infty\\left[\\left(\\frac{\\kappa_1^2 + r^2 }{\\kappa_2^2 + r^2}\\right)^{\\alpha}-1\\right]^2r^{d-1}\\,dr &lt; \\infty.\n\\] To check that this integral is finite, first note that, near \\(r=0\\), the integrand is104 \\(\\mathcal{O}({r^{d-1}})\\), so there is no problem there. Near \\(r = \\infty\\) (aka the other place bad stuff can happen), the integrand is \\[\n2\\alpha(\\kappa_1^2 - \\kappa_2^2)^2 r^{d-5} + \\mathcal{O}(r^{d-7}).\n\\] This is integrable for large \\(r\\) whenever105 \\(d \\leq 3\\). Hence, the two fields are equivalent whenever \\(d\\leq 3\\) and \\(\\kappa_1^{2\\nu}\\sigma_1^2 = \\kappa_2^{2\\nu}\\sigma_2^2\\). It is harder, but possible to show that the fields are singular when \\(d&gt;4\\). The case with \\(d=4\\) is boring and nobody cares.\nCase 2:  \\(\\kappa_1^{2\\nu}\\sigma_1^2 \\neq \\kappa_2^{2\\nu}\\sigma_2^2\\).\nLet’s define \\(\\sigma_3 = \\sigma_2(\\kappa_2/\\kappa_1)^\\nu\\). Then it’s clear that \\(\\kappa_1^{2\\nu}\\sigma_3^2 = \\kappa_2^{2\\nu}\\sigma_2^2\\) and therefore the Matérn field \\(u_3\\) with parameters \\((\\kappa_1, \\sigma_3, \\nu)\\) is equivalent to \\(u_2(\\cdot)\\).\nWe will now show that \\(u_1\\) and \\(u_3\\) are singular, which implies that \\(u_1\\) and \\(u_2\\) are singular. To do this, we just need to note that, as \\(u_1\\) and \\(u_3\\) have the same value of \\(\\kappa\\), \\[\nu_3(s) = \\frac{\\sigma_3}{\\sigma_1}u_1(s).\n\\] We know, from the previous blog post, that \\(u_3\\) and \\(u_1\\) will be singular unless \\(\\sigma_1 = \\sigma_3\\), but this only happens when \\(\\kappa_1^{2\\nu}\\sigma_1^2 = \\kappa_2^{2\\nu}\\sigma_2^2\\), which is not true by assumption.\nHence we have proved the first part of the following Theorem due, in this form, to Zhang106 (2004) and Anderes107 (2010).\n\nTheorem 9 (Thm 2 of Zhang (2004)) Two Gaussian process on \\(\\mathbb{R}^d\\), \\(d\\leq 3\\), with Matérn covariance functions with parameters \\((\\ell_j, \\sigma_j, \\nu)\\), \\(j=1,2\\) induce equivalent Gaussian measures if and only if \\[\n\\frac{\\sigma_1^2}{\\ell_1^{2\\nu}} = \\frac{\\sigma_2^2}{\\ell_2^{2\\nu}}.\n\\] When \\(d &gt; 4\\), the measures are always singular (Anderes, 2010)."
  },
  {
    "objectID": "posts/2022-09-07-priors5/priors5.html#part-3-deriving-the-pc-prior",
    "href": "posts/2022-09-07-priors5/priors5.html#part-3-deriving-the-pc-prior",
    "title": "Priors for the parameters in a Gaussian process",
    "section": "Part 3: Deriving the PC prior",
    "text": "Part 3: Deriving the PC prior\nWith all of that in hand, we are finally (finally!) in a position to show that, in 3 or fewer dimensions, the PC prior distance is \\(d(\\kappa) = \\kappa^{d/2}\\). After this, we can put everything together! Hooray!\n\nApproximating the Kullback-Leibler divergence for a Matérn random field\nNow, you can find a proof of this in the appendix of our JASA paper, but to be honest it’s quite informal. But although you can sneak any old shite into JASA, this is a blog goddammit and a blog has integrity. So let’s do a significantly more rigorous proof of our argument.\nTo do this, we will need to find the KL divergence between \\(u_1\\), with parameters \\((\\kappa, \\tau \\kappa_1^{-\\nu}, \\nu)\\) and a base model \\(u_0\\) with parameters \\((\\kappa_0, \\tau \\kappa_0^{-\\nu}, \\nu)\\), where \\(\\kappa_0\\) is some fixed, small number and \\(\\tau &gt;0\\) is fixed. We will actually be interested in the behaviour of the KL divergence as \\(\\kappa_0\\) goes to zero. Why? Because \\(\\kappa_0 = 0\\) is our base model.\nThe specific choice of standard deviation in both models ensures that \\(\\kappa^{2\\nu}\\sigma^2 = \\kappa_0^{2\\nu}\\sigma_0^2\\) and so the KL divergence is finite.\nIn order to approximate the KL divergence, we are going to find a basis that simultaneously diagonalises both processes. In the paper, we simply declared that we could do this. And, morally, we can. But as I said a blog aims to a higher standard than mere morality. Here we strive for meaningless rigour.\nTo that end, we are going to spend a moment thinking about how this can be done in a way that isn’t intrinsically tied to a given domain \\(D\\). There may well be a lot of different ways to do this, but the most obvious one is to notice that if \\(u(\\cdot)\\) is periodic on the cube \\([-L,L]^d\\) for some \\(L \\gg 0\\), then it can be considered as a GP on a \\(d\\)-dimensional torus. If \\(L\\) is large enough that \\(D \\subset [-L,L]^d\\), then we might be able to focus on our cube and forget all about the specific domain \\(D\\).\nA nice thing about periodic GPs is that we actually know their Karhunen-Loève108 representation. In particular, if \\(c_p(\\cdot)\\) is a stationary covariance function on a torus, then we109 know that it’s eigenfunctions are \\[\n\\phi_k(s) = e^{-\\frac{2\\pi i}{L} k^Th}, \\quad k \\in \\mathbb{Z}^d\n\\] and its eigenvalues are \\[\n\\lambda_k = \\int_{\\mathbb{T}^d} e^{-\\frac{2\\pi i}{L} k^Th} c_p(h)\\,dh.\n\\] This gives110 \\[\nc_p(h) = \\left(\\frac{2\\pi}{L}\\right)^d \\sum_{k \\in \\mathbb{Z}^d}\\lambda_k  e^{-\\frac{2\\pi i}{L} k^Th}.\n\\]\nNow we have some work to do. Firstly, our process is not periodic111 on \\(\\mathbb{R}^d\\). That’s a bit of a barrier. Secondly, even if it were, we don’t actually know what \\(\\lambda_k\\) is going to be. This is probably112 an issue.\nSo let’s make this sucker periodic. The trick is to note that, at long enough distances, \\(u(s)\\) and \\(u(s')\\) are almost uncorrelated. In particular, if \\(\\|s - s'\\| \\gg \\ell\\), then \\(\\operatorname{Cov}(u(s), u(s')) \\approx 0\\). This means that if we are interested in \\(u(\\cdot)\\) on a fixed domain \\(D\\), then we can replace it with \\(u_p(s)\\) that is a GP where the covariance function \\(c_p(\\cdot)\\) is the periodic extension of \\(c(h)\\) from \\([-L,L]^d\\) to \\(\\mathbb{R}^d\\) (aka we just repeat it!).\nThis repetition won’t be noticed on \\(D\\) as long as \\(L\\) is big enough. But we can run into the small113 problem. This procedure can lead to a covariance function \\(c_p(\\cdot)\\) that is not positive definite. Big problem. Huge.\nIt turns out that one way to fix this is is to use a smooth cutoff function \\(\\delta(h)\\) that is 1 on \\([-L,L]^d\\) and 0 outside of \\([-\\gamma,\\gamma]^d\\), where \\(L&gt;0\\) is big enough so that \\(D \\subset [-L, L]^d\\) and \\(\\gamma &gt; L\\). We can then build the periodic extension of a stationary covariance function \\(c(\\cdot)\\) as \\[\nc_p(h) = \\sum_{k \\in \\mathbb{Z}^d}c(x + 2Lk)\\delta(x + 2 Lk).\n\\] It’s important114 to note that this is not the same thing as simply repeating the covariance function in a periodic manner. Near the boundaries (but outside of the domain) there will be some reach-around contamination. Bachmayr, Cohen, and Migliorati show that this does not work for general stationary covariance functions, but does work under the additional condition that \\(\\gamma\\) is big enough and there exist some \\(s \\geq r &gt; d/2\\) and \\(0 &lt; \\underline{C} \\leq \\overline{C} &lt; \\infty\\) such that \\[\n\\underline{C}(1 + \\|\\omega\\|^2)^{-s} \\leq f(\\omega)\\leq \\overline{C}(1 + \\|\\omega\\|^2)^{-r}.\n\\] This condition obviously holds for the Matérn covariance function and Bachmayr, Graham, Nguyen, and Scheichl115 showed that \\(\\gamma &gt; A(d, \\nu)\\ell\\) for some explicit function \\(A\\) that only depends on \\(d\\) and \\(\\nu\\) is sufficient to make this work.\nThe nice thing about this procedure is that \\(c_p(s-s') = c(s-s')\\) as long as \\(s, s' \\in D\\), which means that our inference is going to be identical on our sample as it would be with the non-periodic covariance function! Splendid!\nNow that we have made a valid periodic extension (and hence we know what the eigenfunctions are), we need to work out what the corresponding eigenvalues are.\nWe know that \\[\n\\int_{\\mathbb{R}^d} e^{-\\frac{\\pi i}{L}k^Th}c(h)\\,dh = f\\left(\\frac{\\pi}{L}k\\right).\n\\] But it is not clear what will happen when we take the Fourier transform of \\(c_p(\\cdot)\\).\nThankfully, the convolution theorem is here to help us and we know that, if \\(\\theta(s) = 1 - \\delta(s)\\), then \\[\n\\int_{\\mathbb{R}^d} e^{-\\frac{\\pi i}{L}k^Th}(c(h) - c_p(h))\\,dh = (\\hat{\\theta}*f)\\left(\\frac{\\pi}{L}k\\right),\n\\] where \\(*\\) is the convolution operator.\nIn the perfect world, \\((\\hat{\\theta}*f)(\\omega)\\) would be very close to zero, so we can just replace the Fourier transform of \\(c_p\\) with the Fourier transform of \\(c\\). And thank god we live in a perfect world.\nThe specifics here are a bit tedious116, but you can show that \\((\\hat{\\theta}*f)(\\omega) \\rightarrow 0\\) as \\(\\gamma \\rightarrow \\infty\\). For Matérn fields, Bachmayr etc performed some heroic calculations to show that the difference is exponentially small as \\(\\gamma \\rightarrow \\infty\\) and that, as long as \\(\\gamma &gt; A(\\nu) \\ell\\), everything is positive definite and lovely.\nSo after a bunch of effort and a bit of a literature dive, we have finally got a simultaneous eigenbasis and we can write our KL divergence as \\[\\begin{align*}\n\\operatorname{KL}(u_1 || u_0) &= \\frac{1}{2} \\sum_{\\omega \\in \\frac{2\\pi}{L}\\mathbb{Z}}\\left[\\frac{f_1(\\omega)}{f_0(\\omega)} - 1 - \\log \\left(\\frac{f_1(\\omega)}{f_0(\\omega)}\\right)\\right] \\\\\n&= \\frac{1}{2} \\sum_{\\omega \\in \\frac{2\\pi}{L}\\mathbb{Z}}\\left[\\frac{(\\kappa_0^2 + \\|\\omega\\|^2)^\\alpha}{(\\kappa^2 + \\|\\omega\\|^2)^\\alpha} - 1 - \\log \\left(\\frac{(\\kappa_0^2 + \\|\\omega\\|^2)^\\alpha}{(\\kappa^2 + \\|\\omega\\|^2)^\\alpha} \\right)\\right].\n\\end{align*}\\] We can write this as \\[\n\\operatorname{KL}(u_1 || u_0) =\\frac{1}{2} \\left(\\frac{L \\kappa}{2\\pi}\\right)^d \\sum_{\\omega \\in \\frac{2\\pi}{L}\\mathbb{Z}}\\left(\\left[\\frac{(\\kappa_0^2 + \\|\\omega\\|^2)^\\alpha}{(\\kappa^2 + \\|\\omega\\|^2)^\\alpha} - 1 - \\log \\left(\\frac{(\\kappa_0^2 + \\|\\omega\\|^2)^\\alpha}{(\\kappa^2 + \\|\\omega\\|^2)^\\alpha} \\right)+\\mathcal{O}(e^{-C\\gamma})\\right]\\left(\\frac{2\\pi}{L \\kappa}\\right)^d\\right) ,\n\\] for some constant \\(C\\) that you can actually work out but I really don’t need to. The important thing is that the error is exponentially small in \\(\\gamma\\), which is very large and spiraling rapidly out towards infinity.\nThen, noticing that the sum is just a trapezium rule approximation to a \\(d\\)-dimensional integral, we get, as \\(\\kappa_0 \\rightarrow 0\\) (and hence \\(L, \\gamma\\rightarrow \\infty\\)), \\[\n\\operatorname{KL}(u_1 || u_0) = \\frac{1}{2} \\left(\\frac{L \\kappa}{2\\pi}\\right)^d \\int_{\\mathbb{R}^d}\\left[\\frac{((\\kappa_0/\\kappa)^2 + \\|\\omega\\|^2)^\\alpha}{(1 + \\|\\omega\\|^2)^\\alpha} - 1 - \\log \\left(\\frac{((\\kappa_0/\\kappa)^2 + \\|\\omega\\|^2)^\\alpha}{(1 + \\|\\omega\\|^2)^\\alpha} \\right)\\right] + \\mathcal{O}(1).\n\\] The integral converges whenever \\(d \\leq 3\\).\nThis suggests that we can re-scale the distance by absorbing the \\((L/(2\\pi^d))\\) into the constant in the PC prior, and get \\[\nd(\\kappa) = \\kappa^{d/2}.\n\\]\nThis distance does not depend on the specific domain \\(D\\) (or the observation locations), which is an improvement over the PC prior I derived in the introduction. Instead, it only assumes that \\(D\\) is bounded, which isn’t really a big restriction in practice.\n\n\nThe PC prior for \\((\\sigma, \\ell)\\)\nWith all of this in hand, we can now construct the PC prior. Instead of working directly with \\((\\sigma, \\ell)\\), we will instead derive the prior for the estimable parameter \\(\\tau = \\kappa^\\nu \\sigma\\), and the non-estimable parameter \\(\\kappa\\).\nWe know that \\(\\tau^2\\) multiplies the covariance function of \\(u(\\cdot)\\), so it makes sense to treat \\(\\tau\\) like a standard deviation parameter. In this case, the PC prior is \\[\np(\\tau \\mid \\kappa) = \\lambda_\\tau(\\kappa)e^{-\\lambda_\\tau(\\kappa) \\tau}.\n\\] The canny among you would have noticed that I have made the scaling parameter \\(\\tau\\) depend on \\(\\kappa\\). I have done this because the quantity of interest that we want our prior to control is the marginal standard deviation \\(\\sigma = \\kappa^\\nu \\tau\\), which is a function of \\(\\kappa\\). If we want to ensure \\(\\Pr(\\sigma &lt; U_\\sigma) = \\alpha_\\sigma\\), we need \\[\n\\lambda_\\tau(\\kappa) = -\\kappa^\\nu\\frac{\\log \\alpha_\\sigma}{U_\\sigma}.\n\\]\nWe can now derive the PC prior for \\(\\kappa\\). The distance that we just spent all that effort calculating, and an exponential prior on \\(\\kappa^{d/2}\\) leads117 to the prior \\[\np(\\kappa) = \\frac{d}{2}\\lambda_\\ell \\kappa^{d/2-1}e^{-\\lambda_\\ell \\kappa^{d/2}}.\n\\] Note that in this case, \\(\\lambda_\\ell\\) does not depend on any other parameters: this is because \\(\\ell = \\sqrt{8\\nu}\\kappa^{-1}\\) is our identifiable parameter. If we require \\(\\Pr(\\ell &lt; L_\\ell) = \\alpha_\\ell\\), we get \\[\n\\lambda_\\ell = -\\left(\\frac{L_\\ell}{\\sqrt{8\\nu}}\\right)^{d/2} \\log \\alpha_\\ell.\n\\]\nHence the joint PC prior on \\((\\kappa, \\tau)\\), which is emphatically not the product of two independent priors, is \\[\np(\\kappa, \\tau) = \\frac{d}{2U_\\sigma}\\log (\\alpha_\\ell)\\log(\\alpha_\\sigma)\\left(\\frac{L_\\ell}{\\sqrt{8\\nu}}\\right)^{d/2} \\kappa^{\\nu + d/2-1}\\exp\\left[-\\left(\\frac{L_\\ell}{\\sqrt{8\\nu}}\\right)^{d/2}| \\log (\\alpha_\\ell)| \\kappa^{d/2} -\\frac{|\\log \\alpha_\\sigma|}{U_\\sigma} \\tau\\kappa^\\nu\\right].\n\\]\nGreat gowns, beautiful gowns.\nOf course, we don’t want the prior on some weird parameterisation (even though we needed that parameterisation to derive it). We want it on the original parameterisation. And here is where some magic happens! When we transform this prior to \\((\\ell, \\sigma)\\)-space it magically118 becomes the product of two independent priors! In particular, the PC prior that encodes \\(\\Pr(\\ell &lt; L_\\ell) = \\alpha_\\ell\\) and \\(\\Pr(\\sigma &gt; U_\\sigma) = \\alpha_\\sigma\\) is \\[\np(\\ell, \\sigma) = \\left[\\frac{d}{2}|\\log(\\alpha_\\ell)|L_\\ell^{d/2} \\ell^{-d/2-1}\\exp\\left(-|\\log(\\alpha_\\ell)|L_\\ell^{d/2} \\ell^{-d/2}\\right)\\right] \\times \\left[\\frac{|\\log(\\alpha_\\sigma)|}{U_\\sigma}\\exp\\left(-\\frac{|\\log(\\alpha_\\sigma)|}{U_\\sigma}\\sigma\\right)\\right].\n\\]\nIt. Is. Finished."
  },
  {
    "objectID": "posts/2022-09-07-priors5/priors5.html#footnotes",
    "href": "posts/2022-09-07-priors5/priors5.html#footnotes",
    "title": "Priors for the parameters in a Gaussian process",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe most common feedback was “I hung in for as long as I could”.↩︎\nIf you don’t think we’re gonna get our Maccabees on you’re dreamin’. Hell, I might have to post Enoch-ussy on main.↩︎\nPenalised Complexity priors (or PC priors) are my favourite thing. If you’re unfamilliar with them, I strongly recommend you read the previous post on PC priors to get a good grip on what they are, but essentially they’re a way to construct principled, weakly informative prior distributions. The key tool for PC priors is the Kullback-Leibler divergence between a model with parameter \\(\\theta\\) and a fixed base model with parameter \\(\\theta_0\\). Computing the KL divergence between two GPs is, as we will see, a challenge.↩︎\nFun fact: when we were starting to work on PC priors we were calling them PCP priors, but then I remembered that one episode of CSI where some cheerleaders took PCP and ate their friend and we all agreed that that wasn’t the vibe we were going for.↩︎\nyou might just need to trust me at some points↩︎\nIt could be easily more complex with multilevel component, multiple GPs, time series components etc etc. But the simplest example is a GP regression.↩︎\nThe GP has mean zero for the same reason we usually centre our covariates: it lets the intercept model the overall mean.↩︎\nNot just the likelihood but also everything else in the model↩︎\nA challenge with reference priors is that they are often improper (aka they don’t integrate to 1). This causes some conceptual difficulties, but there is a whole theory of Bayes that’s mostly fine with this as long as the resulting posterior integrates to one. But this is by no means guaranteed and is typically only checked in very specific cases. Jim Berger, one of the bigger proponents of reference prior, used to bring his wife to conference poster sessions. When she got bored, she would simply find a grad student and ask them if they’d checked if the posterior was proper. Sometimes you need to make your own fun.↩︎\nHope has no place in statistics.↩︎\nRemember that any number on the logit scale outside of \\([-3,3]\\) might as well be the same number↩︎\nlog(.Machine$integer.max) = 21.48756↩︎\n\\(e^5 \\approx 148\\), so 70% of the prior mass is less than that. 90% of the prior mass is less than \\(e^{10} \\approx 22026\\) and 99% is less than \\(10^{13}\\). This is still a weak prior.↩︎\nConceptually. The mathematics of what happens as \\(\\ell \\rightarrow 0\\) aren’t really worth focusing on.↩︎\nOr, you know, linear functionals↩︎\nYou can find Bayesians who say that they don’t care if cross validation works or not. You can find Bayesians who will say just about anything.↩︎\nThere are lots of parameterisations, but they’re all easy to move between. Compared to wikipedia, we use the \\(\\sqrt{8}\\) scaling rather than the \\(\\sqrt{2}\\) scaling.↩︎\nEverything in this post can be easily generalised to having different length scales on each dimension.↩︎\nIf you’ve not run into these before, \\(x^{\\nu}K_\\nu(x)\\) is finite at zero and decreases monotonically in an exponential-ish fashion as \\(x\\rightarrow \\infty\\).↩︎\nPossibly trying several values and either selecting the best or stacking all of the models↩︎\nField because by rights GPs with multidimensional parameter spaces should be called Gaussian Fields but we can’t have nice things so whatever. Live your lives.↩︎\nAt which point you need to ask yourself if one goes their faster. It’s chaos.↩︎\nAsymptotics as copaganda.↩︎\nI mean, if you can repeat experiments that’s obviously amazing, but there are lots of situations where that is either not possible or not the greatest use of resources. There’s an interesting sub-field of statistical earth sciences that focuses on working out the value of getting new types of observations in spatial data. This particular variant of the value of information problem throws up some fun corners.↩︎\nor hoping↩︎\nin 3 or fewer dimensions↩︎\nI have not fact checked this↩︎\nBasically everything you care about. Feel free to google the technical definition. But any space with a metric is locally convex. Lots of things that aren’t metric spaces are too.↩︎\nmeasurable↩︎\nThis will seem a bit weird if it’s the first time you’ve seen the concept. In finite dimensions (aka most of statistics) every Gaussian is equivalent to every other Gaussian. In fact, it’s equivalent to every other continuous distribution with non-zero density on the whole of \\(\\mathbb{R}^d\\). But shit gets weird when you’re dealing with functions and we just need to take a hit of the video head cleaner and breathe until we get used to it.↩︎\nThese measures are not the same. They just happen to be non-zero on the same sets.↩︎\nThis was proven in the monster GP blog post.↩︎\neg, computationally where Metropolis-Hastings acceptance probabilities have an annoying tendency to go to zero unless you are extraordinarily careful.↩︎\nif it exists↩︎\nThis can be interpreted as the event that \\(|\\hat\\theta_n - \\theta_0| &gt; \\epsilon\\) infinity many times for every epsilon. If this event occurs with any probability, it would strongly suggest that the estimator is not bloody converging.↩︎\nor even many↩︎\nTechnically, a recent paper in JRSSSB said that if you add an iid Gaussian process you will get identifiability, but that’s maybe not the most realistic asymptotic approximation.↩︎\nThe fourth dimension is where mathematicians go to die↩︎\nIt’s computationally pretty expensive to plot the whole likelihood surface, so I’m just doing it along lines↩︎\npartial freezes a few parameter values, and possibly replaces any calls that return an error with an NA↩︎\nThat I could find↩︎\nTo be fair to van der Vaart and van Zanten their particular problem doesn’t necessarily have a ridge!↩︎\nSaddle up for some spectral theory.↩︎\nI’m terribly sorry.↩︎\nI’m moderately sure that the preprint is pretty similar to the published version but I am not going to check.↩︎\nCan’t stress enough that this is smoothness in a qualitative sense rather than in the more technical “how differentiable is it?” sense.↩︎\nTruly going wild with the scare quotes. Always a sign of excellent writing.↩︎\nFor the usual smoothing spline with the square of the Laplacian, you need \\(\\nu = 2 - d/2\\). Other values of \\(\\nu\\) still give you splines, just with different differentiability assumptions.↩︎\nIf your data is uniformly spaced, you can use the minimum. Otherwise, I suggest a low quantile of the distribution of distances. Or just a bit of nous.↩︎\nThe second half of this post is devoted to proving this. And it is long.↩︎\nWith this parameterisation it’s sometimes known as a Type-II Gumbel distribution. Because why not.↩︎\nAnd only in this case! The reference prior changes a lot when there is a non-zero mean, when there are other covariates, when there is observation noise, etc etc. It really is quite a wobbly construction.↩︎\nReaders, I have not bothered to show.↩︎\nPart of why I’m reluctant to claim this is a good idea in particularly high dimensions is that volume in high dimensions is frankly a bit gross.↩︎\nI, for one, love a sneaky transformation to spherical coordinates.↩︎\nSo why do all the technical shit to derive the PC prior when this option is just sitting there? Fuck you, that’s why.↩︎\nThis is sometimes called “automatic relevance determination” because words don’t have meaning anymore. Regardless, it’s a pretty sensible idea when you have a lot of covariates that can be quite different.↩︎\nIt is possible that a horseshoe-type prior on \\(\\log(\\ell_j)\\) would serve better, but there are going to be some issues as that will shrink the geometric mean of the length scales towards 1.↩︎\nPart of the motivation for writing this was to actually have enough of the GP theory needed to think about these priors in a single place.↩︎\nIn fact, it’s isotropic, which is a stricter condition on most spaces. But there’s no real reason to specialise to isotropic processes so we simply won’t.↩︎\nWe are assuming that the mean is zero, but absent that assumption, we need to assume that the mean is constant.↩︎\nFor non-Gaussian processes, this property is known as second-order stationarity. For GPs this corresponds to strong stationary, which is a property of the distribution rather than the covariance function ↩︎\nIf you’ve been exposed to the concept of ergodicity of random fields you may be eligible for compensation.↩︎\nPossibly with different length scales in different directions or some other form of anisotropy↩︎\nThis is normalisation is to make my life easier.↩︎\nLet’s not lie, I just jumped straight to complex numbers. Some of you are having flashbacks.↩︎\nFourier-Stieljes↩︎\ncountably additive set-valued function. Like a probability but it doesn’t have to total to one↩︎\nand complexify↩︎\nor a Cameron-Martin space↩︎\nThat is, this measure bullshit isn’t just me pretending to be smart. It’s necessary.↩︎\nFeeling annoyed by a reparameterisation this late in the blog post? Well tough. I’ve got to type this shit out and if I had to track all of those \\(\\sqrt{8\\nu}\\)s I would simply curl up and die.↩︎\nIn my whole damn life I have never successfully got the constant correct, so maybe check that yourself. But truly it does not matter. All that matters for the purposes of this post is the density as a function of \\((\\omega, \\sigma,\\kappa)\\).↩︎\nThis is not restricted to being Gaussian, but for all intents and porpoises it is.↩︎\nCountably additive set-valued function taking any value in \\(\\mathbb{C}\\)↩︎\n\\(\\nu\\)-measurable↩︎\n\\(A \\cap B = \\emptyset\\)↩︎\nIf \\(Z_\\nu(A)\\) is also Gaussian then this is the same as them being independent↩︎\nThis is the technical term for this type of function because mathematicians weren’t hugged enough as children.↩︎\nfor a particular value of “any”↩︎\nfor a particular value of “ordinary”↩︎\nWell enough for a statistician anyway. You can look it up the details but if you desperately need to formalise it, you build an isomorphism between \\(\\operatorname{span}\\{u(s), s \\in \\mathbb{R}^d\\}\\) and \\(\\operatorname{span}\\{e^{i\\omega^Ts}, s \\in \\mathbb{R}^d\\}\\) and use that to construct \\(W\\). It’s not wildly difficult but it’s also not actually interesting except for mathturbatory reasons.↩︎\nNon-Gaussian!↩︎\nOn more spaces, the same construction still works. Just use whatever Fourier transform you have available.↩︎\nor stochastic processes↩︎\nYes, it’s a stochastic process over some \\(\\sigma\\)-algebra of sets in my definition. Sometimes people use \\[\n\\tilde{Z}_\\nu(s) = Z_\\nu((-\\infty, s_1]\\times\\cdots \\times (-\\infty, s_d])\n\\] as the spectral process and interpret the integrals as Lebesgue-Stieltjes integrals. All power to them! So cute! It makes literally no difference and truly I do not think it makes anything easier. By the time you’re like “you know what, I reckon Stieltjes integrals are the way to go” you’ve left “easier” a few miles back. You’ve still got to come up with an appropriate concept of an integral.↩︎\nAlso known as the Reproducing Kernel Hilbert Space even though it doesn’t actually have to be one. This is the space of all means. See the previous GP blog.↩︎\nclosure of the↩︎\nIn the previous post, I wrote this in terms of the inverse of the covariance operator. For a stationary operator, the covariance operator is (by the convolution theorem) \\[\nCh(s) = \\int_{\\mathbb{R}}e^{i\\omega s}\\hat{h}(\\omega) f(\\omega)\\,d\\omega\n\\] and it should be pretty easy to convince yourself that \\[\nC^{-1}h(s) = \\int_{\\mathbb{R}}e^{i\\omega s}\\hat{h}(\\omega) \\frac{1}{f(\\omega)}\\,d\\omega.\n\\]↩︎\nie one where we can represent functions using a Fourier series rather than a Fourier transform↩︎\nie one with an inner product↩︎\nBogachev’s Gaussian Measures book, Corollary 6.4.11 with some interpretation work to make it slightly more human-readable. I also added the minus sign he missed in the density.↩︎\nRecall that this is the integral operator \\(C_1 f = \\int_D c_1(x,x')f(x')\\,d x'\\).↩︎\nBecause of condition 1 if it’s in one of them it’s in the other too!↩︎\nTechnically, they are an orthonormal basis in the closure of \\(\\{\\ell -\\mu(\\ell) : \\ell \\in X^* \\}\\) under the \\(R_{u_1}\\) norm, but let’s just be friendly to ourselves and pretend \\(u_j\\) have zero mean so these spaces are the same. The theorem is very explicit about what they are. If \\(\\phi_k\\) are the (\\(X\\)-orthonormal) eigenfunctions corresponding to \\(\\delta_k\\), then \\[\n\\eta_k = \\int_{\\mathbb{R}^d} C_1^{1/2}\\phi_k(s)\\,dW_1(s),\n\\] where \\(W_1(s)\\) is the spectral process associated with \\(u_1\\). Give or take, this the same thing I said in the main text.↩︎\nAfter reading all of that, let me tell you that it simply does not matter even a little bit.↩︎\nYes - this is Mercer’s theorem again. The only difference is that we are assuming that the eigenfunctions are the same for each \\(j\\) so they don’t need an index.↩︎\n\\[\\begin{align*}\nC_j^\\beta[C_j^{-\\beta}h] &= \\sum_{m=1}^\\infty (\\lambda_m^{(j)})^\\beta \\left\\langle\\phi_m, \\sum_{k=1}^\\infty (\\lambda_k^{(j)})^{-\\beta} \\langle\\phi_k, h\\rangle \\phi_k\\right\\rangle \\phi_m \\\\\n&= \\sum_{m=1}^\\infty (\\lambda_m^{(j)})^\\beta\\sum_{k=1}^\\infty (\\lambda_k^{(j)})^{-\\beta} \\langle\\phi_k, h\\rangle \\left\\langle\\phi_m,   \\phi_k\\right\\rangle \\phi_m \\\\\n&=\\sum_{m=1}^\\infty (\\lambda_m^{(j)})^\\beta (\\lambda_m^{(j)})^{-\\beta} \\langle\\phi_m, h\\rangle \\phi_m \\\\\n&= h\n\\end{align*}\\]↩︎\nYou simply cannot make me care enough to prove that we can swap summation and expectation. Of course we bloody can. Also \\(\\mathbb{E}_{\\mu_1} \\eta_k^2 = 1\\).↩︎\nBut not impossible. Kristin Kirchner and David Bolin have done some very nice work on this recently.↩︎\nThis is a stronger condition than the one in the paper, but it’s a) readily verifiable and b) domain independent.↩︎\nThis is legitimately quite hard to parse. You’ve got to back-transform their orthogonal basis \\(g_k\\) to an orthogonal basis on \\(L^2(D)\\), which is where those inverse square roots come from!↩︎\nRemember \\(\\kappa = \\sqrt{8\\nu}\\ell^{-1}\\) because Daddy hates typing.↩︎\nThrough the magical power of WolframAlpha or, you know, my own ability to do simple Taylor expansions.↩︎\n\\(d-5&lt;-1\\)↩︎\n\\(d\\leq 3\\)↩︎\n\\(d&gt;4\\)↩︎\nThe other KL. The spicy, secret KL. KL after dark. What Loève but a second-hand Karhunen?↩︎\nThis is particularly bold use of the inclusive voice here. You may or may not know. Nevertheless it is true.↩︎\nSpecifically, this kinda funky set of normalisation choices that statisticians love to make gives↩︎\nIf you think a bit about it, a periodic function on \\(\\mathbb{R}^d\\) can be thought of as a process on a torus by joining the approrpriate edges together!↩︎\nWe will see that this is not an issue, but you better bloody believe that our JASA paper just breezed the fuck past these considerations. Proof by citations that didn’t actually say what we needed them to say but were close enough for government work. Again, this is one of those situations where the thing we are doing is obviously valid, but the specifics (which are unimportant for our situation because we are going to send \\(\\kappa_0\\rightarrow 0\\) and \\(L \\rightarrow \\infty\\) in a way that’s much faster than \\(\\kappa_0^{-1}\\)) are tedious and, I cannot stress this enough, completely unimportant in this context. But it’s a fucking blog and a blog has a type of fucking integrity that the Journal of the American Fucking Statistical Association does not even almost claim to have. I’ve had some red wine.↩︎\nbig↩︎\nI cannot stress enough that we’re not bloody implementing this scheme, so it’s not even slightly important. Scan on, McDuff.↩︎\nFun fact. I worked in the same department as authors 2 and 4 for a while and they are both very lovely.↩︎\nCheck out either of the Bachmayr et al. papers if you’re interested.↩︎\nThanks Mr Jacobian!↩︎\nI feel like I’ve typed enough, if you want to see the Jacobian read the appendices of the paper.↩︎"
  },
  {
    "objectID": "posts/2022-06-03-that-psis-proof/that-psis-proof.html",
    "href": "posts/2022-06-03-that-psis-proof/that-psis-proof.html",
    "title": "Tail stabilization of importance sampling etimators: A bit of theory",
    "section": "",
    "text": "Imagine you have a target probability distribution \\(p(\\theta)\\) and you want to estimate the expectation \\(I_h = \\int h(\\theta) p(\\theta)\\,d(\\theta)\\). That’s lovely and everything, but if it was easy none of us would have jobs. High-dimensional quadrature is a pain in the arse.\nA very simple way to get an decent estimate of \\(I_h\\) is to use importance sampling, that is taking draws \\(\\theta_s\\), \\(s = 1,\\ldots, S\\) from some proposal distribution \\(\\theta_s \\sim g(\\theta)\\). Then, noting that \\[\nI_h = \\int h(\\theta) p (\\theta)\\,d\\theta = \\int h(\\theta) \\underbrace{\\frac{p(\\theta)}{g(\\theta)}}_{r(\\theta)}g(\\theta)\\,d\\theta,\n\\] we can use Monte Carlo to estimate the second integral. This leads to the importance sampling estimator \\[\nI_h^S = \\sum_{s=1}^S h(\\theta_s) r(\\theta_s).\n\\]\nThis all seems marvellous, but there is a problem. Even though \\(h\\) is probably a very pleasant function and \\(g\\) is a nice friendly distribution, \\(r(\\theta)\\) can be an absolute beast. Why? Well it’s1 the ratio of two densities and there is no guarantee that the ratio of two nice functions is itself a nice function. In particular, if the bulk of the distributions \\(p\\) and \\(g\\) are in different places, you’ll end up with the situation where for most draws \\(r(\\theta_s)\\) is very small2 and a few will be HUGE3.\nThis will lead to an extremely unstable estimator.\nIt is pretty well known that the raw importance sampler \\(I_h^S\\) will behave nicely (that is will be unbiased with finite variance) precisely when the distribution of \\(r_s = r(\\theta_s)\\) has finite variance.\nElementary treatments stop there, but they miss two very big problems. The most obvious one is that it’s basically impossible to check if the variance of \\(r_s\\) is finite. A second, much larger but much more subtle problem, is that the variance can be finite but massive. This is probably the most common case in high dimensions. McKay has an excellent example where the importance ratios are bounded, but that bound is so large that it is infinite for all intents and purposes.\nAll of which is to say that importance sampling doesn’t work unless you work on it."
  },
  {
    "objectID": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#truncated-importance-sampling",
    "href": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#truncated-importance-sampling",
    "title": "Tail stabilization of importance sampling etimators: A bit of theory",
    "section": "Truncated importance sampling",
    "text": "Truncated importance sampling\nIf the problem is the fucking ratios then by gum we will fix the fucking ratios. Or so the saying goes.\nThe trick turns out to be modifying the largest ratios enough that we stabilise the variance, but not so much as to overly bias the estimate.\nThe first version of this was truncated importance sampling (TIS), which selects a threshold \\(T\\) and estimates the expectation as \\[\nI_\\text{TIS}^S = \\frac{1}{S}\\sum_{s= 1}^S h(\\theta_s) \\max\\{r(\\theta_s), T\\}.\n\\] It’s pretty obvious that \\(I^S_\\text{TIS}\\) has finite variance for any fixed \\(T\\), but we should be pretty worried about the bias. Unsurprisingly, there is going to be a trade-off between the variance and the bias. So let’s explore that.\n\nThe bias of TIS\nTo get an expression for the bias, first let us write \\(r_s = r(\\theta_s)\\) and \\(h_s = h(\\theta_s)\\) for \\(\\theta_s \\sim g\\). Occasionally we will talk about the joint distribution or \\((r_s,h_s) \\sim (R,H)\\). Sometimes we will also need to use the indicator variables \\(z_i = 1_{r_i &lt; T}\\).\nThen, we can write4 \\[\nI = \\mathbb{E}(HR \\mid R \\leq T) \\Pr(R \\leq T) + \\mathbb{E}(HR \\mid R &gt; T) \\Pr(R &gt; T).\n\\]\nHow does this related to TIS? Well. Let \\(M = \\sum_{s=1}^S z_i\\) be the random variable denoting the number of times \\(r_i &gt; T\\). Then, \\[\\begin{align*}\n\\mathbb{E}(I_\\text{TIC}^S) &= \\mathbb{E}\\left( \\frac{1}{S}\\sum_{s=1}^Sz_ih_ir_i\\right)  + \\mathbb{E}\\left( \\frac{T}{S}\\sum_{s=1}^S(1-z_i)h_i\\right) \\\\\n&=\\mathbb{E}_M\\left[\\frac{S-M}{S}\\mathbb{E}(HR \\mid R &lt; T) + \\frac{MT}{S}\\mathbb{E}(H \\mid R &gt; T)\\right] \\\\\n&=\\mathbb{E}(HR \\mid R \\leq T) \\Pr(R \\leq T) + T\\mathbb{E}(H \\mid R &gt; T) \\Pr(R &gt; T).\n\\end{align*}\\]\nHence the bias in TIS is \\[\nI - \\mathbb{E}(I_\\text{TIS}^S) = \\mathbb{E}(H(R-T) \\mid R &gt; T) \\Pr(R &gt; T).\n\\]\nTo be honest, this doesn’t look phenomenally interesting for fixed \\(T\\), however if we let \\(T = T_S\\) depend on the sample size then as long as \\(T_S \\rightarrow \\infty\\) we get vanishing bias.\nWe can get more specific if we make the assumption about the tail of the importance ratios. In particular, we will assume that5 \\(1-R(r) = \\Pr(R &gt; r) = cr^{-1/k}(1+o(1))\\) for some6 \\(k&lt;1\\).\nWhile it seems like this will only be useful for estimating \\(\\Pr(R&gt;T)\\), it turns out that under some mild7 technical conditions, the conditional excess distribution function8 \\[\nR_T(y) = \\Pr(R - T \\leq y \\mid R &gt; T) = \\frac{R(T + y) - R(T)}{1-R(T)},\n\\] is well approximated by a Generalised Pareto Distribution as \\(T\\rightarrow \\infty\\). Or, in maths, as \\(T\\rightarrow \\infty\\), \\[\nR_T(y) \\rightarrow \\begin{cases} 1- \\left(1 + \\frac{ky}{\\sigma}\\right)^{-1/k}, \\quad & k \\neq 0 \\\\\n1- \\mathrm{e}^{-y/\\sigma}, \\quad &k = 0,\n\\end{cases}\n\\] for some \\(\\sigma &gt; 0\\) and \\(k \\in \\mathbb{R}\\). The shape9 parameter \\(k\\) is very important for us, as it tells us how many moments the distribution has. In particular, if a distribution \\(X\\) has shape parameter \\(k\\), then \\[\n\\mathbb{E}|X|^\\alpha &lt; \\infty, \\quad \\forall \\alpha &lt; \\frac{1}{k}.\n\\] We will focus exclusively on the case where \\(k &lt; 1\\). When \\(k &lt; 1/2\\), the distribution has finite variance.\nIf \\(1- R(r) = cr^{-1/k}(1+  o(1))\\), then the conditional exceedence function is \\[\\begin{align*}\nR_T(y) &=  \\frac{cT^{-1/k}(1+  o(1)) - c(T+y)^{-1/k}(1+  o(1))}{cT^{-1/k}(1+  o(1)))} \\\\\n&= \\left[1 - \\left(1 + \\frac{y}{T}\\right)^{-1/k}\\right](1 + o(1)),\n\\end{align*}\\] which suggests that as \\(T\\rightarrow \\infty\\), \\(R_T\\) converges to a generalised Pareto distribution with shape parameter \\(k\\) and scale parameter \\(\\mathcal{O}(T)\\).\nAll of this work lets us approximate the distribution of \\((R-T \\mid R&gt;T )\\) and use the formula for the mean of a generalised Pareto distribution. This gives us the estimate \\[\n\\mathbb{E}(R- T \\mid R&gt;T) \\approx \\frac{T}{1-k},\n\\] which estimates the bias when \\(h(\\theta)\\) is constant10 as \\[\nI - \\mathbb{E}(I_\\text{TIS}^S) \\approx \\mathcal{O}\\left(T^{1-1/k}\\right).\n\\]\nFor what it’s worth, Ionides got the same result more directly in the TIS paper, but he wasn’t trying to do what I’m trying to do.\n\n\nThe variance in TIS\nThe variance is a little bit more annoying. We want it to go to zero.\nAs before, we condition on \\(z_s\\) (or, equivalently, \\(M\\)) and then use the law of total variance. We know from the bias calculation that \\[\n\\mathbb{E}(I_\\text{TIS}^S \\mid M) =\\frac{S-M}{S}\\mathbb{E}(HR \\mid R&gt;T) + \\frac{TM}{S}\\mathbb{E}(H \\mid R&gt;T).\n\\]\nA similarly quick calculation tells us that \\[\n\\mathbb{V}(I_\\text{TIS}^S \\mid M) = \\frac{S-M}{S^2}\\mathbb{V}(HR \\mid R \\leq T) +\\frac{MT^2}{S^2}\\mathbb{V}(H \\mid R&gt;T).\n\\] To close it out, we recall that \\(M\\) is the sum of Bernoulli random variables so \\[\nM \\sim \\text{Binomial}(S, \\Pr(R &gt; T)).\n\\]\nWith this, we can get an expression for the unconditional variance. To simplify the expression, let’s write \\(p_T = \\Pr(R &gt; T)\\). Then, \\[\\begin{align*}\n\\mathbb{V}(I_\\text{TIS}^S) &=\\mathbb{E}_M\\mathbb{V}(I_\\text{TIS}^S \\mid M) + \\mathbb{V}_M\\mathbb{E}(I_\\text{TIS}^S \\mid M) \\\\\n&= S^{-1}(1-p_T)\\mathbb{V}(HR \\mid R \\leq T) +S^{-1}T^2p_T\\mathbb{V}(H \\mid R&gt;T)\\\\\n&\\quad + S^{-1}p_T(1-p_T)\\mathbb{E}(HR \\mid R&gt;T)^2 + S^{-1}Tp_T(1-p_T)\\mathbb{E}(H \\mid R&gt;T)^2.\n\\end{align*}\\]\nThere are four terms in the variance. The first and third terms are clearly harmless: they go to zero no matter how we choose \\(T_S\\). Our problem terms are the second and fourth. We can tame the fourth term if we choose \\(T_S = o(S)\\). But that doesn’t seem to help with the second term. But it turns out it is enough. To see this, we note that \\[\\begin{align*}\nTp_T\\mathbb{V}(H\\mid R&gt;T) &=\\leq Tp_T\\mathbb{E}(H^2 \\mid R&gt;T)\\\\\n&\\leq p_T\\mathbb{E}(H^2 R\\mid R&gt;T) \\\\\n&\\leq \\mathbb{E}(H^2 R)\\\\\n&= \\int h(\\theta)^2 p(\\theta)\\,d\\theta &lt; \\infty.\n\\end{align*}\\] where the second inequality uses the fact that \\(R&gt;T\\) and the third comes from the law of total probability.\nSo the TIS estimator has vanishing bias and variance as long as the truncation \\(T_S \\rightarrow \\infty\\) and \\(T_S = o(S)\\). Once again, this is in the TIS paper, where it is proved in a much more compact way.\n\n\nAsymptotic properties\nIt can also be useful to have an understanding of how wild the fluctuations \\(I - I_\\text{TIS}^S\\) are. For traditional importance sampling, we know that if \\(\\mathbb{E}(R^2)\\) is finite, then then the fluctuations are, asymptotically, normally distributed with mean zero. Non-asymptotic results were given by Chatterjee and Diaconis that also hold even when the estimator has infinite variance.\nFor TIS, it’s pretty obvious that for fixed \\(T\\) and \\(h \\geq 0\\), \\(I_\\text{TIS}^S\\) will be asymptotically normal (it is, after all, the sum of bounded random variables). For growing sequences \\(T_S\\) it’s a tiny bit more involved: it is now a triangular array11 rather than a sequence of random variables. But in the end very classical results tell us that for bounded12 \\(h\\), the fluctuations of the TIS estimator are asymptotically normal.\nIt’s worth saying that when \\(h(\\theta)\\) is unbounded, it might be necessary to truncate the product \\(h_ir_i\\) rather than just \\(r_i\\). This is especially relevant if \\(\\mathbb{E}(H \\mid R=r)\\) grows rapidly with \\(r\\). Personally, I can’t think of a case where this happens: \\(r(\\theta)\\) usually grows (super-)exponentially in \\(\\theta\\) while \\(h(\\theta)\\) usually grows polynomially, which implies \\(\\mathbb{E}(H \\mid R=r)\\) grows (poly-)logarithmically.\nThe other important edge case is that when \\(h(\\theta)\\) can be both positive and negative, it might be necessary to truncate \\(h_ir_i\\) both above and below."
  },
  {
    "objectID": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#winsorised-importance-sampling",
    "href": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#winsorised-importance-sampling",
    "title": "Tail stabilization of importance sampling etimators: A bit of theory",
    "section": "Winsorised importance sampling",
    "text": "Winsorised importance sampling\nTIS has lovely theoretical properties, but it’s a bit challenging to use in practice. The problem is, there’s really no practical guidance on how to choose the truncation sequence.\nSo let’s do this differently. What if instead of specifying a threshold directly, we instead decided that the largest \\(M\\) values are potentially problematic and should be modified? Recall that for TIS, the number of samples that exceeded the threshold, \\(M\\), was random while the threshold was fixed. This is the opposite situation: the number of exceedences is fixed but the threshold is random.\nThe threshold is now the \\(M\\)th largest value of \\(r_s\\). We denote this using order statistics notation: we re-order the sample so that \\[\nr_{1:S} \\leq r_{2:S}\\leq \\ldots r_{S:S}.\n\\] With this notation, the threshold is \\(T = r_{S-M+1:S}\\) and the Winsorized importance sampler (WIS) is \\[\nI^S_\\text{WIS} = \\frac{1}{S}\\sum_{s = 1}^{S-M} h_{s:S}r_{s:S} + \\frac{r_{S-M+1:S}}{S}\\sum_{s=S-M+1}^S h_{s:S},\n\\] where \\((r_{s:S}, h_{s:S})\\) are the \\((r_s, h_s)\\) pairs ordered so that \\(r_{1:S} \\leq r_{2:S}\\leq \\cdots \\leq r_{S:S}\\). Note that \\(h_{s:S}\\) are not necessarily in increasing order: they are known as concomitants of \\(r_{s:S}\\), which is just a fancy way to say that they’re along for the ride. It’s very important that we reorder the \\(h_s\\) when we reorder the \\(r_s\\), otherwise we won’t preserve the joint distribution and we’ll end up with absolute rubbish.\nWe can already see that this is both much nicer and much wilder than the TIS distribution. It is convenient that \\(M\\) is no longer random! But what the hell are we going to do about those order statistics? Well, the answer is very much the same thing as before: condition on them and hope for the best.\nConditioned on the event13 \\(\\{r_{S-M+1:S} = T\\}\\), we get \\[\n\\mathbb{E}\\left(I_\\text{WIS}^S \\mid r_{S-M+1:S} = T\\right) = \\left(1 - \\frac{M}{S}\\right)\\mathbb{E}(RH \\mid R &lt; T) + \\frac{MT}{S} \\mathbb{E}(H \\mid R \\geq T).\n\\] From this, we get that the bias, conditional on \\(r_{S-M+1:S} = T\\) is \\[\\begin{multline*}\n\\left|I - \\mathbb{E}\\left(I_\\text{WIS}^S \\mid r_{S-M+1:S} = T\\right)\\right| =\\left|\\left[\\Pr(R &lt; T) - \\left(1 - \\frac{M}{S}\\right)\\right]\\mathbb{E}(RH \\mid R &lt; T) \\right.\\\\\n\\left.+ \\left[\\Pr(R \\geq T) - \\frac{M}{S}\\right] \\mathbb{E}(H(R - T) \\mid R \\geq T)\\right|.\n\\end{multline*}\\]\nYou should immediately notice that we are in quite a different situation from TIS, where only the tail contributed to the bias. By fixing \\(M\\) and randomising the threshold, we have bias contributions from both the bulk (due, essentially, to a weighting error) and from the tail (due to both the weighting error and the truncation). This is going to require us to be a bit creative.\nWe could probably do something more subtle and clever here, but that is not my way. Instead, let’s use the triangle inequality to say \\[\n\\left|\\mathbb{E}(RH \\mid R &gt; T)\\right| \\leq \\frac{\\mathbb{E}(R |H| 1(R&lt;T))}{\\Pr(R &lt;T)} \\leq \\frac{\\|h\\|_{L^1(p)}}{\\Pr(R  &lt;T)}\n\\] and so the first term in the bias can be bounded if we can bound the relative error \\[\n\\mathbb{E}\\left|1 - \\frac{1- M/S}{\\Pr(R &lt; r_{S-M+1:S})}\\right|.\n\\]\nNow the more sensible among you will say Daniel, No! That’s a ratio! That’s going to be hard to bound. And, of course, you are right. But here’s the thing: if \\(M\\) is small relative to \\(S\\), it is tremendously unlikely that \\(r_{S-M+1:S}\\) is anywhere near zero. This is intuitively true, but also mathematically true.\nTo attack this expectation, we are going to look at a slightly different quantity that has the good grace of being non-negative.\n\nLemma 1 Let \\(X_s\\), \\(s= 1, \\ldots S\\) be an iid sample from \\(F_X\\), let \\(0\\leq k\\leq S\\) be an integer. Then \\[\n\\frac{p}{F_X(x_{k:S})} -p \\stackrel{d}{=} \\frac{p(S-k+1)}{k} \\mathcal{F},\n\\] and \\[\n\\frac{1-p}{1- F_x/(x_{k:S})} - (1-p) \\stackrel{d}{=} \\frac{k(1-p)}{S-k+1}\\mathcal{F}^{-1}\n\\] where \\(\\mathcal{F}\\) is an F-distributed random variable with parameters \\((2(S-k+1), 2k)\\).\n\n\nProof. For any \\(t\\geq 0\\), \\[\\begin{align*}\n\\Pr\\left(\\frac{p}{F_X(x_{k:S})} - p \\leq t\\right) &=\\Pr\\left(p - pF_X(x_{k:S}) \\leq tF_X(x_{k:S})\\right) \\\\\n&= \\Pr\\left(p  \\leq (t+p)F_X(x_{k:S})\\right) \\\\\n&=\\Pr\\left(F_X(x_{k:S}) \\geq \\frac{p}{p+t}\\right)\\\\\n&= \\Pr\\left(x_{k:S} \\geq F_X^{-1}\\left(\\frac{p}{p+t}\\right)\\right)\\\\\n&= 1- I_{\\frac{p}{p+t}}(k, S-k+1) \\\\\n&= I_{\\frac{t}{p+t}}(S-k+1, k),\n\\end{align*}\\] where \\(I_p(a,b)\\) is the incomplete Beta function.\nYou could, quite reasonably, ask where the hell that incomplete Beta function came from. And if I had thought to look this up, I would say that it came from Equation 2.1.5 in David and Nagaraja’s book on order statistics. Unfortunately, I did not look this up. I derived it, which is honestly not very difficult. The trick is to basically note that the event \\(\\{x_{k:S} \\leq \\tau\\}\\) is the same as the event that at least \\(k\\) of the samples \\(x_s\\) are less than or equal to \\(\\tau\\). Because the \\(x_s\\) are independent, this is the probability of observing at least \\(k\\) heads from a coin with the probability of a head \\(\\Pr(x \\leq \\tau) = F_X(\\tau)\\). If you look this up on Wikipedia14 you see15 that it is \\(I_{1-F_X(\\tau)}(k,S-k+1)\\). The rest just come from noting that \\(\\tau = F_X^{-1}(t/(p+t))\\) and using the symmetry \\(1-I_p(a,b) = I_{1-p}(b,a)\\).\nTo finish this off, we note that \\[\n\\Pr(\\mathcal{F} \\leq x) = I_{\\frac{S-k+1}{(S-k+1)x+ k}}(S-k+1,k).\n\\] From which, we see that \\[\\begin{align*}\n\\Pr\\left(\\frac{p}{F_X(x_{k:S})} - p \\leq t\\right) &=\\Pr\\left(\\mathcal{F} \\leq \\frac{k}{p(S-k+1)}t\\right) \\\\\n&= \\Pr\\left(\\frac{p(S-k+1)}{k}\\mathcal{F} \\leq t\\right).\n\\end{align*}\\]\nThe second result follows the same way and by noting that \\(\\mathcal{F}^{-1}\\) is also F-distributed with parameters \\((k, S-k+1)\\).\nThe proof has ended\n\nNow, obviously, in this house we do not trust mathematics. Which is to say that I made a stupid mistake the first time I did this and forgot that when \\(Z\\) is binomial, \\(\\Pr(Z \\geq k) = 1 - \\Pr(Z \\leq k-1)\\) and had a persistent off-by-one error in my derivation. But we test out our results so we don’t end up doing the dumb thing.\nSo let’s do that. For this example, we will use generalised Pareto-distributed \\(X\\).\n\nlibrary(tidyverse)\nxi &lt;- 0.7\ns &lt;- 2\nu &lt;- 4\n\nsamp &lt;- function(S, k, p, \n                 Q = \\(x) u + s*((1-x)^(-xi)-1)/xi, \n                 F = \\(x) 1 - (1 + xi*(x - u)/s)^(-1/xi)) {\n  # Use theory to draw x_{k:S}\n  xk &lt;- Q(rbeta(1, k, S - k + 1))\n  c(1 - p / F(xk), 1-(1-p)/(1-F(xk)))\n}\n\nS &lt;- 1000\nM &lt;- 50\nk &lt;- S - M + 1\np &lt;- 1-M/S\nN &lt;- 100000\n\nfs &lt;- rf(N, 2 * (S - k + 1), 2 * k )\ntibble(theoretical = 1-p - p * fs * (S - k + 1)/k,\n       xks = map_dbl(1:N, \\(x) samp(S, k, p)[1])) %&gt;%\n  ggplot() + stat_ecdf(aes(x = xks), colour = \"black\") + \n  stat_ecdf(aes(x = theoretical), colour = \"red\", linetype = \"dashed\") +\n  ggtitle(expression(1 - frac(1-M/S , R(r[S-M+1:S]))))\n\n\n\n\n\n\n\ntibble(theoretical = p - (1-p) * k/(fs * (S - k + 1)),\n       xks = map_dbl(1:N, \\(x) samp(S, k, p)[2])) %&gt;%\n  ggplot() + stat_ecdf(aes(x = xks), colour = \"black\") + \n  stat_ecdf(aes(x = theoretical), colour = \"red\", linetype = \"dashed\") +\n  ggtitle(expression(1 - frac(M/S , 1-R(r[S-M+1:S]))))\n\n\n\n\n\n\n\n\nFabulous. It follow then that \\[\n\\left|1 - \\frac{1-M/S}{R(r_{S-M+1})} \\right| \\stackrel{d}= \\left|\\frac{M}{S} -  \\frac{M(S-M)}{S(S-M-1)}\\mathcal{F}\\right| \\leq \\frac{M}{S} +  \\frac{M(S-M)}{S(S-M-1)} \\mathcal{F},\n\\] where \\(\\mathcal{F}\\) has an F-distribution with \\((M, S-M+1)\\) degrees of freedom. As \\(\\mathbb{E}(\\mathcal{F}) = 1 + 1/(S-M-1)\\), it follows that this term goes to zero as long as \\(M = o(S)\\). This shows that the first term in the bias goes to zero.\nIt’s worth noting here that we’ve also calculated that the bias is at most \\(\\mathcal{O}(M/S)\\), however, this rate is extremely sloppy. That upper bound we just computed is unlikely to be tight. A better person than me would probably check, but honestly I just don’t give a shit16\nThe second term in the bias is \\[\n\\left[\\Pr(R \\geq T) - \\frac{M}{S}\\right] \\mathbb{E}(H(R - T) \\mid R \\geq T).\n\\] As before, we can write this as \\[\n\\left(1 - \\frac{M/S}{1-R(T)}\\right)|\\mathbb{E}(H(R - T) 1_{R \\geq T})| \\leq \\left|1 - \\frac{M/S}{1-R(T)}\\right|\\|h\\|_{L^1(p)}.\n\\] By our lemma, we know that the distribution of the term in the absolute value when \\(T = r_{S-M+1}\\) is the same as \\[\n1-\\frac{M}{S} -\\left(1 - \\frac{M}{S} + \\frac{1}{S}\\right)\\mathcal{F} = (\\mu_F-\\mathcal{F})  +\\frac{M}{S}(\\mathcal{F}-\\mu_F) - \\frac{1}{S}\\mathcal{F} +  \\frac{1}{M-1}\\left(\\frac{M}{S} - 1\\right),\n\\] where \\(\\mathcal{F} \\sim \\text{F}_{2(S-M+1), 2M}\\), which has mean \\(\\mu_F = 1+(M-1)^{-1}\\) and variance \\[\n\\sigma^2_F = \\frac{M^2S}{(S-M+1)(M-1)^2(M-2)} = \\frac{1}{M}(1 + \\mathcal{O}(M^{-1} + MS^{-1}).\n\\] From Jensen’s inequality, we get \\[\n\\mathbb{E}(|\\mathcal{F} - \\mu_F|) \\leq \\sigma_F = M^{-1/2}(1 + o(1)).\n\\] If follows that \\[\n\\mathbb{E}\\left|1 - \\frac{M/S}{1-R(r_{S-M+1:S})}\\right| \\leq M^{-1/2}(1+o(1))M^{1/2}S^{-1}(1 + o(1)) + S^{-1}(1+ o(1)) + (M-1)^{-1}(1+o(1)),\n\\] and so we get vanishing bias as long as \\(M\\rightarrow \\infty\\) and \\(M/S \\rightarrow 0\\).\nOnce again, I make no claims of tightness17. Just because it’s a bit sloppy at this point doesn’t mean the job isn’t done.\n\nTheorem 1 Let \\(\\theta_s\\), \\(s = 1,\\ldots, S\\) be an iid sample from \\(G\\) and let \\(r_s = r(\\theta_s) \\sim R\\). Assume that\n\n\\(R\\) is absolutely continuous\n\\(M  \\rightarrow \\infty\\) and \\(S^{-1}M \\rightarrow 0\\)\n\\(h \\in L^1(p)\\)\n\nThen Winsorized importance sampling converges in \\(L^1\\) and is asymptotically unbiased.\n\nOk so that’s nice. But you’ll notice that I did not mention our piss-poor rate. That’s because there is absolutely no way in hell that the bias is \\(\\mathcal{O}(M^{-1/2})\\)! That rate is an artefact of a very sloppy bound on \\(\\mathbb{E}|1-\\mathcal{F}|\\).\nUnfortunately, Mathematica couldn’t help me out. Its asymptotic abilities shit the bed at the sight of \\({}_2F_1(a,b;c;z))\\), which is everywhere in the exact expression (which I’ve put below in the fold.\n\n\nMathematica expression for \\(\\mathbb{E}|1-\\mathcal{F}|\\).\n\n-(((M/(1 + S))^(-(1/2) - S/2)*Gamma[(1 + S)/2]*\n     (6*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        5*M*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        M^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        8*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        6*M*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        M^2*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        2*S^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        M*S^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n         6*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] + 8*M*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] - \n        2*M^2*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] - 8*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*S*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] + \n        4*M*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*S*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] - 2*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*S^2*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] + \n        6*M*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), (1/2)*(3 - M + S), \n                          (-1 + M - S)/M] - 5*M^2*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), \n                                      (1/2)*(3 - M + S), (-1 + M - S)/M] + M^3*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(1 - M + S), (1/2)*(3 - M + S), (-1 + M - S)/M] + \n        2*M*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), (1/2)*(3 - M + S), \n                          (-1 + M - S)/M] - M^2*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), \n                                      (1/2)*(3 - M + S), (-1 + M - S)/M] - 2*M*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(3 - M + S), (1/2)*(5 - M + S), (-1 + M - S)/M] + \n        3*M^2*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), (1/2)*(5 - M + S), \n                          (-1 + M - S)/M] - M^3*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), \n                                      (1/2)*(5 - M + S), (-1 + M - S)/M] - 2*M*S*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(3 - M + S), (1/2)*(5 - M + S), (-1 + M - S)/M] + \n        M^2*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), (1/2)*(5 - M + S), \n                          (-1 + M - S)/M]))/(((1 + S)/(1 - M + S))^S*\n                                               (2*(-2 + M)*M*Sqrt[(-1 - S)/(-1 + M - S)]*Gamma[M/2]*\n                                                  Gamma[(1/2)*(5 - M + S)])))\n\nBut do not fear: we can recover. At the cost of an assumption about the tails of \\(R\\). (We’re also going to assume that \\(h\\) is bounded because it makes things ever so slightly easier, although unbounded \\(h\\) is ok18 as long as it doesn’t grow too quickly relative to \\(r\\).)\nWe are going to make the assumption that \\(R - T \\mid R\\geq T\\) is in the domain of attraction of a generalized Pareto distribution with shape parameter \\(k\\). A sufficient condition, due to von Mises, is that \\[\n\\lim_{r\\rightarrow \\infty} \\frac{r R'(r)}{1-R(r)} = \\frac{1}{k}.\n\\]\nThis seems like a weird condition, but it’s basically just a regularity condition at infinity. For example if \\(1-R(r)\\) is regularly varying at infinity19 and \\(R'(r)\\) is, eventually, monotone20 decreasing, then this condition holds.\nThe von Mises condition is very natural for us as Falk and Marohn (1993) show that the relative error we get when approximating the tail of \\(R\\) by a generalised Pareto density is the same as the relative error in the von Mises condition. That is if \\[\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 + \\mathcal{O}(r^{-\\alpha}))\n\\] then \\[\nR'(r) = c w(cr - d)(1 + \\mathcal{O}(r^{-\\alpha})),\n\\] where \\(c,d\\) are constants and \\(w\\) is the density of a generalised Pareto distribution.\nAnyway, under those two assumptions, we can swap out the density of \\((R-T)\\mid R&gt;T\\) with its asymptotic approximation and get that, conditional on \\(T=  r_{S-M+1:S}\\), \\[\n\\mathbb{E}(H(R-T) \\mid R&gt;T) = (k-1)^{-1}T.\n\\]\nHence, the second term in the bias goes to zero if \\[\n\\mathbb{E}\\left(r_{S-M+1:S}\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)\\right)\n\\] goes to zero.\nNow this is not particularly pleasant, but it helps to recognise that even if a distribution doesn’t have finite moments, away from the extremes, its order statistics always do. This means that we can use Cauchy-Schwartz to get \\[\n\\left|\\mathbb{E}\\left(r_{S-M+1:S}\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)\\right)\\right| \\leq\\mathbb{E}\\left(r_{S-M+1:S}^2\\right)^{1/2}\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right]^{1/2}.\n\\]\nArguably, the most alarming term is the first one, but that can21 be tamed. To do this, we lean into a result from Bickel (1967) who, if you examine the proof and translate some obscurely-stated conditions and fix a typo22, you get that \\[\n\\mathbb{E}(r_{k:M}^2) \\leq C k\\begin{pmatrix} S \\\\ k\\end{pmatrix} \\int_0^1 t^{k-2-1}(1-t)^{S-k-2}\\,dt.\n\\] You might worry that this is going to grow too quickly. But it doesn’t. Noting that \\(B(n,m) = \\Gamma(n)\\Gamma(m)/\\Gamma(n+m)\\), we can rewrite the upper bound in terms of the Beta function to get \\[\n\\mathbb{E}(r_{k:M}^2) \\leq C \\frac{\\Gamma(S+1)}{\\Gamma(S-3)} \\frac{\\Gamma(k-2)}{\\Gamma(k+1)}\\frac{\\Gamma(S-k-1)}{\\Gamma(S-k+1)}.\n\\]\nTo show that this doesn’t grow too quickly, we use the identity \\[\n\\frac{\\Gamma(x + a)}{\\Gamma(x + b)} \\propto x^{a-b}(1 + \\mathcal{O}(x^{-1})).\n\\] From this, it follows that \\[\n\\mathbb{E}(r_{k:M}^2) \\leq C S^4k^{-3}(S-k)^{-2}(1+ \\mathcal{O}(S^{-1}))(1+ \\mathcal{O}(k^{-1}))(1+ \\mathcal{O}((S+k)^{-1})).\n\\] In this case, we are interested in \\(k = S-M+1\\), so \\[\n\\mathbb{E}(r_{k:M}^2) \\leq C S^4S^{-3}M^{-2}(1 - M/S + 1/S)^{-3}(1 - 1/M)^{-2}(1+ \\mathcal{O}(S^{-1}))(1+ \\mathcal{O}(S^{-1}))(1+ \\mathcal{O}(M^{-1})).\n\\]\nHence the we get that \\(\\mathbb{E}(r_{k:M}^2) = \\mathcal{O}(SM^{-2})\\). This is increasing23 in \\(S\\), but we will see that it is not going up too fast.\nFor the second half of this shindig, we are going to attack \\[\n\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right] = \\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S})\\right)^2 - 2\\left(1 - R(r_{s-M+1:S})\\right)\\frac{M}{S} +\\left(\\frac{M}{S}\\right)^2\\right].\n\\] A standard result24 from extreme value theory is that \\(R(r_{k:S})\\) has the same distribution as the \\(k\\)th order statistics from a sample of \\(S\\) iid \\(\\text{Uniform}([0,1])\\) random variables. Hence25, \\[\nR(r_{S-M+1:S}) \\sim \\text{Beta}(S-M+1, M).\n\\] If follows26 that \\[\n\\mathbb{E}(1- R(r_{S-M+1:S})) = \\frac{M}{S+1} = \\frac{M}{S}\\frac{1}{1+S^{-1}}\n\\] and \\[\n\\mathbb{E}((1- R(r_{S-M+1:S}))^2) = \\frac{M(M+1)}{(S+1)(S+2)} = \\frac{M^2}{S^2}\\left(\\frac{1 + M^{-1}}{1 + 3S^{-1} + 2S^{-2}}\\right).\n\\] Adding these together and doing some asymptotic expansions, we get \\[\n\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right] = \\frac{M^2}{S^2} + \\mathcal{O}\\left(\\frac{M}{S^2}\\right),\n\\] which goes to zero27 like \\(\\mathcal{O}(S^{-1})\\) if \\(M = \\mathcal{O}(S^{1/2})\\).\nWe can multiply this rate together and get that the second term in the bias is bounded above by \\[\n\\left[\\left(\\frac{S}{M^2} (1 + \\mathcal{O}(M^{-1} + MS^{-1}))\\right)\\left(\\frac{M^2}{S^2} (1 + \\mathcal{O}(M^{-1} + MS^{-1})\\right)\\right]^{1/2} = S^{-1/2}(1 + o(1)).\n\\]\nPutting all of this together we have proved the following Corollary.\n\nCorollary 1 Let \\(\\theta_s\\), \\(s = 1,\\ldots, S\\) be an iid sample from \\(G\\) and let \\(r_s = r(\\theta_s) \\sim R\\). Assume that\n\n\\(R\\) is absolutely continuous and satisfies the von Mises condition28 \\[\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 +\\mathcal{O}(r^{-1})).\n\\]\n\\(M  = o(S)\\)\n\\(h\\) is bounded29\n\nWinsorized importance sampling converges in \\(L^1\\) with rate of, at most, \\(\\mathcal{O}(MS^{-1} + S^{-1/2})\\), which is balanced when \\(M = \\mathcal{O}(S^{1/2})\\). Hence, WIS is30 \\(\\sqrt{n}\\)-consistent.\n\n\nVariance of Winsorized Importance Sampling\nRight, that was a bit of a journey, but let’s keep going to the variance.\nIt turns out that following the route I thought I was going to follow does not end well. That lovely set of tricks breaking up the variance into two conditional terms turns out to be very very unnecessary. Which is good, because I thoroughly failed to make the argument work.\nIf you’re curious, the problem is that the random variable \\[\n\\frac{Mr_{S-M+1:S}}{S} \\mathbb{E}(H \\mid R \\geq r_{S-M+1:S}) = \\frac{Mr_{S-M+1:S}}{S(1-R(r_{S-M+1:S}))} \\mathbb{E}(H 1_{R \\geq r_{S-M+1:S}})\n\\] is an absolute bastard to bound. The problem is that \\(1- R({r_{S-M+1:S}}) \\approx M/S\\) and so the usual trick of bounding that truncated expectation by \\(\\|h\\|\\) or some such thing will prove that the variance is finite but not that it goes to zero. There is a solid chance that the Cauchy-Schwartz inequality \\[\n\\frac{Mr_{S-M+1:S}^{1/2}}{S(1-R(r_{S-M+1:S}))} \\mathbb{E}(r_{S-M+1:S}^{1/2}H 1_{R \\geq r_{S-M+1:S}}) \\leq\\frac{Mr_{S-M+1:S}^{1/2}}{S(1-R(r_{S-M+1:S}))}R(r_{S-M+1:S})\\|h\\|_{L^2(p)}\n\\] would work. But truly that is just bloody messy31.\nSo let’s do it the easy way, shall we. Fundamentally, we will use \\[\n\\mathbb{V}\\left(I_\\text{WIS}^S\\right) \\leq \\mathbb{E}\\left([I_\\text{WIS}^S]^2\\right).\n\\] Noting that we can write \\(I_\\text{WIS}^S\\) compactly as \\[\nI_\\text{WIS}^S = \\frac{1}{S}\\sum_{s=1}^S h(\\theta_s)\\min\\{r(\\theta_s), r_{S-M+1:S}\\}.\n\\] Hence, \\[\\begin{align*}\n\\mathbb{E}\\left([I_\\text{WIS}^S]^2\\right) &= \\mathbb{E}_{T\\sim r_{S-M+1:S}}\\left[\\mathbb{E}\\left([I_\\text{WIS}^S]^2 \\mid r_{S-M+1:S} = T\\right)\\right]\\\\\n&=\\frac{1}{S^2}\\mathbb{E}_{T\\sim r_{S-M+1:S}}\\left[\\mathbb{E}\\left(H^2 \\min\\{R^2,T^2\\} \\mid r_{S-M+1:S} = T\\right)\\right]\\\\\n&\\leq\\frac{1}{S^2}\\mathbb{E}_{T\\sim r_{S-M+1:S}}\\left[\\mathbb{E}\\left(RTH^2 \\mid r_{S-M+1:S} = T\\right)\\right] \\\\\n&\\leq\\frac{1}{S^2}\\mathbb{E}_{T\\sim r_{S-M+1:S}}\\left[T\\|h\\|_{L^2(p)}^2\\right]\n\\end{align*}\\]\nThis goes to zero as long as \\(\\mathbb{E}(r_{S-M+1:S}) = o(S^2)\\).\nBickel (1967) shows that, noting that \\(\\mathbb{E}(R) &lt; \\infty\\), \\[\n\\mathbb{E}(r_{S-M+1:S}) \\leq C (S-M+1)\\frac{\\Gamma(S+1)\\Gamma(S-M+1-1)\\Gamma(M)}{\\Gamma(S-M+1+1)\\Gamma(M+1)\\Gamma(S-1)} = \\frac{S}{M}(1 + o(1)),\n\\] and so the variance is bounded.\nThe previous argument shows that the variance is \\(\\mathcal{O}(M^{-1}S^{-1})\\). We can refine that if we assume the von Mises condition hold. In that case we know that \\(R(r) = 1- cr^{-1/k} + o(1)\\) as \\(r\\rightarrow \\infty\\) and therefore \\[\\begin{align*}\nR\\left(R^{-1}\\left(1-\\frac{M}{S}\\right)\\right) &= 1-\\frac{M}{S+1}\\\\\n1 - cR^{-1}\\left(1-\\frac{M}{S+1}\\right)^{-1/k}(1+o(1)) &= 1- \\frac{M}{S+1} \\\\\nR^{-1}\\left(1-\\frac{M}{S+1}\\right) &= c^{-k}\\left(\\frac{M}{S+1}\\right)^{-k}(1 + o(1)).\n\\end{align*}\\] Bickel (1967) shows that \\(\\mathbb{E}(r_{k:S}) = R^{-1}(1-M/(S+1)) + o(1)\\) so combining this with the previous result gives a variance of \\(\\mathcal{O}((M/S)^{k-2})\\). If we take \\(M =\\mathcal{O}(S^{1/2})\\), this gives \\(\\mathcal{S}^{k/2-1}\\), which is smaller than the previous bound for \\(k&lt;1\\). It’s worth noting that Hence the variance goes to zero.\nThe argument that we used here is a modification of the argument in the TIS paper. This lead to a great deal of panic: did I just make my life extremely difficult? Could I have modified the TIS proof to show the bias goes to zero? To be honest, someone might be able to, but I can’t.\nSo anyway, we’ve proved the following theorem.\n\nTheorem 2 Let \\(\\theta_s\\), \\(s = 1,\\ldots, S\\) be an iid sample from \\(G\\) and let \\(r_s = r(\\theta_s) \\sim R\\). Assume that\n\n\\(R\\) is absolutely continuous\n\\(M \\rightarrow \\infty\\) and \\(M^{-1}S \\rightarrow 0\\)\n\\(h \\in L^2(p)\\).\n\nThe variance in Winsorized importance sampling is at most \\(\\mathcal{O}(M^{-1}S)\\)."
  },
  {
    "objectID": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#pareto-smoothed-importance-sampling",
    "href": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#pareto-smoothed-importance-sampling",
    "title": "Tail stabilization of importance sampling etimators: A bit of theory",
    "section": "Pareto-smoothed importance sampling",
    "text": "Pareto-smoothed importance sampling\nPareto-smoothed importance sampling (or PSIS) takes the observation that the tails are approximately Pareto distributed to add some bias correction to the mix. Essentially, it works by noting that approximating \\[\n(1-R(r_{S-M+1:S}))\\mathbb{E}(HR \\mid R&gt;r_{S-M+1:S}) \\approx \\frac{1}{S}\\sum_{m=1}^M w_m h_{S-M+m:S},\n\\] where \\(w_m\\) is the median32 \\(m\\)th order statistic in an iid sample of \\(M\\) Generalised Pareto random variables with tail parameters fitted to the distribution.\nThis is a … funky … quadrature rule. To see that, we can write \\[\n\\mathbb{E}(HR \\mid R&gt;T) = \\mathbb{E}(R \\mathbb{E}(H \\mid R)).\n\\] If we approximate the distribution of \\(R &gt; T\\) by \\[\n\\tilde{R}_\\text{PSIS}(r) = \\frac{1}{M}\\sum_{m=1}^M 1( w_m&lt;r)\n\\] and approximate the conditional probability by \\[\n\\Pr(H &lt; h\\mid R = w_m) \\approx 1(h_{S-M+m:S}&lt; h).\n\\]\nEmpirically, this is a very good choice (with the mild caveat that you need to truncate the largest expected order statistic by the observed maximum in order to avoid some variability issues). I would love to have a good analysis of why that is so, but honest I do not.\nBut, to the issue of this blog post the convergence and vanishing variance still holds. To see this, we note that \\[\nw_m = r_{S-M+1}  + k^{-1}\\sigma\\left[\\left(1-\\frac{j-1/2}{M}\\right)^{-k} -1\\right].\n\\] So we are just re-weighting our tail \\(H\\) samples by \\[\n1 + \\frac{\\sigma}{kr_{S-M+1:S}}\\left[\\left(1-\\frac{j-1/2}{M}\\right)^{-k} -1\\right].\n\\]\nRecalling that when \\(R(r) = 1- cr^{-1/k}(1+ o(1))\\), we had \\(\\sigma = \\mathcal{O}(r_{S-M+1:S})\\), this term is at most \\(\\mathcal{O}(1 + M^{-k})\\). This will not trouble either of our convergence proofs.\nThis leads to the following modification of our previous results.\n\nTheorem 3 Let \\(\\theta_s\\), \\(s = 1,\\ldots, S\\) be an iid sample from \\(G\\) and let \\(r_s = r(\\theta_s) \\sim R\\). Assume that\n\n\\(R\\) is absolutely continuous.\n\\(M  = \\mathcal{O}(S^{1/2})\\)\n\\(h \\in L^2(p)\\)\n\\(k\\) and \\(\\sigma\\) are known with \\(\\sigma = \\mathcal{O}(r_{S-M+1:S})\\).\n\nPareto smoothed importance sampling converges in \\(L^1\\) and its variance goes to zero and it is consistent and asymptotically unbiased.\n\n\nCorollary 2 Assume further that\n\nR satisfies the von Mises condition33 \\[\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 +\\mathcal{O}(r^{-1})).\n\\]\n\\(h\\) is bounded34.\n\nThen the L^1 convergence occurs at a rate of of, at most, \\(\\mathcal{O}(S^{-1/2})\\). Furthermore, the variance of the PSIS estimator goes to zero at least as fast as \\(\\mathcal{O}(S^{k/2-1})\\).\n\nHence, under these additional conditions PSIS is35 \\(\\sqrt{n}\\)-consistent."
  },
  {
    "objectID": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#final-thoughts",
    "href": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#final-thoughts",
    "title": "Tail stabilization of importance sampling etimators: A bit of theory",
    "section": "Final thoughts",
    "text": "Final thoughts\nSo that’s what truncation and winsorization does to importance sampling estimates. I haven’t touched on the fairly important topic of asymptotic normality. Essentially, Griffin (1988), in a fairly complex36 paper that suggests that if you winsorize the product \\((h(\\theta_s)r(\\theta_s))\\) and winsorize it at both ends, the von Mises condition37 imply that the WIS estimator is asymptotically normal.\nWhy is this important, well the same proof shows that doubly winsorized importance sampling (dWIS) applied to the vector valued function \\(\\tilde h(\\theta) = (h(\\theta),1)\\) will also be asymptotically normal, which implies, via the delta method, that the self normalized dWIS estimator \\[\nI^S_\\text{SN-IS} = \\frac{\\sum_{s=1}^S\\max\\{\\min\\{h(\\theta_i) r(\\theta_i),T_{S-M+1:S}\\}, T_{M:S}\\}}{\\sum_{s=1}^S\\max\\{\\min\\{r(\\theta_i),T_{S-M+1:S}\\},T_{M:S}\\}}\n\\] is consistent, where \\(T_{m:S}\\) is the \\(m\\)th order statistic of \\(\\max\\{h(\\theta_s)r(\\theta_s), r(\\theta_s)\\}\\).\nIt is very very likely that this can be shown (perhaps under some assumptions) for something closer to the version of PSIS we use in practice. But that is an open question."
  },
  {
    "objectID": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#footnotes",
    "href": "posts/2022-06-03-that-psis-proof/that-psis-proof.html#footnotes",
    "title": "Tail stabilization of importance sampling etimators: A bit of theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nproportional to↩︎\nbecause \\(p(\\theta_s)\\) is very small↩︎\nbecause \\(p(\\theta_s)\\) is a reasonable size, but \\(g(\\theta_s)\\) is tiny.↩︎\nI have surreptitiously dropped the \\(h\\) subscript because I am gay and sneaky.↩︎\nThat it’s parameterised by \\(1/k\\) is an artefact of history.↩︎\nWe need \\(\\mathbb{E}(R)\\) to be finite, so we need \\(k&lt;1\\).↩︎\nvery fucking complex↩︎\nI have used that old trick of using the same letter for the CDF as the random variable when I have a lot of random variables. ↩︎\naka the tail index↩︎\nThis is a relevant case. But if you think a little bit about it, our problem happens when \\(r(\\theta)\\) grows much faster than \\(h(\\theta)\\). For example if \\(P = \\operatorname{Exp}(1)\\) and \\(G = \\operatorname{Exp}(1/\\lambda)\\) for \\(\\lambda&gt;1\\), then \\(k = 1-1/\\lambda\\), \\(r(\\theta) = \\exp((\\lambda-1)\\theta)\\) and if \\(|h(\\theta)| &lt; |\\theta|^\\alpha\\), then \\(|h(\\theta)| \\leq C \\log(r)^\\alpha\\), which is a slowly growing function.↩︎\nBecause the truncation depends on \\(S\\), moving from the \\(S\\)th partial sum to the \\(S+1\\)th partial sum changes the distribution of \\(z_ih_ir_i\\). This is exactly why the dead Russians gifted us with triangular arrays.↩︎\nAlso practical unbounded \\(h\\), but it’s just easier for bounded \\(h\\)↩︎\nShut up. I know. Don’t care.↩︎\nor, hell, even in a book↩︎\nStraight up, though, I spent 2 days dicking around with tail bounds on sums of Bernoulli random variables for some bloody reason before I just looked at the damn formula.↩︎\nOk. I checked. And yeah. Same technique as below using Jensen in its \\(\\mathbb{E}(|X-\\mathbb{E}(X)|)^2 \\leq \\mathbb{V}(X)\\). If you put that together you get something that goes to zero like \\(M^{1/2}S^{-1}\\), which is \\(\\mathcal{O}(S^{-3/4})\\) for our usual choice of \\(M\\). Which confirms the suspicion that the first term in the bias goes to zero much faster than the second (remembering, of course, that Jensen’s inequality is notoriously loose!).↩︎\nIt’s Pride month↩︎\nThe result holds exactly if \\(\\mathbb{E}(H \\mid R=r) = \\mathcal{O}(\\log^k(r))\\) and with a \\(k\\) turning up somewhere if it’s \\(o(r^{1/k - 1})\\).↩︎\n\\(1-R(r) \\sim c r^{(-1/k)}\\mathcal{L(r)}\\) for a slowly varying function (eg a power of a logarithm) \\(\\mathcal{L}(r)\\).↩︎\nA property that implies this is that \\(1-R(r)\\) is differentiable and convex at infinity, which is to say that there is some finite \\(r_0\\) such that \\(R'(r)\\) exists for all \\(r \\geq r_0\\) and \\(1-R(r)\\) is a monotone function on \\([r_0, \\infty)\\).↩︎\nThere’s a condition here that \\(S\\) has to be large enough, but it’s enough if \\((S-M+1) &gt; 2\\).↩︎\nThe first \\(k\\) in the equation below is missing in the paper. If you miss this, you suddenly get the expected value converging to zero, which would be very surprising. Always sense-check the proofs, people. Even if a famous person did it in the 60s.↩︎\nWe need to take \\(M = \\mathcal{O}(S^{1/2})\\) to be able to estimate the tail index \\(k\\) from a sample, which gives an upper bound by a constant.↩︎\nNote that if \\(U \\sim \\text{Unif}(0,1)\\), then \\(R^{-1}(U) \\sim R\\). Because this is monotone, it doesn’t change ordering of the sample↩︎\nThis is, incidentally, how Bickel got the upper bound on the moments. He combined this with an upper bound on the quantile function.↩︎\nSave the cheerleader, save the world. Except it’s one minus a beta is still beta but with the parameters reversed.↩︎\nAs long as \\(M = o(S)\\)↩︎\nThe rate here is probably not optimal, but it will guarantee that the error in the Pareto approximation doesn’t swamp the other terms.↩︎\nOr \\(\\mathbb{E}(h(\\theta) \\mid r(\\theta) = r)\\) doesn’t grow to quickly, with some modification of the rates in the unlikely case that it grows polynomially.↩︎\nalmost, there’s an epsilon gap but I don’t give a shit↩︎\nAnd girl do not get me started on messy. I ended up going down a route where I used the [inequality]((https://www.sciencedirect.com/science/article/pii/0167715288900077) \\[\n\\mathbb{V}(g(U)) \\leq \\mathbb{E}(U)\\int_0^1\\left[F_U(u) - \\frac{\\mathbb{E}(U1_{U\\leq u})}{\\mathbb{E}(U)}\\right][g'(u)]^2\\,du\n\\] which holds for any \\(U\\) supported on \\([0,1]\\) with differentiable density. And let me tell you. If you dick around with enough beta distributions you can get something. Is it what you want? Fucking no. It is a lot of work, including having to differentiate the conditional expectation, and it gives you sweet bugger all.↩︎\nOr, the expected within \\(o(S^{-1/2})\\)↩︎\nThe rate here is probably not optimal, but it will guarantee that the error in the Pareto approximation doesn’t swamp the other terms.↩︎\nOr \\(\\mathbb{E}(h(\\theta) \\mid r(\\theta) = r)\\) doesn’t grow to quickly, with some modification of the rates in the unlikely case that it grows polynomially.↩︎\nalmost, there’s an epsilon gap but I don’t give a shit↩︎\nI mean, the tools are elementary. It’s just a lot of detailed estimates and Berry-Esseen as far as the eye can see.↩︎\nand more general things↩︎"
  },
  {
    "objectID": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html",
    "href": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html",
    "title": "Sparse Matrices 5: I bind you Nancy",
    "section": "",
    "text": "This is part five of our ongoing series on implementing differentiable sparse linear algebra in JAX. In some sense this is the last boring post before we get to the derivatives. Was this post going to include the derivatives? It sure was but then I realised that a different choice was to go to bed so I can get up nice and early in the morning and vote in our election.\nIt goes without saying that before I split the posts, it was more than twice as long and I was nowhere near finished. So probably the split was a good choice."
  },
  {
    "objectID": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#but-how-do-you-add-a-primative-to-jax",
    "href": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#but-how-do-you-add-a-primative-to-jax",
    "title": "Sparse Matrices 5: I bind you Nancy",
    "section": "But how do you add a primative to JAX?",
    "text": "But how do you add a primative to JAX?\nWell, the first step is you read the docs.\nThey tell you that you need to implement a few things:\n\nAn implementation of the call with “abstract types”\nAn implementation of the call with concrete types (aka evaluation the damn function)\n\nThen,\n\nif you want your primitive to be JIT-able, you need to implement a compilation rule.\nif you want your primitive to be batch-able, you need to implement a batching rule.\nif you want your primitive to be differentiable, you need to implement the derivatives in a way that allows them to be propagated appropriately.\n\nIn this post, we are going to do the first task: we are going to register JAX-traceable versions of the four main primitives we are going to need for our task. For the most part, the implementations here will be replaced with C++ bindings (because only a fool writes their own linear algebra code). But this is the beginning1 of our serious journey into JAX."
  },
  {
    "objectID": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#first-things-first-some-primitives",
    "href": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#first-things-first-some-primitives",
    "title": "Sparse Matrices 5: I bind you Nancy",
    "section": "First things first, some primitives",
    "text": "First things first, some primitives\nIn JAX-speak, a primitive is a function that is JAX-traceable2. It is not necessary for every possible transformation to be implemented. In fact, today I’m not going to implement any transformations. That is a problem for future Dan.\nWe have enough today problems.\nBecause today we need to write four new primitives.\nBut first of all, let’s build up a test matrix so we can at least check that this code runs. This is the same example from blog 3. You can tell my PhD was in numerical analysis because I fucking love a 2D Laplacian.\n\nfrom scipy import sparse\nimport numpy as np\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n)).tocsc()\n    A_lower = sparse.tril(A, format = \"csc\")\n    A_index = A_lower.indices\n    A_indptr = A_lower.indptr\n    A_x = A_lower.data\n    return (A_index, A_indptr, A_x, A)\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\n\n\nPrimitive one: \\(A^{-1}b\\)\nBecause I’m feeling lazy today and we don’t actually need the Cholesky directly for any of this, I’m going to just use scipy. Why? Well, honestly, just because I’m lazy. But also so I can prove an important point: the implementation of the primitive does not need to be JAX traceable. So I’m implementing it in a way that is not now and will likely never be JAX traceable3.\nFirst off, we need to write the solve function and bind it4 to JAX. Specific information about what exactly some of these commands are doing can be found in the docs, but the key thing is that there is no reason to dick around whit JAX types in any of these implementation functions. They are only ever called using (essentially) numpy5 arrays. So we can just program like normal human beings.\n\nfrom jax import numpy as jnp\nfrom jax import core\n\nsparse_solve_p = core.Primitive(\"sparse_solve\")\n\ndef sparse_solve(A_indices, A_indptr, A_x, b):\n  \"\"\"A JAX traceable sparse solve\"\"\"\n  return sparse_solve_p.bind(A_indices, A_indptr, A_x, b)\n\n@sparse_solve_p.def_impl\ndef sparse_solve_impl(A_indices, A_indptr, A_x, b):\n  \"\"\"The implementation of the sparse solve. This is not JAX traceable.\"\"\"\n  A_lower = sparse.csc_array((A_x, A_indices, A_indptr)) \n  \n  assert A_lower.shape[0] == A_lower.shape[1]\n  assert A_lower.shape[0] == b.shape[0]\n  \n  A = A_lower + A_lower.T - sparse.diags(A_lower.diagonal())\n  return sparse.linalg.spsolve(A, b)\n\n## Check it works\nb = jnp.ones(100)\nx = sparse_solve(A_indices, A_indptr, A_x, b)\n\nprint(f\"The error in the sparse sovle is {np.sum(np.abs(b - A @ x)): .2e}\")\n\nThe error in the sparse sovle is  0.00e+00\n\n\nIn order to facilitate its transformations, JAX will occasionally6 call functions using abstract data types. These data types know the shape of the inputs and their data type. So our next step is to specialise the sparse_solve function for this case. We might as well do some shape checking while we’re just hanging around. But the essential part of this function is just saying that the output of \\(A^{-1}b\\) is the same shape as \\(b\\) (which is usually a vector, but the code is no more complex if it’s a [dense] matrix).\n\nfrom jax._src import abstract_arrays\n\n@sparse_solve_p.def_abstract_eval\ndef sparse_solve_abstract_eval(A_indices, A_indptr, A_x, b):\n  assert A_indices.shape[0] == A_x.shape[0]\n  assert b.shape[0] == A_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)"
  },
  {
    "objectID": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#primitive-two-the-triangular-solve",
    "href": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#primitive-two-the-triangular-solve",
    "title": "Sparse Matrices 5: I bind you Nancy",
    "section": "Primitive two: The triangular solve",
    "text": "Primitive two: The triangular solve\nThis is very similar. We need to have a function that computes \\(L^{-1}b\\) and \\(L^{-T}b\\). The extra wrinkle from the last time around is that we need to pass a keyword argument transpose to indicate which system should be solved.\nOnce again, we are going to use the appropriate scipy function (in this case sparse.linalg.spsolve_triangular). There’s a little bit of casting between sparse matrix types here as sparse.linalg.spsolve_triangular assumes the matrix is in CSR format.\n\nsparse_triangular_solve_p = core.Primitive(\"sparse_triangular_solve\")\n\ndef sparse_triangular_solve(L_indices, L_indptr, L_x, b, *, transpose: bool = False):\n  \"\"\"A JAX traceable sparse  triangular solve\"\"\"\n  return sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose = transpose)\n\n@sparse_triangular_solve_p.def_impl\ndef sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, *, transpose = False):\n  \"\"\"The implementation of the sparse triangular solve. This is not JAX traceable.\"\"\"\n  L = sparse.csc_array((L_x, L_indices, L_indptr)) \n  \n  assert L.shape[0] == L.shape[1]\n  assert L.shape[0] == b.shape[0]\n  \n  if transpose:\n    return sparse.linalg.spsolve_triangular(L.T, b, lower = False)\n  else:\n    return sparse.linalg.spsolve_triangular(L.tocsr(), b, lower = True)\n\nNow we can check if it works. We can use the fact that our matrix (A_indices, A_indptr, A_x) is lower-triangular (because we only store the lower triangle) to make our test case.\n\n## Check if it works\nb = np.random.standard_normal(100)\nx1 = sparse_triangular_solve(A_indices, A_indptr, A_x, b)\nx2 = sparse_triangular_solve(A_indices, A_indptr, A_x, b, transpose = True)\nprint(f\"\"\"Error in trianglular solve: {np.sum(np.abs(b - sparse.tril(A) @ x1)): .2e}\nError in triangular transpose solve: {np.sum(np.abs(b - sparse.triu(A) @ x2)): .2e}\"\"\")\n\nError in trianglular solve:  3.53e-15\nError in triangular transpose solve:  5.08e-15\n\n\nAnd we can also do the abstract evaluation.\n\n@sparse_triangular_solve_p.def_abstract_eval\ndef sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, *, transpose = False):\n  assert L_indices.shape[0] == L_x.shape[0]\n  assert b.shape[0] == L_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n\nGreat! Now on to the next one!\n\nPrimitive three: The sparse cholesky\nOk. This one is gonna be a pain in the arse. But we need to do it. Why? Because we are going to need a JAX-traceable version further on down the track.\nThe issue here is that the non-zero pattern of the Cholesky decomposition is computed on the fly. This is absolutely not allowed in JAX. It must know the shape of all things at the moment it is called.\nThis is going to make for a somewhat shitty user experience for this function. It’s unavoidable with JAX designed7 the way it is.\nThe code in jax.experimental.sparse.bcoo.fromdense has this exact problem. In their case, they are turning a dense matrix into a sparse matrix and they can’t know until they see the dense matrix how many non-zeros there are. So they do the sensible thing and ask the user to specify it. They do this using the nse keyword parameter. If you’re curious what nse stands for, it turns out it’s not “non-standard evaluation” but rather “number of specified entries”. Most other systems use the abbreviation nnz for “number of non-zeros”, but I’m going to stick with the JAX notation.\nThe one little thing we need to add to this code is a guard to make sure that if the sparse_cholesky function is called without specifying\n\nsparse_cholesky_p = core.Primitive(\"sparse_cholesky\")\n\ndef sparse_cholesky(A_indices, A_indptr, A_x, *, L_nse: int = None):\n  \"\"\"A JAX traceable sparse cholesky decomposition\"\"\"\n  if L_nse is None:\n    err_string = \"You need to pass a value to L_nse when doing fancy sparse_cholesky.\"\n    _ = core.concrete_or_error(None, A_x, err_string)\n  return sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse = L_nse)\n\n@sparse_cholesky_p.def_impl\ndef sparse_cholesky_impl(A_indices, A_indptr, A_x, *, L_nse = None):\n  \"\"\"The implementation of the sparse cholesky This is not JAX traceable.\"\"\"\n  \n  L_indices, L_indptr= _symbolic_factor(A_indices, A_indptr)\n  if L_nse is not None:\n    assert len(L_indices) == nse\n    \n  L_x = _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)\n  L_x = _sparse_cholesky_impl(L_indices, L_indptr, L_x)\n  return L_indices, L_indptr, L_x\n\nThe rest of the code is just the sparse Cholesky code from blog 2 and I’ve hidden it under the fold. (You would think I would package this up properly, but I simply haven’t. Why not? Who knows8.)\n\n\nClick here to see the implementation\n\n\ndef _symbolic_factor(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] &gt; j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) &gt; 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\n\n\ndef _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\ndef _sparse_cholesky_impl(L_indices, L_indptr, L_x):\n  n = len(L_indptr) - 1\n  descendant = [[] for j in range(0, n)]\n  for j in range(0, n):\n    tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n    for bebe in descendant[j]:\n      k = bebe[0]\n      Ljk= L_x[bebe[1]]\n      pad = np.nonzero(                                                       \\\n          L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n      update_idx = np.nonzero(np.in1d(                                        \\\n                    L_indices[L_indptr[j]:L_indptr[j+1]],                     \\\n                    L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n      tmp[update_idx] = tmp[update_idx] -                                     \\\n                        Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n    diag = np.sqrt(tmp[0])\n    L_x[L_indptr[j]] = diag\n    L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n    for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n      descendant[L_indices[idx]].append((j, idx))\n  return L_x\n\n\nOnce again, we can check to see if this worked!\n\nL_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr))\nprint(f\"The error in the sparse cholesky is {np.sum(np.abs((A - L @ L.T).todense())): .2e}\")\n\nThe error in the sparse cholesky is  1.02e-13\n\n\nAnd, of course, we can do abstract evaluation. Here is where we actually need to use L_nse to work out the dimension of our output.\n\n@sparse_cholesky_p.def_abstract_eval\ndef sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, *, L_nse):\n  return core.ShapedArray((L_nse,), A_indices.dtype),                   \\\n         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             \\\n         core.ShapedArray((L_nse,), A_x.dtype)"
  },
  {
    "objectID": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#primitive-four-loga",
    "href": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#primitive-four-loga",
    "title": "Sparse Matrices 5: I bind you Nancy",
    "section": "Primitive four: \\(\\log(|A|)\\)",
    "text": "Primitive four: \\(\\log(|A|)\\)\nAnd now we have our final primitive: the log determinant! Wow. So much binding. For this one, we compute the Cholesky factorisation and note that \\[\\begin{align*}\n|A| = |LL^T| = |L||L^T| = |L|^2.\n\\end{align*}\\] If we successfully remember that the determinant of a triangular matrix is the product of its diagonal entries, we have a formula we can implement.\nSame deal as last time.\n\nsparse_log_det_p = core.Primitive(\"sparse_log_det\")\n\ndef sparse_log_det(A_indices, A_indptr, A_x):\n  \"\"\"A JAX traceable sparse log-determinant\"\"\"\n  return sparse_log_det_p.bind(A_indices, A_indptr, A_x)\n\n@sparse_log_det_p.def_impl\ndef sparse_log_det_impl(A_indices, A_indptr, A_x):\n  \"\"\"The implementation of the sparse log-determinant. This is not JAX traceable.\n  \"\"\"\n  L_indices, L_indptr, L_x = sparse_cholesky_impl(A_indices, A_indptr, A_x)\n  return 2.0 * sum(np.log(L_x[L_indptr[:-1]]))\n\nA canny reader may notice that I’m assuming that the first element in each column is the diagonal. This will be true as long as the diagonal elements of \\(L\\) are non-zero, which is true as long as \\(A\\) is symmetric positive definite.\nLet’s test9 it out.\n\nld = sparse_log_det(A_indices, A_indptr, A_x)\nLU = sparse.linalg.splu(A)\nld_true = sum(np.log(LU.U.diagonal()))\nprint(f\"The error in the log-determinant is {ld - ld_true: .2e}\")\n\nThe error in the log-determinant is  0.00e+00\n\n\nFinally, we can do the abstract evaluation.\n\n@sparse_log_det_p.def_abstract_eval\ndef sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):\n  return core.ShapedArray((1,), A_x.dtype)"
  },
  {
    "objectID": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#where-are-we-now-but-nowhere",
    "href": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#where-are-we-now-but-nowhere",
    "title": "Sparse Matrices 5: I bind you Nancy",
    "section": "Where are we now but nowhere?",
    "text": "Where are we now but nowhere?\nSo we are done for today. Our next step will be to implement all of the bits that are needed to make the derivatives work. So in the next instalment we will differentiate log-determinants, Cholesky decompositions, and all kinds of other fun things.\nIt should be a blast."
  },
  {
    "objectID": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#footnotes",
    "href": "posts/2022-05-18-sparse4-some-primatives/sparse4-some-primatives.html#footnotes",
    "title": "Sparse Matrices 5: I bind you Nancy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe second half of this post is half written but, to be honest, I want to go to bed more than I want to implement more derivatives, so I’m splitting the post.↩︎\naka JAX can map out how the pieces of the function go together and it can then use that map to make its weird transformations↩︎\nBut mostly because although I’m going to have to implement the Cholesky and triangular solves later on down the line, I’m writing this in order and I don’t wanna.↩︎\nThe JAX docs don’t use decorators for their bindings but I use decorators because I like decorators.↩︎\nSomething something duck type. They’re arrays with numbers in them that work in numpy and scipy. Get off my arse.↩︎\nThis is mostly for JIT, so it’s not necessary today, but to be very honest it’s the only easy thing to do here and I’m not above giving myself a participation trophy.↩︎\nThis is a … fringe problem in JAX-land, so it makes sense that there is a less than beautiful solution to the problem. I think this would be less of a design problem in Stan, where it’s possible to make the number of unknowns in the autodiff tree depend on int arrays is a complex way.↩︎\nWell, me. I’m who knows. I’m still treating this like scratch code in a notepad. Although we are moving towards the point where I’m going to have to set everything out properly. Maybe that’s the next post?↩︎\nFull disclosure: first time out I forgot to multiply by two. This is why we test.↩︎"
  },
  {
    "objectID": "posts/2022-11-12-robins-ritov/robins-ritov.html",
    "href": "posts/2022-11-12-robins-ritov/robins-ritov.html",
    "title": "On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for",
    "section": "",
    "text": "Paradoxes and counterexamples live in statistics as our morality plays and our ghost stories. They serve as the creepy gas station attendants that populate the roads leading to the curséd woods; existing not to force change on the adventurer, but to signpost potential danger.1\nAs a rule, we should also look in askance at attempts to resolve these paradoxes and counterexamples. That is not what they are for. They are community resources, objects of our collective culture, monuments to thwarted desire.\nBut sometimes, driven by the endless thirst for content, it’s worth diving down into a counterexample and resolving it. This quixotic quest is not to somehow patch a hole, but to rather expand the hole until it can comfortably encase our wants, needs, and prayers.\nTo that end, let’s gather ’round the campfire and attend the tale of The Bayesian and the Ancillary Coin.\nThis example2 was introduced by Robins and Ritov, and greatly popularised (and frequently reformulated) by Larry Wasserman3. It says4 this:\n\nA committed subjective Bayesian (one who cleaves to the likelihood priniciple tighter than Rose clings to that door) will sometimes get a very wrong answer under some simple, but realistic, forms of randomization. Only a less committed Bayesian will be able to skirt the danger.\n\nSo this is what we’re going to do now. First let’s introduce a version of the problem that does not trigger the counterexample. We then introduce the randomization scheme that leads to the error and talk about exactly how things go wrong. As someone who is particular skeptical of any claims to purity5, the next job is going to be deconstructing this idea of a committed6 subjective Bayesian. I will, perhaps unsurprisingly, argue that this is the only part of the Robins and Ritov (and Wasserman) conclusions that are somewhat questionable. In fact, a true committed subjective Bayesian7 can solve the problem. It’s just a matter of looking at it through the correct lens.\n\n\nThis example exists in a number of forms, that each add important corners to the problem, but in the interest of simplicity, we will start with a simple situation where no problems occur.\nAssume that there is a large, but fixed, finite number \\(J\\), and \\(J\\) unknown parameters \\(\\mu_j\\), \\(j=1,\\ldots, J\\). The large number \\(J\\) can be thought of as the number of strata in a population, while \\(\\mu_j\\) are the means of the corresponding stratum. Now construct an experiment where you draw \\[\ny_i \\mid \\mu,x = j \\sim N(\\mu_j, 1).\n\\] To close out the generative model, we assume that the covariates have a known distribution \\(x_i \\sim \\text{Unif}\\{1,\\ldots, p\\}\\).\nA classical problem in mathematical statistics is to construct a \\(\\sqrt{n}\\)-consistent8 estimator \\(\\hat\\mu_n\\) of the vector \\(\\mu\\). But in the setting of this problem, this is quite difficult. The challenge is that if \\(J\\) is a very large number, then we would need a gargantuan9 number of observations (\\(n \\gg J\\)) in order to resolve all of the parameters properly.\nBut there is a saving grace! The population10 average \\[\n\\mu = \\mathbb{E}(y) = \\sum_{j=1}^J \\mu_j \\Pr(x = j)= \\frac{1}{J}\\sum_{j=1}^J \\mu_j\n\\] can be estimated fairly easily. In fact, the sample mean (aka the most obvious estimator) \\(\\bar{y} = n^{-1} \\sum_{i=1}^n y_i\\) is going to be \\(\\sqrt{n}\\)-consistent.\nSimilarly, if we were to construct a Bayesian estimate of the population mean based off the prior \\(\\mu_j \\mid m \\sim N(m, 1)\\) and \\(m \\sim N(0,\\tau^2)\\), then the posterior estimate of the population mean is, for large enough11 \\(n\\), \\[\n\\hat \\mu_{\\text{Bayes},n}= \\mathbb{E}(\\mu \\mid y) \\approx \\frac{1}{n + 2/\\tau} \\sum_{i=1}^n y_i.\n\\] This means that the12 Bayesian resolution of this problem is roughly the same as the classical resolution. This is a nice thing. For very simple problems, these estimators should be fairly similar. It’s only when shit gets complicated where things become subtle.\nThis scenario, where a model is parameterized by an extremely high dimensional parameter \\(\\mu\\) but the quantity of inferential inference is a low-dimensional summary of \\(\\mu\\), is widely and deeply studied under the name of semi-parametric statistics.\nSemi-parametric statistics is, unsurprisingly, harder than parametric statistics, but it also quite a bit more challenging than non-parametric statistics. The reason is that if we want to guarantee a good estimate of a particular finite dimensional summary, it turns out that it’s not enough to generically get a “good” estimate of the high-dimensional parameter. In fact, getting a good estimate of the high-dimensional parameter is often not possible (see the example we just considered).\nInstead understanding semi-parametric models becomes the fine art of understanding what needs to be done well and what we can half arse. A description of this would take us well outside the scope of a mere blog post, but if you want to learn more about the topic, that’s what to google.\n\n\n\nIn order to destroy all that is right and good about the previous example, we only need to do one thing: randomize in a nefarious way. Robins and Ritov (actually, Wasserman who proposed the case with a finite \\(J\\)) add to their experiment \\(J\\) biased coins \\(r_j\\) with the property that \\[\n\\Pr(r_j = 1 \\mid X=j) = \\xi_j,\n\\] for some known \\(0 &lt; \\delta \\leq \\xi_j &lt; 1-\\delta\\), \\(j=1,\\ldots, J\\) and some \\(c&gt;0\\).\nThey then go through the data and add a column \\(r_i \\sim \\text{Bernouili}(\\xi_{x_i})\\). The new data is now a three dimensional vector \\((y_i, x_i, r_i)\\). It’s important to this problem that the \\(\\xi_j\\) are known and that we have the conditional independence structure \\(y \\perp r \\mid x\\).\nRobins, Ritov, and Wasserman all ask the same question: Can we still estimate the population mean if we only observe samples from the conditional distribution \\((y_i, x_i) \\sim p(x,y \\mid r=1)\\)?\nThe answer is going to turn out that there is a perfectly good estimator from classical survey statistics, but a Bayesian estimator is a bit more challenging to find.\nBefore we get there, it’s worth noting that unlike the problem in the previous section, this problem is at least a little bit interesting. It’s a cartoon of a very common situation where there is covariate-dependent randomization in a clinical trial. Or, maybe even more cleanly, a cartoon of a simple probability survey.\nA critical feature of this problem is that because the \\(\\xi_j\\) are known and \\(p(x)\\) is known, the joint likelihood factors as \\[\np(y,x,r \\mid \\mu) = p(x)p(r\\mid x) p(y \\mid x, \\mu) = p(r , x) p(y \\mid x, \\mu),\n\\] so \\(r\\) is ancillary13 for \\(\\mu\\).\nThe simplest classical estimator for \\(\\mathbb{E}(y)\\) is the Horvitz-Thompson estimator \\[\n\\bar{y}_\\text{HT} = \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i}{\\xi_{x_i}}.\n\\] It’s easy to show that this is a \\(\\sqrt{n}\\)-consistent estimator. Better yet, uniform over \\(\\mu\\) in the sense that the convergence of the estimator isn’t affected (to leading order) by the specific \\(\\mu_j\\) values. This uniformity is quite useful as it gives some hope of good finite-data behaviour.\nSo now that we know that the problem can be solved, let’s see if we can solve it in a Bayesian way. Robins and Ritov gave the following result.\n\nThere is no uniformly consistent Baysesian estimator of the parameter \\(\\mu\\) unless the prior depends on the \\(\\xi_j\\) values.\n\nRobins and Ritov argue that a “committed subjective Bayesian” would, by the Likelihood Principle, never allow their prior to depend on the ancillary statistic \\(\\xi\\) as the Likelihood Principle clearly states that inference should be independent on ancillary information.\nThere are, of course, ways to construct priors that depend on the sampling probabilities. Wasserman calls this “frequentist chasing”\nSo let’s investigate this, by talking about what went wrong, how to fix it, and whether fixing it makes us bad Bayesians."
  },
  {
    "objectID": "posts/2022-11-12-robins-ritov/robins-ritov.html#a-counterexample-always-proceedes-from-the-least-interesting-premise",
    "href": "posts/2022-11-12-robins-ritov/robins-ritov.html#a-counterexample-always-proceedes-from-the-least-interesting-premise",
    "title": "On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for",
    "section": "",
    "text": "This example exists in a number of forms, that each add important corners to the problem, but in the interest of simplicity, we will start with a simple situation where no problems occur.\nAssume that there is a large, but fixed, finite number \\(J\\), and \\(J\\) unknown parameters \\(\\mu_j\\), \\(j=1,\\ldots, J\\). The large number \\(J\\) can be thought of as the number of strata in a population, while \\(\\mu_j\\) are the means of the corresponding stratum. Now construct an experiment where you draw \\[\ny_i \\mid \\mu,x = j \\sim N(\\mu_j, 1).\n\\] To close out the generative model, we assume that the covariates have a known distribution \\(x_i \\sim \\text{Unif}\\{1,\\ldots, p\\}\\).\nA classical problem in mathematical statistics is to construct a \\(\\sqrt{n}\\)-consistent8 estimator \\(\\hat\\mu_n\\) of the vector \\(\\mu\\). But in the setting of this problem, this is quite difficult. The challenge is that if \\(J\\) is a very large number, then we would need a gargantuan9 number of observations (\\(n \\gg J\\)) in order to resolve all of the parameters properly.\nBut there is a saving grace! The population10 average \\[\n\\mu = \\mathbb{E}(y) = \\sum_{j=1}^J \\mu_j \\Pr(x = j)= \\frac{1}{J}\\sum_{j=1}^J \\mu_j\n\\] can be estimated fairly easily. In fact, the sample mean (aka the most obvious estimator) \\(\\bar{y} = n^{-1} \\sum_{i=1}^n y_i\\) is going to be \\(\\sqrt{n}\\)-consistent.\nSimilarly, if we were to construct a Bayesian estimate of the population mean based off the prior \\(\\mu_j \\mid m \\sim N(m, 1)\\) and \\(m \\sim N(0,\\tau^2)\\), then the posterior estimate of the population mean is, for large enough11 \\(n\\), \\[\n\\hat \\mu_{\\text{Bayes},n}= \\mathbb{E}(\\mu \\mid y) \\approx \\frac{1}{n + 2/\\tau} \\sum_{i=1}^n y_i.\n\\] This means that the12 Bayesian resolution of this problem is roughly the same as the classical resolution. This is a nice thing. For very simple problems, these estimators should be fairly similar. It’s only when shit gets complicated where things become subtle.\nThis scenario, where a model is parameterized by an extremely high dimensional parameter \\(\\mu\\) but the quantity of inferential inference is a low-dimensional summary of \\(\\mu\\), is widely and deeply studied under the name of semi-parametric statistics.\nSemi-parametric statistics is, unsurprisingly, harder than parametric statistics, but it also quite a bit more challenging than non-parametric statistics. The reason is that if we want to guarantee a good estimate of a particular finite dimensional summary, it turns out that it’s not enough to generically get a “good” estimate of the high-dimensional parameter. In fact, getting a good estimate of the high-dimensional parameter is often not possible (see the example we just considered).\nInstead understanding semi-parametric models becomes the fine art of understanding what needs to be done well and what we can half arse. A description of this would take us well outside the scope of a mere blog post, but if you want to learn more about the topic, that’s what to google."
  },
  {
    "objectID": "posts/2022-11-12-robins-ritov/robins-ritov.html#robins-and-ritov-toss-an-ancillary-coin-and-let-slip-the-dogs-of-war",
    "href": "posts/2022-11-12-robins-ritov/robins-ritov.html#robins-and-ritov-toss-an-ancillary-coin-and-let-slip-the-dogs-of-war",
    "title": "On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for",
    "section": "",
    "text": "In order to destroy all that is right and good about the previous example, we only need to do one thing: randomize in a nefarious way. Robins and Ritov (actually, Wasserman who proposed the case with a finite \\(J\\)) add to their experiment \\(J\\) biased coins \\(r_j\\) with the property that \\[\n\\Pr(r_j = 1 \\mid X=j) = \\xi_j,\n\\] for some known \\(0 &lt; \\delta \\leq \\xi_j &lt; 1-\\delta\\), \\(j=1,\\ldots, J\\) and some \\(c&gt;0\\).\nThey then go through the data and add a column \\(r_i \\sim \\text{Bernouili}(\\xi_{x_i})\\). The new data is now a three dimensional vector \\((y_i, x_i, r_i)\\). It’s important to this problem that the \\(\\xi_j\\) are known and that we have the conditional independence structure \\(y \\perp r \\mid x\\).\nRobins, Ritov, and Wasserman all ask the same question: Can we still estimate the population mean if we only observe samples from the conditional distribution \\((y_i, x_i) \\sim p(x,y \\mid r=1)\\)?\nThe answer is going to turn out that there is a perfectly good estimator from classical survey statistics, but a Bayesian estimator is a bit more challenging to find.\nBefore we get there, it’s worth noting that unlike the problem in the previous section, this problem is at least a little bit interesting. It’s a cartoon of a very common situation where there is covariate-dependent randomization in a clinical trial. Or, maybe even more cleanly, a cartoon of a simple probability survey.\nA critical feature of this problem is that because the \\(\\xi_j\\) are known and \\(p(x)\\) is known, the joint likelihood factors as \\[\np(y,x,r \\mid \\mu) = p(x)p(r\\mid x) p(y \\mid x, \\mu) = p(r , x) p(y \\mid x, \\mu),\n\\] so \\(r\\) is ancillary13 for \\(\\mu\\).\nThe simplest classical estimator for \\(\\mathbb{E}(y)\\) is the Horvitz-Thompson estimator \\[\n\\bar{y}_\\text{HT} = \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i}{\\xi_{x_i}}.\n\\] It’s easy to show that this is a \\(\\sqrt{n}\\)-consistent estimator. Better yet, uniform over \\(\\mu\\) in the sense that the convergence of the estimator isn’t affected (to leading order) by the specific \\(\\mu_j\\) values. This uniformity is quite useful as it gives some hope of good finite-data behaviour.\nSo now that we know that the problem can be solved, let’s see if we can solve it in a Bayesian way. Robins and Ritov gave the following result.\n\nThere is no uniformly consistent Baysesian estimator of the parameter \\(\\mu\\) unless the prior depends on the \\(\\xi_j\\) values.\n\nRobins and Ritov argue that a “committed subjective Bayesian” would, by the Likelihood Principle, never allow their prior to depend on the ancillary statistic \\(\\xi\\) as the Likelihood Principle clearly states that inference should be independent on ancillary information.\nThere are, of course, ways to construct priors that depend on the sampling probabilities. Wasserman calls this “frequentist chasing”\nSo let’s investigate this, by talking about what went wrong, how to fix it, and whether fixing it makes us bad Bayesians."
  },
  {
    "objectID": "posts/2022-11-12-robins-ritov/robins-ritov.html#a-simple-posterior-and-its-post-processing",
    "href": "posts/2022-11-12-robins-ritov/robins-ritov.html#a-simple-posterior-and-its-post-processing",
    "title": "On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for",
    "section": "A simple posterior and its post-processing",
    "text": "A simple posterior and its post-processing\nOnce again, recall the setting: we are observing the triple23 \\[\nz_i = (x_i,r_i,y_i) = (x_i, r_i, \\texttt{r[i]==1? y[i]: NA}).\n\\] In particular, we can process this data to get some quantities:\n\n\\(N\\): The total sample size\n\\(n= \\sum_{i=1}^N r_i\\): The number of observed \\(y\\)\n\\(N_j = \\sum_{i=1}^N 1_{x_i = j}\\): The total number of times group \\(j\\) was sampled\n\\(n_j = \\sum_{i=1}^N r_i1_{x_i = j}\\): The number of times an observation from group \\(j\\) was recorded.\n\nBecause of the structure of the problem, most observed values of \\(N_j\\) and \\(n_j\\) will be zero or one.\nNevertheless, we persist.\nWe now need priors on the \\(\\mu_j\\). There are probably a tonne of options here, but I’m going to go with the simplest one, which is just to make them iid \\(N(0, \\tau^2)\\) for some fixed and known value \\(\\tau\\). We can then fit the resulting model and get the posterior for each \\(\\mu_j\\). Note that because of the data sparsity, most of the posteriors will just be the same as the prior.\nThen we can ask ourselves a much more Bayesian question: What would the average in our sample have been if we had recorded every \\(y_i\\)? Our best estimate of that quantity is \\[\n\\frac{1}{N}\\sum_{j=1}^J N_j \\mu_j\n\\]\nThat’s all well and good. And, again, if I had small enough \\(J\\) or large enough \\(N\\) that I had a good estimate for all of the \\(\\mu_j\\), this would be a good estimate. Moreover, for finite data this is likely to be a much better estimator than \\(J^{-1}\\sum_{j=1}^J \\mu_j\\) as it at least partially corrects for any potential imbalance in the covariate sampling.\nIt’s also worth noting here that there is nothing “Bayesian” about this. I am simply taking the knowledge I have from the sample I observed and processing the posterior to compute a quantity that I am interested in.\nBut, of course, that isn’t actually the quantity that I’m interested in. I’m interested in that quantity averaged over realisations of \\(r\\). We can compute this if we can quantify the effect that \\(n_j\\) has on \\(\\mu_j\\).\nWe can do this pretty easily. Our priors are iid24, so this decouples into \\(J\\) independent normal-normal models.\nFor any \\(j\\), denote \\(y^{(j)}\\) as the subset of \\(y\\) that are in category \\(j\\). We have that25 \\[\\begin{align*}\np(\\mu_j \\mid y) &\\propto \\exp\\left(-\\frac{1}{2}\\sum_{i=1}^{n_j}(y^{(j)}_i - \\mu_j)^2 - \\frac{1}{2\\tau^2}\\mu_j^2\\right)\\\\\n&\\propto \\exp\\left[-\\frac{1}{2}\\left(\\frac{1}{\\tau} + n_j\\right)\\mu_j^2 + \\mu_j\\sum_{i=1}^{n_j}y_i^{(j)}\\right].\n\\end{align*}\\]\nIf we expand the density for a \\(\\mu_j \\mid y \\sim N(m,v^2)\\) we get \\[\np(\\mu_j \\mid y) \\propto \\exp\\left(-\\frac{1}{2v^2}\\mu_j^2 + \\frac{1}{v^2}m\\mu_j\\right).\n\\] Matching terms in these two expressions we get that \\[\nv_j^\\text{post} = \\operatorname{Var}(\\mu_j \\mid y, n_j) =  \\frac{1}{n_j + \\tau^{-2}},\n\\] while the posterior mean is \\[\nm_j^\\text{post} = \\mathbb{E}(\\mu_j \\mid y, n_j) = \\frac{1}{n_j + \\tau^{-2}}\\sum_{i=1}^{n_j}y_i^{(j)},\n\\] where I’ve suppressed the dependence on the sample \\(y\\) in the \\(m_j\\) and \\(v_j\\) notation because, as a true26 Bayesian, my sample is fixed and known. Hence \\[\n\\mu_j \\mid y \\sim N(m_j^{\\text{post}}, v_j^{\\text{post}}).\n\\]\nThen I get the following estimator for the mean of the complete sample \\[\n\\mathbb{E}\\left(\\frac{1}{N}\\sum_{j=1}^JN_j\\mu_j \\mid y \\right)= \\frac{1}{N}\\sum_{j=1}^JN_jm_j^\\text{post}.\n\\] We can also compute the posterior variance27 \\[\n\\operatorname{Var}\\left(\\frac{1}{N}\\sum_{j=1}^JN_j\\mu_j \\mid y \\right)=\\sum_{j=1}^J\\frac{N_j^2}{N^2}v_j^\\text{post}.\n\\] Note that most of the groups won’t have a corresponding observation, so, recalling that \\(A_r\\) is the set of \\(j\\)s that have been updated in the sample, we get \\[\n\\operatorname{Var}\\left(\\frac{1}{N}\\sum_{j=1}^JN_j\\mu_j \\mid y \\right)=\\sum_{j\\in A_r}\\frac{N_j^2}{N^2}v_j^\\text{post} + \\tau^2\\sum_{j \\not \\in A_r}\\frac{N_j^2}{N^2},\n\\] where the term that multiplies \\(\\tau^2\\) is less than 1.\nSo that’s all well and good, but that isn’t really the thing we were trying to estimate. We are actually interested in estimating the population mean, which we will get if we let \\(N\\rightarrow \\infty\\).\nSo let’s see if we can do this without violating any of the universally agreed upon sacred strictures of Bayes."
  },
  {
    "objectID": "posts/2022-11-12-robins-ritov/robins-ritov.html#modelling-the-effect-of-the-ancillary-coin",
    "href": "posts/2022-11-12-robins-ritov/robins-ritov.html#modelling-the-effect-of-the-ancillary-coin",
    "title": "On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for",
    "section": "Modelling the effect of the ancillary coin",
    "text": "Modelling the effect of the ancillary coin\nHere’s the thing, though. We have computed our posterior distributions \\(p(\\mu_j \\mid y)\\) and we can now use them as a generative model28 for our data. We also have the composition of the complete data set (the \\(N_j\\)s) and full knowledge about how a new sample of the \\(n_j\\)s would come into our world.\nWe can put these things together! And that’s not in anyway violating our Bayesian oaths! We are simply using our totally legally obtained posterior distribution to compute things. We are still true committed29 subjective Bayesians.\nSo we are going to ask ourselves a simple question. Imagine, for a given \\(N_j\\), we have \\(n_j \\sim \\text{Binom}(N_j, \\xi_j)\\) iid samples30 \\[\n\\tilde{y}^{(j)}_i \\sim N(m_j^\\text{post}, v_j^\\text{post} + 1).\n\\] What is the posterior mean \\(\\mathbb{E}(\\mu_j \\mid \\tilde{y}^{(j)}, N_j)\\)? In fact, because this is random data drawn from a hypothetical sample, we can (and should31) ask questions about its distribution! To be brutally francis with you, I am too lazy to work out the variance of the posterior mean. So I’m just going to look at the mean of the posterior mean.\nFirst things first, we need to look at the (average) posterior for \\(\\mu_j\\) when \\(n_j = n\\). The exact calculation we did before gives us \\[\nm_j(n) = \\left(1-\\frac{1}{\\tau^2n + 1}\\right) m_j^\\text{post}.\n\\] And, while I said I wasn’t going to focus on the variance, it’s easy enough to write down as \\[\nv_j(n) = \\frac{1}{n + \\tau^{-2}} + \\left(1 - \\frac{1}{\\tau^2n + 1}\\right)(1 + v^\\text{post}_j),\n\\] where the second term takes into account the variance due to the imputation.\nWith this, we can estimate sample mean for any number \\(\\tilde N\\) and any set of \\(\\tilde N_j\\) that sum to \\(\\tilde N\\) and any set of \\(\\tilde n_j \\sim \\text{Binom}(\\tilde N_j, \\xi_j)\\) as \\[\\begin{align*}\n\\frac{1}{\\tilde N}\\sum_{j=1}^J \\tilde N_j m_j(n_j) &= \\frac{1}{\\tilde N}\\sum_{j=1}^J \\frac{\\tilde N_j}{\\tilde n_j} \\tilde n_j \\tilde m_j(n_j) \\\\\n&= \\frac{1}{\\tilde N}\\sum_{j=1}^J \\frac{1}{\\xi_j} \\tilde n_j m_j^\\text{post} + o(1),\n\\end{align*}\\] where in the last line I’ve used the fact that the empirical proportion converges to \\(\\xi_j\\) and the posterior mean converges to \\(m_j^\\text{post}\\). The little-o32 error term is as \\(\\tilde N\\) (and hence \\(\\tilde N_j\\) and \\(\\tilde n_j\\)) goes to infinity.\nTo turn this into a practical estimate, we can plug in our values of \\(n_j\\) and \\(N\\) to get our Bayesian approximation to the population mean \\[\\begin{align*}\n\\hat \\mu &= \\frac{1}{N}\\sum_{j=1}^J \\frac{n_j}{\\xi_j}m_j^{\\text{post}} \\\\\n&=\\frac{1}{N} \\sum_{j \\in A_r} \\frac{n_j}{\\xi_j}m_j^\\text{post} \\\\\n&=\\frac{1}{N}\\sum_{j=1}^J\\sum_{i=1}^{n_j} \\frac{1}{\\xi_j}\\left(1 - \\frac{\\tau^{-2}}{n_j}\\right)y_i^{(j)},\n\\end{align*}\\] which is (up to the small term in brackets) the Horvitz-Thompson estimator!"
  },
  {
    "objectID": "posts/2022-11-12-robins-ritov/robins-ritov.html#is-it-bayesian",
    "href": "posts/2022-11-12-robins-ritov/robins-ritov.html#is-it-bayesian",
    "title": "On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for",
    "section": "Is it Bayesian?",
    "text": "Is it Bayesian?\nI stress, again, that there is nothing inherently non-Bayesian about this derivation. Except possibly the question that it is asking. What I did was compute the posterior distribution and then I took it seriously and used it to compute a quantity of interest.\nThe only oddity is that the quantity of interest (the population mean) has a slightly awkward link to the observed sample. Hence, I estimated something that had a more direct link to the population mean: the sample mean of the completely observed sample under different realisations of the randomisation \\(r_i\\).\nIn order to estimate the sample mean under different realisations of the randomisation, I needed to use the posterior predictive distribution to impute these fictional samples. I then averaged over the imputed samples and sent the sample size to infinity to get an estimator33.\nOr, to put it differently, I used Bayes to get a posterior estimate for new data \\[\np(\\tilde y, \\tilde r, \\tilde x) = \\int_{\\mathbb{R}^J}p(\\tilde y \\mid \\tilde x, \\mu)\\,d\\mu p(\\tilde r \\mid \\tilde x) p(\\tilde x)\n\\] and then used this probabilistic model to estimate \\(\\mathbb{E}(\\tilde y)\\). There was no reason to use Bayesian methods to do this. Non-Bayesian questions do not invite Bayesian answers.\nNow, would I go to all of this effort in real life? Probably not. And in the applications that I’ve come across, I’ve never had to. I’ve done a bunch of MRP34, which is structurally quite similar to this problem except we can reasonably model the dependence structure between the \\(\\mu_j\\)s. This paper I wrote with Alex Gao, Lauren Kennedy, and Andrew Gelman is an example of the type of modelling you can do."
  },
  {
    "objectID": "posts/2022-11-12-robins-ritov/robins-ritov.html#footnotes",
    "href": "posts/2022-11-12-robins-ritov/robins-ritov.html#footnotes",
    "title": "On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHuge thanks to Sameer Deshpande for great comments!↩︎\nI first came across this in a series of posts on Larry Wasserman’s now defunct but quite excellent blog.↩︎\nIt’s worth saying that these three people do fabulous statistics of the form that I don’t usually do. But that doesn’t make it less important to understand their contributions. You could say that while I am not a Lazbian, I think it’s important to know the theory.↩︎\nI might have slightly reworded it.↩︎\nPurity is needed in good olive oil and that’s it↩︎\nA committed subjective Bayesian prefers Dutch baby to a Dutch book.↩︎\nA true committed subjective Bayesian doesn’t wear anything under his kilt.↩︎\nThat is, an estimator where \\(\\Pr(|\\hat \\mu_n - \\mu| &gt; \\sqrt{n}\\epsilon) \\rightarrow 0\\) for all \\(\\epsilon&gt;0\\). This, roughly, means, that you can find a \\(C\\) such that \\(\\mu \\in [ \\hat \\mu_n - C\\sqrt{n}, \\hat \\mu_n + C\\sqrt{n}]\\) with high probability.↩︎\nThe asymptotics say that we should count our data in multiples of \\(J\\), so we’d \\(n &gt; 100J\\) to get even one decimal place of accuracy.↩︎\nRemember \\(\\mu_j = \\mathbb{E}(y \\mid x=j)\\).↩︎\nTheorem 2 of Harmeling and Toussaint↩︎\na↩︎\nIf you’ve not come across it, ancillary is the term used for parts of the data that don’t influence parameter estimates. It’s the opposite of a sufficient statistic. One way to see that it’s ancillary for any model \\(p(y\\mid x, \\theta)\\), is to consider the log of the joint density \\[\n\\log(p(x,y,r \\mid \\theta)) = \\log p(y\\mid x, \\theta) + \\log p(r \\mid x) + \\log p(x)\n\\], where the last two terms are constant in \\(\\theta\\).↩︎\nYou need to be specific here. Obviously this would be false if you were trying to do a statistical prediction. Or if you were trying to make a decision. Those things necessarily depend on extra stuff!↩︎\nThis is a lie. Insisting on talking about this shit rather than actually making Bayes useful and using it in new and exciting ways to do things that are hard to do without Bayesian methods is a waste of time. Worse than that, when you start pretending your method of choice is the only possible thing that a sensible and principled person would use, you start to look like a bit of a dickhead. It also turns people off trying these very flexible and useful methods. So yeah. I maybe do care a little bit. ↩︎\nThe expected number of samples to see one draw where \\(x_i =j\\) is \\(J\\). The expected number of draws where \\(x_i = j\\) that you need to actually observe the corresponding \\(y_i\\) is \\(\\xi_j^{-1}\\). This suggests it will potentially take a lot of draws to even have effectively one sample from each category, let alone the 20-100 you’d need to, practically, get some sort of reasonable estimate.↩︎\nRobins and Ritov have always been open that if there is a true parametric model for the \\(\\mathbb{E}(Y \\mid x = j)\\) (or if that function is “very smooth” in some technical sense, eg a realisation of a smooth Gaussian process) then the Bayesian estimator that incorporates this information will do perfectly well. ↩︎\nSo the RR example uses binary data, so then it’s the correlation between \\(\\mathbb{E}(y \\mid x=j)\\) and \\(\\xi_j\\), but the exact same argument works if \\(\\xi_j\\) is correlated something like \\(\\operatorname{expit}(\\mu_j)\\). I went with the Gaussian version because at one point I thought I might end up having to derive posteriors and I’m all about simplicity.↩︎\nexpit is the inverse of the logit transform↩︎\nCheck the paper for the details as the situation is slightly different to the one I’m sketching out here, but there’s no real substantive difference.↩︎\nOf course, if this were true we could use a GP prior for the \\(\\mu_j\\)s and we’d probably get a decent estimator anyway.↩︎\nIf you want to interfere, there are plenty of ways to build priors that incorporate the \\(\\xi_j\\) information. The Ritov etc paper has nice references to the various things that sprung up from this example. Are these useful beyond simply making sure the posterior mean of \\(\\mu\\) estimates \\(\\mathbb{E}(y)\\)? Not really. They are priors designed to solve exactly one problem.↩︎\nI’m using the C/C++ ternary operator. In R this would be parsed as ifelse(r[i] == 1, y[i], NA). ↩︎\nNot exchangeable—there are no shared parameters!↩︎\nRemember that \\(y \\mid x = j \\sim N(\\mu_j, 1)\\). If we wanted a more flexible variance, we could obviously have one, but it makes not real difference to anything.↩︎\nI promise I’m just rolling my eyes to see if I can see my brain.↩︎\nRemember everything is independent!↩︎\nThis is the posterior predictive distribution!↩︎\nA true committed subjective Bayesian knows that DP stands for Dirichlet Process. No matter the context.↩︎\nThe variance is \\(v_j^\\text{post} + 1\\) because this is the posterior predictive distribution.↩︎\nDoes this seem like a frequentist question? I guess. But really it’s a question we can always ask about the posterior. Should we? Well if you are trying to estimate a population quantity you sort of have to. Because there isn’t really a concept of a population parameter within a Bayesian framework (true committed subjective or otherwise).↩︎\nRemember that this means that the error (which is a random variable) goes to 0 as \\(n\\rightarrow \\infty\\). A more careful person could probably work out how fast it would happen.↩︎\nI only computed the mean, so feel free to pretend that I’m minimizing a loss function↩︎\nMultilevel regression with poststratification, a survey modelling technique↩︎\nNo true Scotsman etc↩︎\nor meta-Bayesian in the event that we are doing things like building a Bayesian pseudo-model of on the space of all considered model that just happens to give every model equal probability because Harold Fucking Jeffreys gave you an erection and you could either process that event like an adult or build a whole personality around it. And you chose the latter.↩︎\nCan you tell that I hate this entire discussion?↩︎"
  },
  {
    "objectID": "posts/2022-11-27-sparse7/sparse7.html",
    "href": "posts/2022-11-27-sparse7/sparse7.html",
    "title": "Sparse matrices part 7a: Another shot at JAX-ing the Cholesky decomposition",
    "section": "",
    "text": "The time has come once more to resume my journey into sparse matrices. There’s been a bit of a pause, mostly because I realised that I didn’t know how to implement the sparse Cholesky factorisation in a JAX-traceable way. But now the time has come. It is time for me to get on top of JAX’s weird control-flow constructs.\nAnd, along the way, I’m going to re-do the sparse Cholesky factorisation to make it, well, better.\nIn order to temper expectations, I will tell you that this post does not do the numerical factorisation, only the symbolic one. Why? Well I wrote most of it on a long-haul flight and I didn’t get to the numerical part. And this was long enough. So hold your breaths for Part 7b, which will come as soon as I write it.\nYou can consider this a much better re-do of Part 2. This is no longer my first python coding exercise in a decade, so hopefully the code is better. And I’m definitely trying a lot harder to think about the limitations of JAX.\nBefore I start, I should probably say why I’m doing this. JAX is a truly magical thing that will compute gradients and every thing else just by clever processing of the Jacobian-vector product code. Unfortunately, this is only possible if the Jacobian-vector product code is JAX traceable and this code is structurally extremely similar1 to the code for the sparse Cholesky factorisation.\nI am doing this in the hope of (eventually getting to) autodiff. But that won’t be this blog post. This blog post is complicated enough."
  },
  {
    "objectID": "posts/2022-11-27-sparse7/sparse7.html#control-flow-of-the-damned",
    "href": "posts/2022-11-27-sparse7/sparse7.html#control-flow-of-the-damned",
    "title": "Sparse matrices part 7a: Another shot at JAX-ing the Cholesky decomposition",
    "section": "Control flow of the damned",
    "text": "Control flow of the damned\nThe first an most important rule of programming with JAX is that loops will break your heart. I mean, whatever, I guess they’re fine. But there’s a problem. Imagine the following function\n\ndef f(x: jax.Array, n: Int) -&gt; jax.Array:\n  out = jnp.zeros_like(x)\n  for j in range(n):\n    out = out + x\n  return out\n\nThis is, basically, the worst implementation of multiplication by an integer that you can possibly imagine. This code will run fine in Python, but if you try to JIT compile it, JAX is gonna get angry. It will produce the machine code equivalent of\n\ndef f_n(x):\n  out = x\n  out = out + x\n  out = out + x\n  // do this n times\n  return out\n\nThere are two bad things happening here. First, note that the “compiled” code depends on n and will have to be compiled anew each time n changes. Secondly, the loop has been replaced by n copies of the loop body. This is called loop unrolling and, when used judiciously by a clever compiler, is a great way to speed up code. When done completely for every loop this is a nightmare and the corresponding code will take a geological amount of time to compile.\nA similar thing2 happens when you need to run autodiff on f(x,n). For each n an expression graph is constructed that contains the unrolled for loop. This suggests that autodiff might also end up being quite slow (or, more problematically, more memory-hungry).\nSo the first rule of JAX is to avoid for loops. But if you can’t do that, there are three built-in loop structures that play nicely with JIT compilation and sometimes3 differentiation. These three constructs are\n\nA while loop jax.lax.while(cond_func, body_func, init)\nAn accumulator jax.lax.scan(body_func, init, xs)\nA for loop jax.lax.fori_loop(lower, upper, body_fun, init)\n\nOf those three, the first and third work mostly as you’d expect, while the second is a bit more hairy. The while function is roughly equivalent to\n\n`\ndef jax_lax_while_loop(cond_func, body_func, init):\n  x  = init\n  while cond_func(x):\n    x = body_func(x)\n  return x\n\nSo basically it’s just a while loop. The thing that’s important is that it compiles down to a single XLA operation4 instead of some unrolled mess.\nOne thing that is important to realise is that while loops are only forwards-mode differentiable, which means that it is very expensive5 to compute gradients. The reason for this is that we simply do not know how long that loop actually is and so it’s impossible to build a fixed-size expression graph.\nThe jax.lax.scan function is probably the one that people will be least familiar with. That said, it’s also the one that is roughly “how a for loop should work”. The concept that’s important here is a for-loop with carry over. Carry over is information that changes from one step of the loop to the next. This is what separates us from a map statement, which would apply the same function independently to each element of a list.\nThe scan function looks like\n\ndef jax_lax_scan(body_func, init, xs):\n  len_x0 = len(x0)\n  if not all(len(x) == len_x0 for x in xs):\n    raise ValueError(\"All x must have the same length!!\")\n  carry = init\n  ys = []\n  for x in xs:\n    carry, y = body_func(carry, x)\n    ys.append(y)\n  \n  return carry, np.stack(ys)\n\nA critically important limitation to jax.lax.scan is that is that every x in xs must have the same shape! This mean, for example, that\n\nxs = [[1], [2,3], [4], 5,6,7]\n\nis not a valid argument. Like all limitations in JAX, this serves to make the code transformable into efficiently compiled code across various different processors.\nFor example, if I wanted to use jax.lax.scan on my example from before I would get\n\nfrom jax import lax\nfrom jax import numpy as jnp\n\ndef f(x, n):\n  init = jnp.zeros_like(x)\n  xs = jnp.repeat(x, n)\n  def body_func(carry, y):\n    val = carry + y\n    return (val, val)\n  \n  final, journey = lax.scan(body_func, init, xs)\n  return (final, journey)\n\nfinal, journey = f(1.2, 7)\nprint(final)\nprint(journey)\n\n8.4\n[1.2       2.4       3.6000001 4.8       6.        7.2       8.4      ]\n\n\nThis translation is a bit awkward compared to the for loop but it’s the sort of thing that you get used to.\nThis function can be differentiated6 and compiled. To differentiate it, I need a version that returns a scalar, which is easy enough to do with a lambda.\n\nfrom jax import jit, grad\n\nf2 = lambda x, n: f(x,n)[0]\nf2_grad = grad(f2, argnums = 0)\n\nprint(f2_grad(1.2, 7))\n\n7.0\n\n\nThe argnums option tells JAX that we are only differentiating wrt the first argument.\nJIT compilation is a tiny bit more delicate. If we try the natural thing, we are going to get an error.\n\nf_jit_bad = jit(f)\nbad = f_jit_bad(1.2, 7)\n\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced&lt;ShapedArray(int32[], weak_type=True)&gt;with&lt;DynamicJaxprTrace(level=0/1)&gt;\nWhen jit-compiling jnp.repeat, the total number of repeats must be static. To fix this, either specify a static value for `repeats`, or pass a static value to `total_repeat_length`.\nThe error occurred while tracing the function f at /var/folders/08/4p5p665j4d966tr7nvr0v24c0000gn/T/ipykernel_24749/3851190413.py:4 for jit. This concrete value was not available in Python because it depends on the value of the argument 'n'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n\n\nIn order to compile a function, JAX needs to know how big everything is. And right now it does not know what n is. This shows itself through the ConcretizationTypeError, which basically says that as JAX was looking through your code it found something it can’t manipulate. In this case, it was in the jnp.repeat function.\nWe can fix this problem by declaring this parameter static.\n\nf_jit = jit(f, static_argnums=(1,))\nprint(f_jit(1.2,7)[0])\n\n8.4\n\n\nA static parameter is a parameter value that is known at compile time. If we define n to be static, then the first time you call f_jit(x, 7) it will compile and then it will reuse the compiled code for any other value of x. If we then call f_jit(x, 9), the code will compile again.\nTo see this, we can make use of a JAX oddity: if a function prints something7, then it will only be printed upon compilation and never again. This means that we can’t do debug by print. But on the upside, it’s easy to check, when things are compiling.\n\ndef f2(x, n):\n  print(f\"compiling: n = {n}\")\n  return f(x,n)[0]\n\nf2_jit = jit(f2, static_argnums=(1,))\nprint(f2_jit(1.2,7))\nprint(f2_jit(1.8,7))\nprint(f2_jit(1.2,9))\nprint(f2_jit(1.8,7))\n\ncompiling: n = 7\n8.4\n12.6\ncompiling: n = 9\n10.799999\n12.6\n\n\nThis is a perfectly ok solution as long as the static parameters don’t change very often. In our context, this is going to have to do with the sparsity pattern.\nFinally, we can talk about jax.lax.fori_loop, the in-built for loop. This is basically a convenience wrapper for jax.lax.scan (when lower and upper are static) or jax.lax.while (when they are not). The Python pseudocode is\n\ndef jax_lax_fori_loop(lower, upper, body_func, init):\n  out = init\n  for i in range(lower, upper):\n    out = body_func(i, out)\n  return out\n\nTo close out this bit where I repeat the docs, there is also a traceable if/else: jax.lax.cond which has the pseudocode\n\ndef jax_lax_cond(pred, true_fun, false_fun, val):\n  if pred:\n    return true_fun(val)\n  else:\n    return false_fun(val)"
  },
  {
    "objectID": "posts/2022-11-27-sparse7/sparse7.html#building-a-jax-traceable-symbolic-sparse-choleksy-factorisation",
    "href": "posts/2022-11-27-sparse7/sparse7.html#building-a-jax-traceable-symbolic-sparse-choleksy-factorisation",
    "title": "Sparse matrices part 7a: Another shot at JAX-ing the Cholesky decomposition",
    "section": "Building a JAX-traceable symbolic sparse Choleksy factorisation",
    "text": "Building a JAX-traceable symbolic sparse Choleksy factorisation\nIn order to build a JAX-traceable sparse Cholesky factorisation \\(A = LL^T\\), we are going to need to build up a few moving parts.\n\nBuild the elimination tree of \\(A\\) and find the number of non-zeros in each column of \\(L\\)\nBuild the symbolic factorisation8 of \\(L\\) (aka the location of the non-zeros of \\(L\\))\nDo the actual numerical decomposition.\n\nIn the previous post we did not explicitly form the elimination tree. Instead, I used dynamic memory allocation. This time I’m being more mature.\n\nBuilding the expression graph\nThe elimination tree9 \\(\\mathcal{T}_A\\) is a (forest of) rooted tree(s) that compactly represent the non-zero pattern of the Cholesky factor \\(L\\). In particular, the elimination tree has the property that, for any \\(k &gt; j\\) , \\(L_{kj} \\neq 0\\) if and only if there is a path from \\(j\\) to \\(k\\) in the tree. Or, in the language of trees, \\(L_{kj} \\neq 0\\) if and only if \\(j\\) is a descendant of \\(k\\) in the tree \\(\\mathcal{T}_A\\).\nWe can describe10 \\(\\mathcal{T}_A\\) by listing the parent of each node. The parent node of \\(j\\) in the tree is the smallest \\(i &gt; j\\) with \\(L_{ij} \\neq 0\\).\nWe can turn this into an algorithm. An efficient version, which is described in Tim Davies book takes about \\(\\mathcal{O(\\text{nnz}(A))}\\) operations. But I’m going to program up a slower one that takes \\(\\mathcal{O(\\text{nnz}(L))}\\) operations, but has the added benefit11 of giving me the column counts for free.\nTo do this, we are going to walk the tree and dynamically add up the column counts as we go.\nTo start off, let’s do this in standard python so that we can see what the algorithm look like. The key concept is that if we write \\(\\mathcal{T}_{j-1}\\) as the elimination tree encoding the structure of12 L[:j, :j], then we can ask about how this tree connects with node j.\nA theorem gives a very simple answer to this.\n\nTheorem 1 If \\(j &gt; i\\), then \\(A_{j,i} \\neq 0\\) implies that \\(i\\) is a descendant of \\(j\\) in \\(\\mathcal{T}_A\\). In particular, that means that there is a directed path in \\(\\mathcal{T}_A\\) from \\(i\\) to \\(j\\).\n\nThis tells us that the connection between \\(\\mathcal{T}_{j-1}\\) and node \\(j\\) is that for each non-zero elements \\(i\\) of the \\(j\\)th row of \\(A\\), we can walk $ must have a path in \\(\\mathcal{T}_{j-1}\\) from \\(i\\) and we will eventually get to a node that has no parent in \\(\\{0,\\ldots, j-1\\}\\). Because there must be a path from \\(i\\) to \\(j\\) in \\(T_j\\), it means that the parent of this terminal node must be \\(j\\).\nAs with everything Cholesky related, this works because the algorithm proceeds from left to right, which in this case means that the node label associated with any descendant of \\(j\\) is always less than \\(j\\).\nThe algorithm is then a fairly run-of-the-mill13 tree traversal, where we keep track of where we have been so we don’t double count our columns.\nProbably the most important thing here is that I am using the full sparse matrix rather than just its lower triangle. This is, basically, convenience. I need access to the left half of the \\(j\\)th row of \\(A\\), which is conveniently the same as the top half of the \\(j\\)th column. And sometimes you just don’t want to be dicking around with swapping between row- and column-based representations.\n\nimport numpy as np\n\ndef etree_base(A_indices, A_indptr):\n  n = len(A_indptr) - 1\n  parent = [-1] * n\n  mark = [-1] * n\n  col_count = [1] * n\n  for j in range(n):\n    mark[j] = j\n    for indptr in range(A_indptr[j], A_indptr[j+1]):\n      node = A_indices[indptr]\n      while node &lt; j and mark[node] != j:\n        if parent[node] == -1:\n          parent[node] = j\n        mark[node] = j\n        col_count[node] += 1\n        node = parent[node]\n  return (parent, col_count)\n\nTo convince ourselves this works, let’s run an example and compare the column counts we get to our previous method.\n\n\nSome boilerplate from previous editions.\nfrom scipy import sparse\nimport scipy as sp\n    \n\ndef make_matrix(n):\n  one_d = sparse.diags([[-1.]*(n-2), [2.]*n, [-1.]*(n-2)], [-2,0,2])\n  A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n))\n  A_csc = A.tocsc()\n  A_csc.eliminate_zeros()\n  A_lower = sparse.tril(A_csc, format = \"csc\")\n  A_index = A_lower.indices\n  A_indptr = A_lower.indptr\n  A_x = A_lower.data\n  return (A_index, A_indptr, A_x, A_csc)\n\ndef _symbolic_factor(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] &gt; j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) &gt; 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\n\n\n# A_indices/A_indptr are the lower triangle, A is the entire matrix\nA_indices, A_indptr, A_x, A = make_matrix(37)\nparent, col_count = etree_base(A.indices, A.indptr)\nL_indices, L_indptr = _symbolic_factor(A_indices, A_indptr)\n\ntrue_parent = L_indices[L_indptr[:-2] + 1]\ntrue_parent[np.where(np.diff(L_indptr[:-1]) == 1)] = -1\nprint(all(x == y for (x,y) in zip(parent[:-1], true_parent)))\n\ntrue_col_count  = np.diff(L_indptr)\nprint(all(true_col_count == col_count))\n\nTrue\nTrue\n\n\nExcellent. Now we just need to convert it to JAX.\nOr do we?\nTo be honest, this is a little pointless. This function is only run once per matrix so we won’t really get much speedup14 from compilation.\nNevertheless, we might try.\n\n@jit\ndef etree(A_indices, A_indptr):\n # print(\"(Re-)compiling etree(A_indices, A_indptr)\")\n  ## innermost while loop\n  def body_while(val):\n  #  print(val)\n    j, node, parent, col_count, mark = val\n    update_parent = lambda x: x[0].at[x[1]].set(x[2])\n    parent = lax.cond(lax.eq(parent[node], -1), update_parent, lambda x: x[0], (parent, node, j))\n    mark = mark.at[node].set(j)\n    col_count = col_count.at[node].add(1)\n    return (j, parent[node], parent, col_count, mark)\n\n  def cond_while(val):\n    j, node, parent, col_count, mark = val\n    return lax.bitwise_and(lax.lt(node, j), lax.ne(mark[node], j))\n\n  ## Inner for loop\n  def body_inner_for(indptr, val):\n    j, A_indices, A_indptr, parent, col_count, mark = val\n    node = A_indices[indptr]\n    j, node, parent, col_count, mark = lax.while_loop(cond_while, body_while, (j, node, parent, col_count, mark))\n    return (j, A_indices, A_indptr, parent, col_count, mark)\n  \n  ## Outer for loop\n  def body_out_for(j, val):\n     A_indices, A_indptr, parent, col_count, mark = val\n     mark = mark.at[j].set(j)\n     j, A_indices, A_indptr, parent, col_count, mark = lax.fori_loop(A_indptr[j], A_indptr[j+1], body_inner_for, (j, A_indices, A_indptr, parent, col_count, mark))\n     return (A_indices, A_indptr, parent, col_count, mark)\n\n  ## Body of code\n  n = len(A_indptr) - 1\n  parent = jnp.repeat(-1, n)\n  mark = jnp.repeat(-1, n)\n  col_count = jnp.repeat(1,  n)\n  init = (A_indices, A_indptr, parent, col_count, mark)\n  A_indices, A_indptr, parent, col_count, mark = lax.fori_loop(0, n, body_out_for, init)\n  return parent, col_count\n\nWow. That is ugly. But let’s see15 if it works!\n\nparent_jax, col_count_jax = etree(A.indices, A.indptr)\n\nprint(all(x == y for (x,y) in zip(parent_jax[:-1], true_parent)))\nprint(all(true_col_count == col_count_jax))\n\nTrue\n\n\nTrue\n\n\nSuccess!\nI guess we could ask ourselves if we gained any speed.\nHere is the pure python code.\n\nimport timeit\nA_indices, A_indptr, A_x, A = make_matrix(20)\n\ntimes = timeit.repeat(lambda: etree_base(A.indices, A.indptr),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(50)\ntimes = timeit.repeat(lambda: etree_base(A.indices, A.indptr),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\n\nA_indices, A_indptr, A_x, A = make_matrix(200)\ntimes = timeit.repeat(lambda: etree_base(A.indices, A.indptr),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nn = 400: [0.0, 0.0, 0.0, 0.0, 0.0]\nn = 2500: [0.03, 0.03, 0.03, 0.03, 0.03]\n\n\nn = 40000: [0.83, 0.82, 0.82, 0.82, 0.82]\n\n\nAnd here is our JAX’d and JIT’d code.\n\nA_indices, A_indptr, A_x, A = make_matrix(20)\ntimes = timeit.repeat(lambda: etree(A.indices, A.indptr),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(50)\ntimes = timeit.repeat(lambda: etree(A.indices, A.indptr),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\n\nA_indices, A_indptr, A_x, A = make_matrix(200)\ntimes = timeit.repeat(lambda: etree(A.indices, A.indptr),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nparent, col_count= etree(A.indices, A.indptr)\nL_indices, L_indptr = _symbolic_factor(A_indices, A_indptr)\n\nA_indices, A_indptr, A_x, A = make_matrix(1000)\ntimes = timeit.repeat(lambda: etree(A.indices, A.indptr),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nn = 400: [0.13, 0.0, 0.0, 0.0, 0.0]\n\n\nn = 2500: [0.12, 0.0, 0.0, 0.0, 0.0]\n\n\nn = 40000: [0.14, 0.02, 0.02, 0.02, 0.02]\n\n\nn = 1000000: [2.24, 2.11, 2.12, 2.12, 2.12]\n\n\nYou can see that there is some decent speedup. For the first three examples, the computation time is dominated by the compilation time, but we see when the matrix has a million unknowns the compilation time is negligible. At this scale it would probably be worth using the fancy algorithm. That said, it is probably not worth sweating a three second that is only done once when your problem is that big!\n\n\nThe non-zero pattern of \\(L\\)\nNow that we know how many non-zeros there are, it’s time to populate them. Last time, I used some dynamic memory allocation to make this work, but JAX is certainly not going to allow me to do that. So instead I’m going to have to do the worst thing possible: think.\nThe way that we went about it last time was, to be honest, a bit arse-backwards. The main reason for this is that I did not have access to the elimination tree. But now we do, we can actually use it.\nThe trick is to slightly rearrange16 the order of operations to get something that is more convenient for working out the structure.\nRecall from last time that we used the left-looking Cholesky factorisation, which can be written in the dense case as\n\ndef dense_left_cholesky(A):\n  n = A.shape[0]\n  L = np.zeros_like(A)\n  for j in range(n):\n    L[j,j] = np.sqrt(A[j,j] - np.inner(L[j, :j], L[j, :j]))\n    L[(j+1):, j] = (A[(j+1):, j] - L[(j+1):, :j] @ L[j, :j].transpose()) / L[j,j]\n  return L\n\nThis is not the only way to organise those operations. An alternative is the up-looking Cholesky factorisation, which can be implemented in the dense case as\n\ndef dense_up_cholesky(A):\n  n = A.shape[0]\n  L = np.zeros_like(A)\n  L[0,0] = np.sqrt(A[0,0])\n  for i in range(1,n):\n    #if i &gt; 0:\n    L[i, :i] = (np.linalg.solve(L[:i, :i], A[:i,i])).transpose()\n    L[i, i] = np.sqrt(A[i,i] - np.inner(L[i, :i], L[i, :i]))\n  return L\n\nThis is quite a different looking beast! It scans row by row rather than column by column. And while the left-looking algorithm is based on matrix-vector multiplies, the up-looking algorithm is based on triangular solves. So maybe we should pause for a moment to check that these are the same algorithm!\n\nA = np.random.rand(15, 15)\nA = A + A.transpose()\nA = A.transpose() @ A + 1*np.eye(15)\n\nL_left = dense_left_cholesky(A)\nL_up = dense_up_cholesky(A)\n\nprint(round(sum(sum(abs((L_left - L_up)[:])))),2)\n\n0 2\n\n\nThey are the same!!\nThe reason for considering the up-looking algorithm is that it gives a slightly nicer description of the non-zeros of row i, which will let us find the location of the non-zeros in the whole matrix. In particular, the non-zeros to the left of the diagonal on row i correspond to the non-zero indices of the solution to the lower triangular linear system17 \\[\nL_{1:(i-1),1:(i-1)} x^{(i)} = A_{1:i-1, i}.\n\\] Because \\(A\\) is sparse, this is a system of \\(\\operatorname{nnz}(A_{1:i-1,i})\\) linear equations, rather than \\((i-1)\\) equations that we would have in the dense case. That means that the sparsity pattern of \\(x^{(i)}\\) will be the union of the sparsity patterns of the columns of \\(L_{1:(i-1),1:(i-1)}\\) that correspond to the non-zero entries of \\(A_{1:i-1, i}\\).\nThis means two things. Firstly, if \\(A_{ji}\\neq 0\\), then \\(x^{(i)}_j \\neq 0\\). Secondly, if $x^{(i)}_j $ and \\(L_{kj}\\neq 0\\), then \\(x_k \\neq 0\\). These two facts give us a way of finding the non-zero set of \\(x^{(i)}\\) if we remember just one more fact: a definition of the elimination tree is that \\(L_{kj} \\neq 0\\) if \\(j\\) is a descendant of \\(k\\) in the elimination tree.\nThis reduces the problem of finding the non-zero elements of \\(x^{(i)}\\) to the problem of finding all of the descendants of \\(\\{j: A_{ji} \\neq 0\\}\\) in the subtree \\(\\mathcal{T}_{i-1}\\). And if there is one thing that people who are ok at programming are excellent at it is walking down a damn tree.\nSo let’s do that. Well, I’ve already done it. In fact, that was how I found the column counts in the first place! With this interpretation, the outer loop is taking us across the rows. And once I am in row j18, I then find a starting node node (which is a non-zero in \\(A_{1:(i-1),i}\\)) and I walk along that node checking each time if I’ve actually seen that node19 before. If I haven’t seen it before, I added one to the column count of column node20.\nTo allocate the non-zero structure, I just need to replace that counter increment with an assignment.\n\n\nAttempt 1: Lord that’s slow\nWe will do the pure python version first.\n\ndef symbolic_cholesky_base(A_indices, A_indptr, parent, col_count):\n  n = len(A_indptr) - 1\n  col_ptr = np.repeat(1, n+1)\n  col_ptr[1:] += np.cumsum(col_count) \n  L_indices = np.zeros(sum(col_count), dtype=int)\n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum(col_count)\n  mark = [-1] * n\n\n  for i in range(n):\n    mark[i] = i\n    L_indices[L_indptr[i]] = i\n\n    for indptr in range(A_indptr[i], A_indptr[i+1]):\n      node = A_indices[indptr]\n      while node &lt; i and mark[node] != i:\n        mark[node] = i\n        L_indices[col_ptr[node]] = i\n        col_ptr[node] += 1\n        node = parent[node]\n  \n  return (L_indices, L_indptr)\n\nDoes it work?\n\nA_indices, A_indptr, A_x, A = make_matrix(13)\nparent, col_count = etree_base(A.indices, A.indptr)\n\nL_indices, L_indptr = symbolic_cholesky_base(A.indices, A.indptr, parent, col_count)\nL_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n\nprint(all(x==y for (x,y) in zip(L_indices, L_indices_true)))\nprint(all(x==y for (x,y) in zip(L_indptr, L_indptr_true)))\n\nTrue\nTrue\n\n\nFabulosa!\nNow let’s do the compiled version.\n\nfrom functools import partial\n@partial(jit, static_argnums = (4,))\ndef symbolic_cholesky(A_indices, A_indptr, L_indptr, parent, nnz):\n  \n  ## innermost while loop\n  def body_while(val):\n    i, L_indices, L_indptr, node, parent, col_ptr, mark = val\n    mark = mark.at[node].set(i)\n    #p = \n    L_indices = L_indices.at[col_ptr[node]].set(i)\n    col_ptr = col_ptr.at[node].add(1)\n    return (i, L_indices, L_indptr, parent[node], parent, col_ptr, mark)\n\n  def cond_while(val):\n    i, L_indices, L_indptr, node, parent, col_ptr, mark = val\n    return lax.bitwise_and(lax.lt(node, i), lax.ne(mark[node], i))\n\n  ## Inner for loop\n  def body_inner_for(indptr, val):\n    i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = val\n    node = A_indices[indptr]\n    i, L_indices, L_indptr, node, parent, col_ptr, mark = lax.while_loop(cond_while, body_while, (i, L_indices, L_indptr, node, parent, col_ptr, mark))\n    return (i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n  \n  ## Outer for loop\n  def body_out_for(i, val):\n     A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = val\n     mark = mark.at[i].set(i)\n     L_indices = L_indices.at[L_indptr[i]].set(i)\n     i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = lax.fori_loop(A_indptr[i], A_indptr[i+1], body_inner_for, (i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark))\n     return (A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n\n  ## Body of code\n  n = len(A_indptr) - 1\n  col_ptr = L_indptr + 1\n  L_indices = jnp.zeros(nnz, dtype=int)\n  \n  mark = jnp.repeat(-1, n)\n  \n  init = (A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n  A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = lax.fori_loop(0, n, body_out_for, init)\n  return L_indices\n\nNow let’s check it works\n\nA_indices, A_indptr, A_x, A = make_matrix(20)\nparent, col_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\n\n\nL_indices = symbolic_cholesky(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1])\nL_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\nprint(all(L_indices == L_indices_true))\nprint(all(L_indptr == L_indptr_true))\n\nA_indices, A_indptr, A_x, A = make_matrix(31)\n\nparent, col_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\n\n\nL_indices = symbolic_cholesky(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1])\nL_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n\nprint(all(L_indices == L_indices_true))\nprint(all(L_indptr == L_indptr_true))\n\nTrue\nTrue\n\n\nTrue\nTrue\n\n\nSuccess!\nOne minor problem. This is slow. as. balls.\n\nA_indices, A_indptr, A_x, A = make_matrix(50)\nparent, col_count = etree_base(A.indices, A.indptr)\ntimes = timeit.repeat(lambda: symbolic_cholesky_base(A.indices, A.indptr, parent, col_count),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(200)\nparent, col_count = etree_base(A.indices, A.indptr)\ntimes = timeit.repeat(lambda: symbolic_cholesky_base(A.indices, A.indptr, parent, col_count),number = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nn = 2500: [0.05, 0.04, 0.04, 0.04, 0.04]\n\n\nn = 40000: [1.97, 2.09, 2.04, 2.03, 1.92]\n\n\nAnd here is our JAX’d and JIT’d code.\n\nA_indices, A_indptr, A_x, A = make_matrix(50)\nparent, col_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda:symbolic_cholesky(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1]),number = 1, repeat = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(200)\nparent, col_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda:symbolic_cholesky(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1]),number = 1, repeat = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nn = 2500: [0.15]\n\n\nn = 40000: [29.19]\n\n\nOooof. Something is going horribly wrong.\n\n\nWhy is it so slow?\nThe first thing to check is if it’s the compile time. We can do this by explicitly lowering the the JIT’d function to its XLA representation and then compiling it.\n\nA_indices, A_indptr, A_x, A = make_matrix(50)\nparent, col_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda: jit(partial(symbolic_cholesky, nnz=int(L_indptr[-1]))).lower(A.indices, A.indptr, L_indptr, parent).compile(),number = 1, repeat = 5)\nprint(f\"Compilation time: n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(200)\nparent, col_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda: jit(partial(symbolic_cholesky, nnz=int(L_indptr[-1]))).lower(A.indices, A.indptr, L_indptr, parent).compile(),number = 1, repeat = 5)\nprint(f\"Compilation time: n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nCompilation time: n = 2500: [0.15, 0.15, 0.15, 0.16, 0.15]\n\n\nCompilation time: n = 40000: [0.16, 0.15, 0.14, 0.16, 0.15]\n\n\nIt is not the compile time.\nAnd that is actually a good thing because that suggests that we aren’t having problems with the compiler unrolling all of our wonderful loops! But that does mean that we have to look a bit deeper into the code. Some smart people would probably be able to look at the jaxpr intermediate representation to diagnose the problem. But I couldn’t see anything there.\nInstead I thought if I were a clever, efficient compiler, what would I have problems with?. And the answer is the classic sparse matrix answer: indirect indexing.\nThe only structural difference between the etree function and the symbolic_cholesky function is this line in the body_while() function:\n\n L_indices = L_indices.at[col_ptr[node]].set(i)\n\nIn order to evaluate this code, the compiler has to resolve two levels of indirection. By contrast, the indexing in etree() was always direct. So let’s see what happens if we take the same function and remove that double indirection.\n\n@partial(jit, static_argnums = (4,))\ndef test_fun(A_indices, A_indptr, L_indptr, parent, nnz):\n  \n  ## innermost while loop\n  def body_while(val):\n    i, L_indices, L_indptr, node, parent, col_ptr, mark = val\n    mark = mark.at[node].set(i)\n    L_indices = L_indices.at[node].set(i)\n    col_ptr = col_ptr.at[node].add(1)\n    return (i, L_indices, L_indptr, parent[node], parent, col_ptr, mark)\n\n  def cond_while(val):\n    i, L_indices, L_indptr, node, parent, col_ptr, mark = val\n    return lax.bitwise_and(lax.lt(node, i), lax.ne(mark[node], i))\n\n  ## Inner for loop\n  def body_inner_for(indptr, val):\n    i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = val\n    node = A_indices[indptr]\n    i, L_indices, L_indptr, node, parent, col_ptr, mark = lax.while_loop(cond_while, body_while, (i, L_indices, L_indptr, node, parent, col_ptr, mark))\n    return (i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n  \n  ## Outer for loop\n  def body_out_for(i, val):\n     A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = val\n     mark = mark.at[i].set(i)\n     L_indices = L_indices.at[L_indptr[i]].set(i)\n     i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = lax.fori_loop(A_indptr[i], A_indptr[i+1], body_inner_for, (i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark))\n     return (A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n\n  ## Body of code\n  n = len(A_indptr) - 1\n  col_ptr = L_indptr + 1\n  L_indices = jnp.zeros(nnz, dtype=int)\n  \n  mark = jnp.repeat(-1, n)\n  \n  init = (A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n  A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = lax.fori_loop(0, n, body_out_for, init)\n  return L_indices\n\nA_indices, A_indptr, A_x, A = make_matrix(50)\nparent, col_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda:test_fun(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1]),number = 1, repeat = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(200)\nparent, col_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda:test_fun(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1]),number = 1, repeat = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nn = 2500: [0.14]\n\n\nn = 40000: [0.17]\n\n\nThat isn’t conclusive, but it does indicate that this might21 be the problem.\nAnd this is a big problem for us! The sparse Cholesky algorithm has similar amounts of indirection. So we need to fix it.\n\n\nAttempt 2: After some careful thought, things stayed the same\nNow. I want to pretend that I’ve got elegant ideas about this. But I don’t. So let’s just do it. The most obvious thing to do is to use the algorithm to get the non-zero structure of the rows of \\(L\\). These are the things that are being indexed by col_ptr[node]], so if we have these explicitly we don’t need multiple indirection. We also don’t need a while loop.\nIn fact, if we have the non-zero structure of the rows of \\(L\\), we can turn that into the non-zero structure of the columns in linear-ish22 time.\nAll we need to do is make sure that our etree() function is also counting the number of nonzeros in each row.\n\n@jit\ndef etree(A_indices, A_indptr):\n # print(\"(Re-)compiling etree(A_indices, A_indptr)\")\n  ## innermost while loop\n  def body_while(val):\n  #  print(val)\n    j, node, parent, col_count, row_count, mark = val\n    update_parent = lambda x: x[0].at[x[1]].set(x[2])\n    parent = lax.cond(lax.eq(parent[node], -1), update_parent, lambda x: x[0], (parent, node, j))\n    mark = mark.at[node].set(j)\n    col_count = col_count.at[node].add(1)\n    row_count = row_count.at[j].add(1)\n    return (j, parent[node], parent, col_count, row_count, mark)\n\n  def cond_while(val):\n    j, node, parent, col_count, row_count, mark = val\n    return lax.bitwise_and(lax.lt(node, j), lax.ne(mark[node], j))\n\n  ## Inner for loop\n  def body_inner_for(indptr, val):\n    j, A_indices, A_indptr, parent, col_count, row_count, mark = val\n    node = A_indices[indptr]\n    j, node, parent, col_count, row_count, mark = lax.while_loop(cond_while, body_while, (j, node, parent, col_count, row_count, mark))\n    return (j, A_indices, A_indptr, parent, col_count, row_count, mark)\n  \n  ## Outer for loop\n  def body_out_for(j, val):\n     A_indices, A_indptr, parent, col_count, row_count, mark = val\n     mark = mark.at[j].set(j)\n     j, A_indices, A_indptr, parent, col_count, row_count, mark = lax.fori_loop(A_indptr[j], A_indptr[j+1], body_inner_for, (j, A_indices, A_indptr, parent, col_count, row_count, mark))\n     return (A_indices, A_indptr, parent, col_count, row_count, mark)\n\n  ## Body of code\n  n = len(A_indptr) - 1\n  parent = jnp.repeat(-1, n)\n  mark = jnp.repeat(-1, n)\n  col_count = jnp.repeat(1,  n)\n  row_count = jnp.repeat(1, n)\n  init = (A_indices, A_indptr, parent, col_count, row_count, mark)\n  A_indices, A_indptr, parent, col_count, row_count, mark = lax.fori_loop(0, n, body_out_for, init)\n  return (parent, col_count, row_count)\n\nLet’s check that the code is actually doing what I want.\n\nA_indices, A_indptr, A_x, A = make_matrix(57)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indices, L_indptr = _symbolic_factor(A_indices, A_indptr)\n\ntrue_parent = L_indices[L_indptr[:-2] + 1]\ntrue_parent[np.where(np.diff(L_indptr[:-1]) == 1)] = -1\nprint(all(x == y for (x,y) in zip(parent[:-1], true_parent)))\n\ntrue_col_count  = np.diff(L_indptr)\nprint(all(true_col_count == col_count))\n\ntrue_row_count = np.array([len(np.where(L_indices == i)[0]) for i in range(57**2)])\nprint(all(true_row_count == row_count))\n\nTrue\nTrue\n\n\nTrue\n\n\nExcellent! With this we can modify our previous function to give us the row-indices of the non-zero pattern instead. Just for further chaos, please note that we are using a CSC representation of \\(A\\) to get a CSR representation of \\(L\\).\nOnce again, we will prototype in pure python and then translate to JAX. The thing to look out for this time is that we know how many non-zeros there are in a row and we know where we need to put them. This suggests that we can compute these things in body_inner_for and then do a vectorised version of our indirect indexing. This should compile down to a single XLA scatter call. This will reduce the number of overall scatter calls from \\(\\operatorname(nnz)(L)\\) to \\(n\\). And hopefully this will fix things.\n\ndef symbolic_cholesky2_base(A_indices, A_indptr, L_indptr, row_count, parent, nnz):\n\n  n = len(A_indptr) - 1\n  col_ptr = L_indptr + 1\n  L_indices = np.zeros(L_indptr[-1], dtype=int)\n  mark = [-1] * n\n\n  for i in range(n):\n    mark[i] = i\n    row_ind = np.repeat(nnz+1, row_count[i])\n    row_ind[-1] = L_indptr[i]\n    counter = 0\n    for indptr in range(A_indptr[i], A_indptr[i+1]):\n      node = A_indices[indptr]\n      while node &lt; i and mark[node] != i:\n        mark[node] = i\n        row_ind[counter] = col_ptr[node]\n        col_ptr[node] += 1\n        node = parent[node]\n        counter +=1\n    L_indices[row_ind] = i\n  \n  return L_indices\n\n\nA_indices, A_indptr, A_x, A = make_matrix(13)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\n\n\nL_indices = symbolic_cholesky2_base(A.indices, A.indptr, L_indptr, row_count, parent, L_indptr[-1])\nL_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n\nprint(all(x==y for (x,y) in zip(L_indices, L_indices_true)))\nprint(all(x==y for (x,y) in zip(L_indptr, L_indptr_true)))\n\nTrue\nTrue\n\n\nExcellent. Now let’s JAX this. The JAX-heads among you will notice that we have a subtle23 problem: in a fori_loop, JAX does not treat i as static, which means that the length of the repeat (row_count[i]) can never be static and it therefore can’t be traced.\nShit.\nIt is hard to think of a good option here. A few months back Junpeng Lao24 sent me a script with his attempts at making the Cholesky stuff JAX transformable. And he hit the same problem. I was, in an act of hubris, trying very hard to not end up here. But that was tragically slow. So here we are.\nHe came up with two methods.\n\nPad out row_ind so it’s always long enough. This only costs memory. The maximum size of row_ind is n. Unfortunately, this happens whenever \\(A\\) has a dense row. Sadly, for Bayesian25 linear mixed models this will happen if we put Gaussian priors on the covariate coefficients26 and we try to marginalise them out with the other multivariate Gaussian parts. It is possible to write the routines that deal with dense rows and columns explicitly, but it’s a pain in the arse.\nDo some terrifying work with lax.scan and dynamic slicing.\n\nI’m going to try the first of these options.\n\n@partial(jit, static_argnums = (5, 6))\ndef symbolic_cholesky2(A_indices, A_indptr, L_indptr, row_count, parent, nnz, max_row):\n  ## innermost while loop\n  def body_while(val):\n    i, counter, row_ind, node, col_ptr, mark = val\n    mark = mark.at[node].set(i)\n    row_ind = row_ind.at[counter].set(col_ptr[node])\n    col_ptr = col_ptr.at[node].add(1)\n    return (i, counter + 1, row_ind, parent[node], col_ptr, mark)\n\n  def cond_while(val):\n    i, counter, row_ind, node, col_ptr, mark = val\n    return lax.bitwise_and(lax.lt(node, i), lax.ne(mark[node], i))\n\n  ## Inner for loop\n  def body_inner_for(indptr, val):\n    i, counter, row_ind, parent, col_ptr, mark = val\n    node = A_indices[indptr]\n    i, counter, row_ind, node, col_ptr, mark = lax.while_loop(cond_while, body_while, (i, counter, row_ind, node, col_ptr, mark))\n    return (i, counter, row_ind, parent, col_ptr, mark)\n  \n  ## Outer for loop\n  def body_out_for(i, val):\n     L_indices, parent, col_ptr, mark = val\n     mark = mark.at[i].set(i)\n     row_ind = jnp.repeat(nnz+1, max_row)\n     row_ind = row_ind.at[row_count[i]-1].set(L_indptr[i])\n     counter = 0\n\n     i, counter, row_ind, parent, col_ptr, mark = lax.fori_loop(A_indptr[i], A_indptr[i+1], body_inner_for, (i, counter, row_ind, parent, col_ptr, mark))\n\n     L_indices = L_indices.at[row_ind].set(i, mode = \"drop\")\n     return (L_indices, parent, col_ptr, mark)\n\n  ## Body of code\n  n = len(A_indptr) - 1\n\n  col_ptr = jnp.array(L_indptr + 1)\n  L_indices = jnp.ones(nnz, dtype=int) * (-1)\n  mark = jnp.repeat(-1, n)\n\n  ## Make everything a jnp array. Really should use jaxtyping\n  A_indices = jnp.array(A_indices)\n  A_indptr = jnp.array(A_indptr)\n  L_indptr = jnp.array(L_indptr)\n  row_count = jnp.array(row_count)\n  parent = jnp.array(parent)\n\n  init = (L_indices, parent, col_ptr, mark)\n  L_indices, parent, col_ptr, mark = lax.fori_loop(0, n, body_out_for, init)\n  return L_indices\n\nOk. Let’s see if that worked.\n\nA_indices, A_indptr, A_x, A = make_matrix(20)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\n\n\nL_indices = symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count)))\nL_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\nprint(all(L_indices == L_indices_true))\nprint(all(L_indptr == L_indptr_true))\n\nA_indices, A_indptr, A_x, A = make_matrix(31)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\n\n\nL_indices = symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count)))\nL_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\nprint(all(L_indices == L_indices_true))\nprint(all(L_indptr == L_indptr_true))\n\nTrue\nTrue\n\n\nTrue\nTrue\n\n\nOk. Once more into the breach. Is this any better?\n\nA_indices, A_indptr, A_x, A = make_matrix(50)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda:symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count))),number = 1, repeat = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(200)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda:symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count))),number = 1, repeat = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\n# A_indices, A_indptr, A_x, A = make_matrix(300)\n# parent, col_count, row_count = etree(A.indices, A.indptr)\n# L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n# L_indptr[1:] = np.cumsum(col_count)\n# times = timeit.repeat(lambda:symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count))),number = 1, repeat = 1)\n# print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nn = 2500: [0.28]\n\n\nn = 40000: [28.31]\n\n\nFuck.\n\n\nAttempt 3: A desperate attempt to make this bloody work\nRight. Let’s try again. What if instead of doing all those scatters we instead, idk, just store two vectors and sort. Because at this point I will try fucking anything. What if we just list out the column index and row index as we find them (aka build the sparse matrix in COO27 format. The jax.experimental.sparse module has support for (blocked) COO objects but doesn’t implement this transformation. scipy.sparse has a fast conversion routine so I’m going to use it. In the interest of being 100% JAX, I tried a version with jnp.lexsort[index[1][jnp.lexsort((index[1], index[0]))]], which basically does the same thing but it’s a lot slower.\n\ndef symbolic_cholesky3(A_indices, A_indptr, L_indptr, parent, nnz):\n  @partial(jit, static_argnums = (4,))\n  def _inner(A_indices_, A_indptr_, L_indptr, parent, nnz):\n    ## Make everything a jnp array. Really should use jaxtyping\n    A_indices_ = jnp.array(A_indices_)\n    A_indptr_ = jnp.array(A_indptr_)\n    L_indptr = jnp.array(L_indptr)\n    parent = jnp.array(parent)\n\n    ## innermost while loop\n    def body_while(val):\n      index, i, counter,  node,  mark = val\n      mark = mark.at[node].set(i)\n      index[0] = index[0].at[counter].set(node) #column\n      index[1] = index[1].at[counter].set(i) # row\n      return (index, i, counter + 1, parent[node], mark)\n\n    def cond_while(val):\n      index, i, counter,  node,  mark = val\n      return lax.bitwise_and(lax.lt(node, i), lax.ne(mark[node], i))\n\n    ## Inner for loop\n    def body_inner_for(indptr, val):\n      index, i, counter, mark = val\n      node = A_indices_[indptr]\n      \n      index, i, counter,  node,  mark = lax.while_loop(cond_while, body_while, (index, i, counter,  node,  mark))\n      return (index, i, counter,  mark)\n    \n    ## Outer for loop\n    def body_out_for(i, val):\n      index, counter,  mark = val\n      mark = mark.at[i].set(i)\n      index[0] = index[0].at[counter].set(i)\n      index[1] = index[1].at[counter].set(i)\n      counter = counter + 1\n      index, i, counter, mark = lax.fori_loop(A_indptr_[i], A_indptr_[i+1], body_inner_for, (index, i, counter,  mark))\n\n      return (index, counter,  mark)\n\n    ## Body of code\n    n = len(A_indptr_) - 1\n    mark = jnp.repeat(-1, n)\n\n    index = [jnp.zeros(nnz, dtype=int), jnp.zeros(nnz, dtype=int)]\n    counter = 0\n\n    init = (index, counter, mark)\n    index, counter, mark = lax.fori_loop(0, n, body_out_for, init)\n    \n    return index\n  n = len(A_indptr) - 1\n  index = _inner(A_indices, A_indptr, L_indptr, parent, nnz)\n  ## return jnp.lexsort[index[1][jnp.lexsort((index[1], index[0]))\n  return sparse.coo_array((np.ones(nnz), (index[1], index[0])), shape = (n,n)).tocsc().indices\n\nFirst things first, let’s check how fast this is.\n\nA_indices, A_indptr, A_x, A = make_matrix(15)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\nL_indices = symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1]))\nL_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\nprint(all(L_indices == L_indices_true))\nprint(all(L_indptr == L_indptr_true))\n\nA_indices, A_indptr, A_x, A = make_matrix(31)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\nL_indices = symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1]))\nL_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\nprint(all(L_indices == L_indices_true))\nprint(all(L_indptr == L_indptr_true))\n\nA_indices, A_indptr, A_x, A = make_matrix(50)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda: symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1])),number = 1, repeat = 5)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(200)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda: symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1])),number = 1, repeat = 5)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(300)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda: symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1])),number = 1, repeat = 5)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nA_indices, A_indptr, A_x, A = make_matrix(1000)\nparent, col_count, row_count = etree(A.indices, A.indptr)\nL_indptr = np.zeros(A.shape[0]+1, dtype=int)\nL_indptr[1:] = np.cumsum(col_count)\ntimes = timeit.repeat(lambda: symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1])),number = 1, repeat = 1)\nprint(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n\nTrue\nTrue\nTrue\nTrue\n\n\nn = 2500: [0.13, 0.13, 0.13, 0.14, 0.14]\n\n\nn = 40000: [0.19, 0.19, 0.19, 0.19, 0.19]\n\n\nn = 90000: [0.43, 0.32, 0.32, 0.33, 0.33]\n\n\nn = 1000000: [11.91]\n\n\nYou know what? I’ll take it. It’s not perfect, in particular I would prefer a pure JAX solution. But everything I tried hit hard against the indirect memory access issue. The best I found was using jnp.lexsort but even it had noticeable performance degradation as nnz increased relative to the scipy solution."
  },
  {
    "objectID": "posts/2022-11-27-sparse7/sparse7.html#footnotes",
    "href": "posts/2022-11-27-sparse7/sparse7.html#footnotes",
    "title": "Sparse matrices part 7a: Another shot at JAX-ing the Cholesky decomposition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re wondering about the break between sparse matrix posts, I realised this pretty much immediately and just didn’t want to deal with it!↩︎\nIf a person who actually knows how the JAX autodiff works happens across this blog, I’m so sorry.↩︎\nomg you guys. So many details↩︎\nThese are referred to as HLOs (Higher-level operations)↩︎\nInstead of doing one pass of reverse-mode, you would need to do \\(d\\) passes of forwards mode to get the gradient with respect to a d-dimensional parameter.↩︎\nUnlike jax.lax.while, which is only forwards differentiable, jax.lax.scan is fully differentiable.↩︎\nIn general, if the function has state.↩︎\nThis is the version of the symbolic factorisation that is most appropriate for us, as we will be doing a lot of Cholesky factorisations with the same sparsity structure. If we rearrange the algorithm to the up-looking Cholesky decomposition, we only need the column counts and this is also called the symbolic factorisation. This is, incidentally, how Eigen’s sparse Cholesky works.↩︎\nActually it’s a forest↩︎\nBecause we are talking about a tree, each child node has at most one parent. If it doesn’t have a parent it’s the root of the tree. I remember a lecturer saying that it should be called “father and son” or “mother and daughter” because every child has 2 parents but only one mother or one father. The 2000s were a wild time.↩︎\nThese can also be computed in approximately \\(\\mathcal{O(\\text{nnz}(A))}\\) time, which is much faster. But the algorithm is, frankly, pretty tricky and I’m not in the mood to program it up. This difference would be quite important if I wasn’t storing the full symbolic factorisation and was instead computing it every time, but in my context it is less clear that this is worth the effort.↩︎\nPython notation! This is rows/cols 0 to j-1↩︎\nPython, it turns out, does not have a do while construct because, apparently, everything is empty and life is meaningless.↩︎\nThe argument for JIT works by amortizing the compile time over several function evaluations. If I wanted to speed this algorithm up, I’d implement the more complex \\(\\mathcal{O}(\\operatorname{nnz}(A))\\) version.↩︎\nObviously it did not work the first time. A good way to debug JIT’d code is to use the python translations of the control flow literals. Why? Well for one thing there is an annoying tendency for JAX to fail silently when their is an out-of-bounds indexing error. Which happens, just for example, if you replace node = A_indices[indptr] with node = A_indices[A_indptr[indptr]] because you got a text message half way through the line.↩︎\nWe will still use the left-looking algorithm for the numerical computation. The two algorithms are equivalent in exact arithmetic and, in particular, have identical sparsity structures.↩︎\nI’m mixing 1-based indexing in the maths with 0-based in the code because I think we need more chaos in our lives.↩︎\nYes. I know. I’m swapping the meaning of \\(i\\) and \\(j\\) but you know that’s because in a symmetric matrix rows and columns are a bit similar. The upper half of column $$ is the left half of row \\(j\\) after all.↩︎\nIf mark[node]==j then I have already found node and all of its ancestors in my sweep of row j↩︎\nThis is because L[j,node] != 0 by our logic.↩︎\nI mean, I’m pretty sure it is. I’m writing this post in order, so I don’t know yet. But surely the compiler can’t reason about the possible values of node, which would be the only thing that would speed this up.↩︎\nConvert from CSR to (i, j, val) (called COO, which has a convenient implementation in jax.experimental.sparse) to CSC. This involves a linear pass, a sort, and another linear pass. So it’s $n n`ish. Hire me fancy tech companies. I can count. Just don’t ask me to program quicksort.↩︎\nReplace “subtle” with “fairly obvious once I realised how it’s converted to a lax.scan, but not at all obvious to me originally”.↩︎\nWho demanded a footnote.↩︎\nThis also happens with the profile likelihood in non-Bayesian methods.↩︎\nthe \\(\\beta\\)s↩︎\nCOO stands for coordinate list and it’s the least space-efficient of our options. It directly stores 3 length n vectors (row, col, value). It’s great for specifying matrices and it’s pretty easy to convert from this format to any of the others.↩︎\nother than holiday↩︎\nA_index and A.index are different↩︎\nI’m probably going to bind Eigen’s AMD decomposition. I’m certainly not writing it myself.↩︎"
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "",
    "text": "Now that we know what they are, I guess we should do something with a Gaussian process. But we immediately hit a problem. You see, Gaussian processes are charming things, sweet and caring. But they have a dark side. Used naively1, they’re computationally expensive when you’ve got a lot of data.\nStab of dramatic music\nYeah. So. What’s the problem here? Well, the first problem is people seem to really like having a lot of data. Fuck knows why. Most of it is rubbish2. But they do.\nThis is a problem for our poor little Gaussian processes because of how the data tends to come.\nA fairly robust model for data is that it comes like \\[\n(y_i, s_i, x_i),\n\\] where \\(y_i\\) is our measurement of choice (which might be a continuous, discrete or weird3), \\(s_i\\) is our location4 in the index set5 \\(\\Omega\\) (usually \\(\\Omega \\subset \\mathbb{R}^d\\)) of the Gaussian process, and \\(x_i\\) is whatever other information we have6. If we want to be really saucy, we could also assume these things are iid samples from some unknown distribution and then pretend like that isn’t a wildly strong structural assumption. But I’m not like that. I’ll assume the joint distribution of the samples is exchangeable7 8 9. Or something. I’m writing this sequentially, so I have no idea where this is going to end up.\nSo where is the problem? The problem is that, if we use the most immediately computational definition of a Gaussian process, then we need to build \\[\n\\begin{pmatrix} u(s_1)\\\\ u(s_2) \\\\ \\vdots \\\\ u(s_n)\\end{pmatrix} \\sim N\\left(\n\\begin{pmatrix} \\mu(s_1)\\\\ \\mu(s_2) \\\\ \\vdots \\\\ \\mu(s_n)\\end{pmatrix},\n\\begin{pmatrix} c(s_1, s_1) & c(s_1, s_2) & \\cdots & c(s_1, s_n)  \\\\ c(s_2, s_1) & c(s_2, s_2) & \\cdots & c(s_2, s_n) \\\\\\vdots &\\vdots  &\\ddots &\\vdots \\\\ c(s_n, s_1) & c(s_n, s_2) & \\cdots & c(s_n, s_n)\\end{pmatrix}\\right).\n\\] Where \\(s_1,\\ldots, s_n\\) are all of the distinct values of \\(s_i\\) in the dataset. If there are a lot of these, the covariance matrix is very large and this becomes a problem. First, we must construct it. Then we must solve it. Then we must do actual computations with it. The storage scales quadratically in \\(n\\). The computation scales cubically in \\(n\\). This is too much storage and too much computation if the data set has a lot of distinct GP evaluations, it will simply be too expensive to do the matrix work that we need to do in order to make this run.\nSo we need to do something else."
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#so.-gaussian-processes-eh.",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#so.-gaussian-processes-eh.",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "",
    "text": "Now that we know what they are, I guess we should do something with a Gaussian process. But we immediately hit a problem. You see, Gaussian processes are charming things, sweet and caring. But they have a dark side. Used naively1, they’re computationally expensive when you’ve got a lot of data.\nStab of dramatic music\nYeah. So. What’s the problem here? Well, the first problem is people seem to really like having a lot of data. Fuck knows why. Most of it is rubbish2. But they do.\nThis is a problem for our poor little Gaussian processes because of how the data tends to come.\nA fairly robust model for data is that it comes like \\[\n(y_i, s_i, x_i),\n\\] where \\(y_i\\) is our measurement of choice (which might be a continuous, discrete or weird3), \\(s_i\\) is our location4 in the index set5 \\(\\Omega\\) (usually \\(\\Omega \\subset \\mathbb{R}^d\\)) of the Gaussian process, and \\(x_i\\) is whatever other information we have6. If we want to be really saucy, we could also assume these things are iid samples from some unknown distribution and then pretend like that isn’t a wildly strong structural assumption. But I’m not like that. I’ll assume the joint distribution of the samples is exchangeable7 8 9. Or something. I’m writing this sequentially, so I have no idea where this is going to end up.\nSo where is the problem? The problem is that, if we use the most immediately computational definition of a Gaussian process, then we need to build \\[\n\\begin{pmatrix} u(s_1)\\\\ u(s_2) \\\\ \\vdots \\\\ u(s_n)\\end{pmatrix} \\sim N\\left(\n\\begin{pmatrix} \\mu(s_1)\\\\ \\mu(s_2) \\\\ \\vdots \\\\ \\mu(s_n)\\end{pmatrix},\n\\begin{pmatrix} c(s_1, s_1) & c(s_1, s_2) & \\cdots & c(s_1, s_n)  \\\\ c(s_2, s_1) & c(s_2, s_2) & \\cdots & c(s_2, s_n) \\\\\\vdots &\\vdots  &\\ddots &\\vdots \\\\ c(s_n, s_1) & c(s_n, s_2) & \\cdots & c(s_n, s_n)\\end{pmatrix}\\right).\n\\] Where \\(s_1,\\ldots, s_n\\) are all of the distinct values of \\(s_i\\) in the dataset. If there are a lot of these, the covariance matrix is very large and this becomes a problem. First, we must construct it. Then we must solve it. Then we must do actual computations with it. The storage scales quadratically in \\(n\\). The computation scales cubically in \\(n\\). This is too much storage and too much computation if the data set has a lot of distinct GP evaluations, it will simply be too expensive to do the matrix work that we need to do in order to make this run.\nSo we need to do something else."
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#a-tangent-or-can-we-just-be-smarter",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#a-tangent-or-can-we-just-be-smarter",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "A tangent; or Can we just be smarter?",
    "text": "A tangent; or Can we just be smarter?\nOn a tangent, because straight lines are for poor souls who don’t know about Gaussian processes, there’s a body of work on trying to circumvent this problem by being good at maths. The idea is to try to find some cases where we don’t need to explicitly form the covariance matrix in order to do all of the calculations. There’s a somewhat under-cooked10 literature on this. It dances around an idea that traces back to fast multipole methods for integral equations: We know that correlations decay as points get further apart, so we do not need to calculate the correlations between points that are far apart as well as we need to calculate the correlations between points that are close together. For a fixed covariance kernel that decays in a certain way, you can modify the fast multipole method, however it’s more fruitful to use an algebraic11 method. H-matrices was the first real version of this, and there’s a paper from 2008 using them to approximate GPs. A solid chunk of time later, there have been two good papers recently on this stuff. Paper 1 Paper 2. These methods really only provide gradient descent type methods for maximum likelihood estimation and it’s not clear to me that you’d be able to extend these ideas easily to a Bayesian setting (particularly when you need to infer some parameters in the covariance function)12.\nI think this sort of stuff is cool for a variety of reasons, but I also don’t think it’s the entire solution. (There was also a 2019 NeurIPS paper that scales a GP fit to a million observations as if that’s a good idea. It is technically impressive, however.) But I think the main possibility of the H-matrix work is that it allows us to focus on the modelling and not have to make premature trade offs with the computation.\nThe problem with modelling a large dataset using a GP is that GPs are usually fit with a bunch of structural assumptions (like stationarity and isotropy) that are great simplifying assumptions for moderate data sizes but emphatically do not capture the complex dependency structures when there is a large amount of data. As you get more data, your model should become correspondingly more complex13 and stationary, and/or isotropic Gaussian processes emphatically do not do this.\nThis isn’t to say that you shouldn’t use GPs on a large data set (I am very much on record as thinking you should), but that it needs to be a part of your modelling arsenal and probably not the whole thing. The real glory of GPs is that they are a flexible enough structure to play well with other modelling techniques. Even if you end up modelling a large data set with a single GP, that GP will most likely be anisotropic, non-stationary, and built up from multiple scales. Which is a different way to say that it likely does not have a squared exponential kernel with different length scales for each feature.\n(It’s probably worth making the disclaimer at this point, but when I’m thinking about GPs, I’m typically thinking about them in 1-4 dimensions. My background is in spatial statistics, so that makes sense. Some of my reasoning doesn’t apply in more typical machine learning applications where \\(s_i\\) might be quite high-dimensional. That said, you simply get a different end of the same problem. In that case you need to balance the smoothness needed to interpolate in high dimensions with the structure needed to allow your variables to be a) scaled differently and b) correlated. Life is pain either way.)"
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#so-can-we-make-things-better",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#so-can-we-make-things-better",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "So can we make things better?",
    "text": "So can we make things better?\nThe problem with Gaussian processes, at least from a computational point of view, is that they’re just too damn complicated. Because they are supported on some infinite dimensional Banach space \\(B\\), the more we need to see of them (for instance because we have a lot of unique \\(s_i\\)s) the more computational power they require. So the obvious solution is to somehow make Gaussian processes less complex.\nThis somehow has occupied a lot of people’s time over the last 20 years and there are many many many many possible options. But for the moment, I just want to focus on one of the generic classes of solutions: You can make Gaussian processes less computationally taxing by making them less expressive.\nOr to put it another way, if you choose an \\(m\\) dimensional subspace \\(V_m \\subset B\\) and rep;ace the GP \\(u\\), which is supported on the whole of \\(B\\), with a different Gaussian process \\(u_m\\) supported on \\(V_m\\), then all of your problems go away.\nWhy? Well because the Gaussian process on \\(V_m\\) can be represented in terms of an \\(m\\)-dimensional Gaussian random vector. Just take \\(\\phi_j\\), \\(j=1,\\ldots, m\\) to be a basis for \\(V_m\\), then the GP \\(u_m\\) can be written as \\[\nu_m = \\sum_{j=1}^m w_j \\phi_j,\n\\] where \\(w \\sim N(\\mu, \\Sigma)\\), for some \\(\\mu\\) and \\(\\Sigma\\). (The critical thing here is that the \\(\\phi_j\\) are functions so \\(u_m\\) is still a random function! That link between the multivariate Gaussian \\(w\\) and the function \\(u_m\\) that can be evaluated at any \\(s_i\\) is really important!)\nThis means that I can express my Gaussian process prior in terms of the multivariate Gaussian prior on \\(w\\), and I only need \\(\\mathcal{O}(m^3)\\) operations to evaluate its log-density.\nIf our observation model is such that \\(p(y_i \\mid u) = p(y_i \\mid u(s_i))\\), and we assume conditional14 independence, then we can eval the log-likelihood term \\[\n\\sum_{i=1}^n p(y \\mid u_m(s_i)) = \\sum_{i=1}^n p(y \\mid a_i^Tw)\n\\] in \\(\\mathcal{O}(m^2 n)\\) operations. Here \\([a_i]_j = \\phi_j(s_i)\\) is the vector that links the basis in \\(u_n\\) that we use to define \\(w\\) to the observation locations15.\nMany have been tempted to look at the previous paragraph and conclude that a single evaluation of the log-posterior (or its gradient) will be \\(\\mathcal{O}(n)\\), as if that \\(m^2\\) multiplier were just a piece of fluff to be flicked away into oblivion.\nThis is, of course, sparkly bullshit.\nThe subspace size \\(m\\) controls the trade off between bias and computational cost and, if we want that bias to be reasonably small, we need \\(m\\) to be quite large. In a lot of cases, it needs to grow with \\(n\\). A nice paper by David Burt, Carl Rasmussen, and Mark van der Wilk suggests that \\(m(n)\\) needs to depend on the covariance function16. In the best case (when you assume your function is so spectacularly smooth that a squared-exponential covariance function is ok), you need something like \\(m = \\mathcal{O}(\\log(n)^d)\\), while if you’re willing to make a more reasonable assumption that your function has \\(\\nu\\) continuous17 derivatives, then you need something like \\(m = \\mathcal{O}(n^\\frac{2d}{2\\nu-d})\\).\nYou might look at those two options for \\(m\\) and say to yourself “well shit. I’m gonna use a squared exponential from now on”. But it is never as simple as that. You see, if you assume a function is so smooth it is analytic18, then you’re assuming that it lacks the derring-do to be particularly interesting between its observed values19. This translates to relatively narrow uncertainty bands. Whereas a function with \\(\\nu\\) derivatives has more freedom to move around the smaller \\(\\nu\\) is. This naturally results in wider uncertainty bands.\nI think20 in every paper I’ve seen that compares a squared exponential covariance function to a Matérn-type covariance function (aka the ones that let you have \\(\\nu\\)-times differentiable sample paths), the Matérn family has performed better (in my mind this is also in terms of squared error, but it’s definitely the case when you’re also evaluating the uncertainty of the prediction intervals). So I guess the lesson is that cheap isn’t always good?\nAnyway. The point of all of this is that if we can somehow restrict our considerations to an \\(m\\)-dimensional subspace of \\(B\\), then we can get some decent (if not perfect) computational savings.\nBut what are the costs?"
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#some-notation-that-rapidly-degenerates-into-a-story-thats-probably-not-interesting",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#some-notation-that-rapidly-degenerates-into-a-story-thats-probably-not-interesting",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "Some notation that rapidly degenerates into a story that’s probably not interesting",
    "text": "Some notation that rapidly degenerates into a story that’s probably not interesting\nSo I guess the key question we need to answer before we commit to any particular approximation of our Gaussian process is what does it cost? That is, how does the approximation affect the posterior distribution?\nTo quantify this, we need a way to describe the posterior of a Gaussian process in general. As happens so often when dealing with Gaussian processes, shit is about to get wild.\nA real challenge with working with Gaussian processes theoretically is that they are objects that naturally live on some (separable21) Banach space \\(B\\). One of the consequences of this is that we cannot just write the density of \\(u\\) as \\[\np(u) \\propto  \\exp\\left(-\\frac{1}{2}C_u(u, u)\\right)\n\\] because there is no measure22 on \\(B\\) such that \\[\n\\Pr(u \\in A) = \\int_A p(u)\\,du.\n\\]\nThis means that we can’t just work with densities to do all of our Bayesian stuff. We need to work with posterior probabilities properly.\nUgh. Measures.\nSo let’s do this. We are going to need a prior probability associated with the Gaussian process \\(u\\), which we will write as \\[\n\\mu_0(A) = \\Pr(u \\in A),\n\\] where \\(A\\) is a nice23 set in \\(B\\). We can then use this as a base for our posterior, which we define as \\[\n\\mu^y(A) = \\Pr(u \\in A \\mid y) = \\frac{1}{Z}\\mathrm{e}^{-\\Phi(u;y)},\n\\] where \\(\\Phi(u;y)\\) is the negative log-likelihood function. Here \\(Z\\) is the normalising constant \\[\nZ = \\mathbb{E}_\\mu\\left( \\mathrm{e}^{-\\Phi(u;y)}\\right),\n\\] which is finite as long as \\(\\exp(-\\Phi(u;y)) \\leq C(\\epsilon)\\exp(\\epsilon\\|u\\|^2)\\) for all \\(\\epsilon &gt; 0\\), where \\(C(\\epsilon)\\geq 0\\) is a constant. This is a very light condition.\nThis way of looking at posteriors resulting Gaussian process priors was popularised in the inverse problems literature24. It very much comes from a numerical analysis lens: the work is framed as here is an object, how do we approximate it?.\nThese questions are different to the traditional ones answered by a theoretical statistics papers, which are almost always riffs on “what happens in asymptopia?”.\nI came across this work for two reasons: one is because I have been low-key fascinated by Gaussian measures ever since I saw a talk about them during my PhD; and secondly my PhD was in numerical analysis, so I was reading the journals when these papers came out.\nThat’s not why I explored these questions, though. That is a longer story. The tl;dr is\n\nI had to learn this so I could show a particular point process model converges, and so now the whole rest of this blog post is contained in a technical appendix that no one has ever read in this paper.\n\n\nHere comes the anecdote. Just skip to the next bit. I know you’re like “but Daniel just delete the bullshit text” but that is clearly not how this works.\n\n\nExpand at your peril\n\nI know these papers pretty much backwards for the usual academic reason: out of absolute spite. One of my postdocs involved developing some approximation methods for Markovian Gaussian processes25, which allowed for fast computation, especially when combined with INLA26, which is a fabulous method for approximating posterior distributions when a big chunk of the unobserved parameters have a joint Gaussian prior27.\nOne of the things that INLA was already pretty good at doing was fitting log-Gaussian Cox processes (LGCP), which are a type of model for point patterns28 that can be approximated over a regular grid by a Poisson regression with a log-mean given by (covariates +) a Gaussian process defined on the grid. If that process is Markov, you can get full posterior inference quickly and accurately using INLA. This compared very favourably with pre-INLA methods, which gave you full posterior inference laboriously using a truncated gradient MALA scheme in about the same amount of time it would take the US to get a high-speed rail system.\nAnyway. I was in Trondheim working on INLA and the, at that stage, very new SPDE29 stuff (the 2011 JRSSSB read paper had not been written yet, let alone been read). Janine Illian, who is a very excellent statistician and an all round fabulous person, had been working on the grided LGCP stuff in INLA and came to Trondheim to work with Håvard30 and she happened to give a seminar on using these new LGCP methods to do species distribution mapping. I was strongly encouraged to work with Janine to extend her grid methods to the new shiny SPDE methods, which did not need a grid.\nJanine had to tell me what a Poisson distribution was.\nAnyway. A little while later31 we had a method that worked and we32 wrote it up. We submitted it to Series B and they desk rejected it. We then, for obscure reasons33, submitted it to Biometrika. Due to the glory of arXiv, I can link to the original version.\nAlthough it was completely unlike anything else that Biometrika publishes, we got some quite nice reviews and either major revisions or a revise and resubmit. But one of the reviewer comments pissed me off: they said that we hadn’t demonstrated that our method converges. Now, I was young at the time and new to the field and kinda shocked by all of the shonky numerics that was all over statistics at the time. So this comment34 pissed me off. More than that, though, I was fragile and I hated the comment because I was new to this and had absolutely no fucking idea how to prove this method would converge. Rasmus Waagepetersen had proven convergence of the grid approximation but a) I didn’t understand the proof and b) our situation was so far away there was no chance of piggybacking off it.\nIt was also very hard to use other existing statistics literature, because, far from being an iid situation, the negative log-likelihood for a Poisson process35 on an observation window \\(\\Omega\\) is \\[\n\\Phi(u;y) = \\int_\\Omega e^{u(s)}\\,ds - \\sum_{s_i \\in y}e^{u(s_i)} - |\\Omega|,\n\\] where the point pattern \\(y\\) is a (random) collection of points \\(s_j\\) and \\(|\\Omega|\\) is the area/volume of the observation window. This is fundamentally not like a standard GP regression.\nSo, long story short36, I was very insecure and rather than admit that it was difficult to show that these approximations converged, I worked on and off for like 2 years trying to work out how to do this37 and eventually came up with a fairly full convergence theory for posteriors derived from approximate likelihoods and finite dimensional approximations to Gaussian processes38. Which I then put into the appendix of a paper that was essentially about something completely different.\nI don’t have all that many professional regrets (which is surprising because I’ve made a lot of questionable choices), but I do regret not just making that appendix its own paper. Because it was really good work.\nBut anyway, I took the inverse problems39 work of Andrew Stuart and Masoumeh Dashti and extended it out to meet my needs. And to that end, I’m going to bring out a small corner of that appendix because it tells us what happens to a posterior when we replace a Gaussian process by a finite dimensional approximation."
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#how-do-we-measure-if-a-posterior-approximation-is-good",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#how-do-we-measure-if-a-posterior-approximation-is-good",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "How do we measure if a posterior approximation is good?",
    "text": "How do we measure if a posterior approximation is good?\nPart of the struggle when you’re working with Gaussian processes as actual objects rather than as a way to generate a single finite-dimensional Gaussian distribution that you use for analysis is that, to quote Cosma Shalizi40, “the topology of such spaces is somewhat odd, and irritatingly abrupt”. Or to put it less mathematically, it is hard to quantify which Gaussian processes are close together.\nWe actually saw this in the last blog where we noted that the distribution of \\(v = cu\\) has no common support with the distribution of \\(u\\) if \\(|c| \\neq 1\\). This means, for instance, that the total variation between \\(u\\) and \\(v\\) is 2 (which is it’s largest possible value) even if \\(c = 1 + \\epsilon\\) for some tiny \\(\\epsilon\\).\nMore generally, if you’re allowed to choose what you mean by “these distributions are close” you can get a whole range of theoretical results for the posteriors of infinite dimensional parameters, ranging from this will never work and Bayes in bullshit to everything is wonderful and you never have to worry.\nSo this is not a neutral choice.\nIn the absence of a neutral choice, we should try to make a meaningful one! An ok option for that is to try to find functions \\(G(u)\\) that we may be interested in. Classically, we would choose \\(G\\) to be bounded (weak41 convergence / convergence in distribution) or bounded Lipschitz42. This is good but it precludes things like means and variances, which we would quite like to converge!\nThe nice thing about everything being based off a Gaussian process is that we know43 that there is some \\(\\epsilon &gt; 0\\) (which may be very small) such that \\[\n\\mathbb{E}_{\\mu_0}\\left(\\mathrm{e}^{\\epsilon \\|u\\|_B^2}\\right) &lt; \\infty.\n\\] This suggests that as long as the likelihood isn’t too evil, the posterior will also have a whole arseload of moments.\nThis is great because it suggests that we can be more ambitious than just looking at bounded Lipschitz functions. It turns out that we can consider convergence over the class of functionals \\(G\\) such that \\[\n|G(u) - G(v)| \\leq L(u) \\|u - v\\|_B,\n\\] where \\(\\mathbb{E}_{\\mu_0}(L(u)) &lt; \\infty\\). Critically this includes functions like moments of \\(\\|u\\|_B\\) and, assuming all of the functions in \\(B\\) are continuous, moments of \\(u(s)\\). These are the functions we tend to care about!"
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#convergence-of-finite-dimensional-gaussian-processes",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#convergence-of-finite-dimensional-gaussian-processes",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "Convergence of finite dimensional Gaussian processes",
    "text": "Convergence of finite dimensional Gaussian processes\nIn order to discuss the convergence of finite dimensional Gaussian processes, we need to define them and, in particular, we need to link them to some Gaussian process on \\(B\\) that they are approximating.\nLet \\(u\\) be a Gaussian process supported on a Banach space \\(B\\). We define a finite dimensional Gaussian process to be a Gaussian process supported on some space \\(V_m \\subset B\\) that satisfies \\[\nu_n = R_m u,\n\\] where \\(R_m: B \\rightarrow V_m\\) is some operator. (For this to be practical we want this to be a family of operators indexed by \\(m\\).)\nIt will turn out that how this restriction is made is important. In particular, we are going to need to see how stable this restriction is. This can be quantified by examining \\[\n\\sup_{\\|f\\|_V = 1} \\|R_m f\\|_{B} \\leq A_m \\|f\\|_V,\n\\] where \\(A_m &gt; 0\\) is a constant that could vary with \\(m\\) and \\(V \\subseteq B\\) is some space we will talk about later. (Confusingly, I have set up the notation so that it’s not necessarily true that \\(V_m \\subset V\\). Don’t hate me because I’m pretty, hate me because I do stupid shit like that.)\n\nExample 1: An orthogonal truncation\nThere is a prototypical example of \\(R_m\\). Every Gaussian process on a separable Banach space admits a Karhunen-Loève representation \\[\nu = \\sum_{k = 0}^\\infty \\lambda_k^{1/2} z_k \\phi_k,\n\\] \\(z_k\\) are iid standard normal random variables and \\((\\lambda_k, \\phi_k)\\) are the eigenpairs44 of the covariance operator \\(C_u\\). The natural restriction operator is then \\[\nR_m f = \\sum_{j=0}^m \\langle f, \\phi_j\\rangle_{L^2}\\phi_j.\n\\] This was the case considered by Dashti and Stuart in their 2011 paper. Although it’s prototypical, we typically do not work with the Karhunen-Loève basis directly, as it tends to commit us to a domain \\(\\Omega\\). (Also because we almost45 never know what the \\(\\phi_j\\) are.)\nBecause this truncation is an orthogonal projection, it follows that we have the stability bound with \\(A_m = 1\\) for all \\(m\\).\n\n\nExample 2: Subset of regressors\nMaybe a more interesting example is the subset of regressors46. In this case, there are a set of inducing points \\(s_1, \\ldots, s_m\\) and \\[\nR_m f = \\sum_{j=1}^m w_j r_u(\\cdot, s_j),\n\\] where the weights solve \\[\nK_m w = b,\n\\] \\([K_m]_{ij} = r_u(s_i, s_j)\\) and \\(b_j = f(s_j)\\).\nIt’s a bit harder to get the stability result in this case. But if we let \\(V_m\\) have the RKHS47 norm, then \\[\\begin{align*}\n\\|R_m f\\|^2_{H_u} &= w^TK_m w \\\\\n&= b^T K^{-1} b \\\\\n&\\leq \\|K^{-1}\\|_2 \\|b\\|_2^2\n\\end{align*}\\]\nAssuming that \\(B\\) contains continuous functions, then \\(\\|b\\|_2 \\leq C\\sqrt{m} \\|f\\|_B\\). I’m pretty lazy so I’m choosing not to give a shit about that \\(\\sqrt{m}\\) but I doubt it’s unimprovable48. To be honest, I haven’t thought deeply about these bounds, I am doing them live, on my couch, after a couple of red wines. If you want a good complexity analysis of subset of regressors, google.\nMore interestingly, \\(\\|K_m^{-1}\\|_2\\) can be bounded, under mild conditions on the locations of the \\(s_j\\) by the \\(m\\)-th largest eigenvalue of the operator \\(Kf = \\int_\\Omega r_u(s,t)f(t)\\,dt\\). This eigenvalue is controlled by how differentiable \\(u\\) is, and is roughly \\(\\mathcal{O}\\left(m^{-\\alpha - d/2}\\right)\\) if \\(u\\) has a version with \\(\\alpha\\)-almost sure (Hölder) derivatives. In the (common) case where \\(u\\) is analytic (eg if you used the squared exponential covariance function), then this bound increases exponentially (or squared exponentially for the squared exponential) in \\(m\\).\nThis means that the stability constant \\(A_m \\geq \\|K_m^{-1}\\|\\) will increase with \\(m\\), sometimes quite alarmingly. Wing argues that it is always at least \\(\\mathcal{O}(m^2)\\). Wathan and Zhu have a good discussion for the one-dimensional case and a lot of references to the more general situation.\n\n\nExample 3: The SPDE method\nMy personal favourite way to approximate Gaussian processes works when they are Markovian. The Markov property, in general, says that if, for every49 set of disjoint open domains \\(S_1\\) and \\(S_2 = \\Omega \\backslash \\bar S_1\\) such that \\(S_1 \\cup \\Gamma \\cup S_2\\), where \\(\\Gamma\\) is the boundary between \\(S_1\\) and \\(S_2\\), then \\[\n\\Pr(A_1 \\cup A_2 \\mid B_\\epsilon) = \\Pr(A_1 \\mid B_\\epsilon) \\Pr(A_2 \\mid B_\\epsilon),\n\\] where \\(A_j \\in \\sigma\\left(\\{u(s), s \\in S_j\\}\\right)\\) and50 \\(B_\\epsilon \\in \\sigma\\left(\\{u(s); d(s, \\Gamma) &lt; \\epsilon\\}\\right)\\) and \\(\\epsilon&gt;0\\).\nWhich is to say that it’s the normal Markov property, but you may need to fatten out the boundary between disjoint domains infinitesimally for it to work.\nIn this case, we51 know that the reproducing kernel Hilbert space has the property that the inner product is local. That means that if \\(f\\) and \\(g\\) are in \\(H_u\\) and have disjoint support52 then \\[\n\\langle f, g\\rangle_{H_u} = 0,\n\\] which, if you squint, implies that the precision operator \\(\\mathcal{Q}\\) is a differential operator. (That the RKHS inner product being local basically defines the Markov property.)\nWe are going to consider a special case53, where \\(u\\) solves the partial differential equation \\[\nL u = W,\n\\] where \\(L\\) is some differential operator and \\(W\\) is white noise54.\nWe make sense of this equation by saying a Gaussian process \\(u\\) solves it if \\[\n\\int_\\Omega \\left(L^*\\phi(s)\\right)\\left( u(s)\\right)\\,ds  \\sim N\\left(0, \\int_\\Omega \\phi^2(s)\\,ds\\right),\n\\] for every smooth function \\(\\phi\\), where \\(L^*\\) is the adjoint of \\(L\\) (we need to do this because, in general, the derivatives of \\(u\\) could be a bit funky).\nIf we are willing to believe this exists (it does—it’s a linear filter of white noise, electrical engineers would die if it didn’t) then \\(u\\) is a Gaussian process with zero mean and covariance operator \\[\n\\mathcal{C} = (L^*L)^{-1},\n\\] where \\(L^*\\) is the adjoint of \\(L\\).\nThis all seems like an awful lot of work, but it’s the basis of one of the more powerful methods for approximating Gaussian processes on low-dimensional spaces (or low-dimensional manifolds). In particular in 1-3 dimensions55 or in (1-3)+1 dimensions56 (as in space-time), Gaussian processes that are built this way can be extremely efficient.\nThis representation was probably first found by Peter Whittle and Finn Lindgren, Johan Lindström and Håvard Rue combined it with the finite element method to produce the SPDE method57 A good review of the work that’s been done can be found here. There’s also a whole literature on linear filters and stochastic processes.\nWe can use this SDPE representation of \\(u\\) to construct a finite-dimensional Gaussian process and a restriction operator \\(R_m\\). To do this, we define \\(L_m\\) as the operator defined implicitly through the equation \\[\n\\langle \\phi, L\\psi\\rangle_{L^2} = \\langle \\phi, L_m\\psi\\rangle_{L^2}, \\quad \\forall \\phi,\\psi \\in V_m.\n\\] This is often called the Galerkin projection58. It is at the heart of the finite element method for solving elliptic partial differential equations.\nWe can use \\(L_m\\) to construct a Gaussian process with covariance function \\[\n\\mathcal{C}_m = (L_m^*L_m)^\\dagger,\n\\] where \\(^\\dagger\\) is a pseudo-inverse59.\nIt follows that \\[\n\\mathcal{C}_m  =(L_m)^\\dagger L L^{-1}(L^*)^{-1}L^*(L_m^*)^\\dagger = R_m \\mathcal{C} R_m^*,\n\\] where \\(R_m = L_m^\\dagger L\\).\nBefore we can get a stability estimate, we definitely need to choose our space \\(V_m\\). In general, the space will depend on the order of the PDE60, so to make things concrete we will work with second-order elliptic61 PDE \\[\nLu = -\\sum_{i,j = 1}^d\\frac{\\partial}{\\partial s_j}\\left(a_{ij}(s) \\frac{\\partial u}{\\partial s_i} \\right) +\\sum_{i=1}^d b_i\\frac{\\partial u}{\\partial s_i} + b_0(s)u(s),\n\\] where all of the \\(a_{ij}(s)\\) and \\(b_j(s)\\) are \\(L^\\infty(\\Omega)\\) and the uniform ellipticity condition62 holds.\nThese operators induce (potentially non-stationary) Gaussian processes that have continuous versions as long as63 \\(d \\leq 3\\).\nWith this fixed, the natural finite element space to use is the space of continuous piecewise linear functions. Traditionally, this is done using combinations of tent functions on a triangular mesh.\n\n\n\nA piecewise linear approximation. Source\n\n\nWith this basis, we can get stability estimates by defining \\(v\\) and \\(v_m\\) by \\(Lv = f\\) and \\(L_m v_m = f_m\\), from which we get \\[\n\\|R_m v\\|_B = \\|v_m\\|_{V_m} \\leq A\\|f\\|_{L_2}\n\\] which holds, in particular, when the \\(L\\) has no first order derivatives64.\nAn oddity about this structure is that functions in \\(V_m\\) are not not continuously differentiable, while the sample paths of \\(u\\) almost surely are65. This means that \\(V_m\\) isn’t necessarily a subset of \\(B\\) as we would naturally define it. In this case, we need to inflate \\(B\\) to be big enough to contain the \\(V_m\\). So instead of taking \\(B = C^1(\\Omega)\\), we need to take \\(B = C(\\Omega)\\) or \\(B = L^2(\\Omega)\\).\nThis has implications on the smoothness assumptions on \\(\\Phi(u;y)\\), which will need to hold uniformly over \\(B\\) and \\(V_m\\) if \\(V_m \\not \\subset B\\) and on the set of functionals \\(G(u)\\) that we use to measure convergence.\n\n\nA bit of perspective\nThe critical difference between the SPDE method and the subset-of-regressors approximation is that for the SPDE method, the stability constant \\(A_m = A\\) is independent of \\(m\\). This will be important, as this constant pops up somewhere important when we are trying to quantify the error in the finite dimensional approximation.\nOn the other hand, the SPDE method only works in three and fewer dimensions and while it allows for quite flexible covariance structures66, it is can only directly construct Gaussian processes with integer numbers of continuous derivatives. Is this a problem? The asymptotics say yes, but they only hold if we are working with the exact Gaussian process (or, I guess, if we let the dimension of \\(V_m\\) hurtle off towards infinity as we get more and more data).\nIn practice, the Gaussian processes constructed via SPDE methods perform very well on real data67. I suspect part of this is that the stable set of basis functions are very good at approximating functions and the misspecification error plays off against the approximation error."
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#bounding-the-approximation-error",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#bounding-the-approximation-error",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "Bounding the approximation error",
    "text": "Bounding the approximation error\nWith all of this setup, we are finally ready to bound the error between the posterior we would get with the full Gaussian process prior and the posterior we would get using the finite dimensional Gaussian process prior.\nWe are going to deal with a simpler scenario than the paper we are (sort of) following, because in that situation, I was forced to deal with simultaneously approximating the likelihood and honestly who needs that trouble.\nTo remind ourselves, we have two priors: the full fat Gaussian process prior, the law of which we denote \\(\\mu_0\\) and the one we could possibly work with \\(\\mu_0^m\\). These lead to two different posteriors \\(\\mu_y\\) and \\(\\mu_y^m\\) given by \\[\n\\frac{d\\mu_y}{d\\mu_0}(u) = \\frac{1}{Z}\\mathrm{e}^{-\\Phi(u;y)} \\quad \\text{and}\\quad \\frac{d\\mu_y^m}{d\\mu_0^m}(u) = \\frac{1}{Z_m}\\mathrm{e}^{-\\Phi(u;y)} ,\n\\] where \\(Z_1\\) and \\(Z_m\\) are normalising constants.\nWe assume that the Gaussian process \\(u\\) is supported on some Banach space \\(V \\subseteq B\\) and the approximating spaces \\(V_m \\subset B\\). This covers the case where the approximating functions are rougher than the true realisations of the Gaussian process we are approximating. With this notation, we have the restriction operator \\(R_m\\) that satisfies \\[\n\\|R_mf\\|_{V_m} \\leq A_m \\|f\\|_V,\n\\] which is a slightly more targeted bound when \\(B\\) is larger than \\(V\\).\nWe will make the following assumptions about the negative log-likelihood (or potential function) \\(\\Phi\\): For every \\(\\epsilon &gt; 0\\), \\(r&gt; 0\\), and68 \\(\\|y\\| &lt; r\\), there exist positive constants \\(C_1, C_2, C_3, C_4\\) that may depend on \\(\\epsilon\\) and \\(r\\) such that the following 4 conditions hold. (Note: when the norm isn’t specified, we want it to hold over both the \\(V\\) and \\(B\\) norms.)\n\nFor all \\(u \\in V \\cup \\left(\\bigcup_{m\\geq 1} V_m\\right)\\) \\[\n\\Phi(u;y) \\geq C_1 - \\epsilon \\|u\\|^2\n\\]\nFor every \\(u\\in B\\), \\(y \\in Y\\) with \\(\\max \\{\\|u\\|, \\|y\\|_Y\\} &lt; r\\), \\[\n\\Phi(u;y) \\leq C_2\n\\]\nFor every \\(\\max \\{\\|u_1\\|_V, \\|u_2\\|_B, \\|y\\|_Y\\} &lt; r\\), \\[\n|\\Phi(u_1; y) - \\Phi(u_2; y )| \\leq \\exp\\left(\\epsilon\\max\\{\\|u_1\\|_V^2, \\|u_2\\|_B^2\\} - C_3\\right) \\|u_1 - u_2\\|_B\n\\]\nFor every \\(u\\in B\\) and \\(\\max \\{\\|y_1\\|_Y, \\|y_2\\|_Y\\} &lt; r\\), \\[\n|\\Phi(u; y_1) - \\Phi(u; y_2) | \\leq  \\exp\\left(\\epsilon \\|u\\|^2 + C_4\\right)\\|y_1 - y_2\\|_Y\n\\]\n\nThese restrictions are pretty light and are basically what are needed to make sure the posteriors exist. The first one say “don’t grow too fast” to the likelihood and is best explained while humming ABBA’s Slipping Through My Fingers. The second one makes sure the likelihood isn’t zero. The third and fourth are Lipschitz conditions that basically make sure that a small change in \\(u\\) (or \\(y\\)) doesn’t make a big change in the likelihood. It should be pretty clear that if that could happen, the two posteriors wouldn’t be close.\nWe are also going to need some conditions on our test functions. Once again, we need them to apply over \\(V\\) and \\(B\\) when no space is specified for the norm.\n\nFor all \\(u \\in V\\), \\(G(u) = \\exp(\\epsilon \\|u\\|^2_V+ C_5)\\)\nFor all \\(u_1 \\in V\\), \\(u_2 \\in V_m\\), \\[\n|G(u_1) - G(u_2)| \\leq \\exp(\\epsilon\\max\\{\\|u_1\\|^2_V, \\|u_2\\|^2_B\\})\\|u_1 - u_2\\|_B.\n\\]\n\nUnder these conditions, we get the following theorem, which is a simplified version of Theorem A2 here.\n\nTheorem 1 Under the above assumptions, \\[\n\\left|\\mathbb{E}_{\\mu_y}(G(u)) - \\mathbb{E}_{\\mu^m_y}(G(u_m))\\right| \\leq C_m \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right),\n\\] where \\(C_m\\) only depends on \\(m\\) through \\(A_m\\).\n\nI seriously doubt that the dependence on \\(A_m\\) is exponential, as it is in the proof, but I’m not going to try to track that down. That said, I’m also quite sure that the dependence \\(C_m\\) is not uniform in \\(m\\) unless \\(A_m\\) is constant.\nIt’s also worth noting that there’s nothing special about \\(G\\) being real-valued. In general it can take values in any Banach space \\(E\\). Just replace all those absolute values with norms. That means that the result covers convergence of approximations to things like covariance matrices.\n\n\nProof, if you’re interested\n\nWe are interested in approximating \\[\ne_G = \\left|\\mathbb{E}_{\\mu_y}(G(u)) - \\mathbb{E}_{\\mu^m_y}(G(u_m))\\right|.\n\\] We can expand this to get \\[\\begin{align*}\ne_G \\leq & \\frac{1}{Z}\\left|\\mathbb{E}_{\\mu_0}\\left(G(u)\\exp(-\\Phi(u;y))\\right)\n- \\mathbb{E}_{\\mu_0^m}\\left(G(u_m)\\exp(-\\Phi(u_mm;y))\\right)\\right| \\\\\n&\\quad +\n\\left|\\frac{1}{Z}\n- \\frac{1}{Z_m}\\right|\\mathbb{E}_{\\mu_0^m}\\left(|G(u_m)|\\exp(-\\Phi(u_m;y))\\right). \\\\\n&= B_1 + B_2.\n\\end{align*}\\]\nIt follows from Andrew Stuart’s work that the normalising constants \\(Z\\) and \\(Z_m\\) can be bounded above and below independently of \\(m\\), so the above expression makes sense.\nWe will now attack \\(B_1\\) and \\(B_2\\) separately. To do this, we need to consider the joint prior \\(\\lambda_0(u, u_m)\\) that is the joint law of the Gaussian process \\(u\\) and its finite dimensional approximation \\(u_m = R_m u\\).\nFor \\(B_1\\) we basically use the same trick again. \\[\\begin{align*}\nZB_1 \\leq & \\mathbb{E}_{\\lambda_0}\\left(|G(u)|\\left|\\exp(-\\Phi(u;y)) -\\exp(\\Phi(u;y))\\right| \\right) \\\\\n&\\quad + \\mathbb{E}_{\\lambda_0}\\left(\\exp(-\\Phi(u_m;y)) | G(u) - G(u_m)|\\right) \\\\\n&\\leq  \\mathbb{E}_{\\lambda_0}\\left(\\mathrm{e}^{C_5 + \\epsilon \\|u\\|_V^2}\\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_v^2 - C_1} \\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_V^2 + C_3}\\|u - u_m\\|_B\\right) \\\\\n& \\quad +\\mathbb{E}_{\\lambda_0}\\left(\\mathrm{e}^{\\epsilon A_m\\|u\\|_V^2 - C_1}\\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_V^2 + C_6}\\|u - u_m\\|_V\\right) \\\\\n&\\leq \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right)\n\\mathrm{e}^{C_3 + C_5 + C_6 -2 C_1}\\mathbb{E}_{\\mu_0}\\left(\\|u\\|_V\\mathrm{e}^{(1+3\\max\\{1,A_m\\} + A_m)\\epsilon \\|u\\|_V^2}\\right)\\\\\n&\\leq C_7 \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right),\n\\end{align*}\\] where the second inequality comes from using all of the assumptions on \\(\\Phi\\) and \\(G\\) and noting that \\(\\left|e^{-x} - e^{-y}\\right| \\leq e^{-\\min\\{x,y\\}}|x-y|\\); and the final inequality comes from Fernique’s theorem, which implies that expectation is finite.\nWe can also bound \\(B_2\\) by noting that \\[\\begin{align*}\n\\left|Z^{-1} - Z_m^{-1} \\right| & \\leq \\max \\{Z^{-2}, Z_m^{-2}\\}\\mathbb{E}_{\\lambda_0}\\left(|\\exp(-\\Phi(u;y)) - \\exp(-\\Phi(u_m;z))\\right) \\\\\n&\\leq C_8 \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right)\n\\end{align*}\\] by the same reasoning as above."
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#dealing-with-the-approximation-error",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#dealing-with-the-approximation-error",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "Dealing with the approximation error",
    "text": "Dealing with the approximation error\nThe theorem above shows that the worst-case error in posterior functionals caused by replacing a Gaussian process \\(u\\) with it’s approximation \\(u_m = R_m u\\) is driven entirely by how well a general function from \\(V\\) can be approximated by a function in \\(V_m\\). This is not really a surprising result: if the approximation \\(u_m\\) is unable to approximate the sample paths of \\(u\\) it is very unlikely it will do a good job with all functionals.\nThankfully, approximation error is one of the better studied things in this world. Especially in the case where \\(V = B\\).\nFor instance, it’s pretty easy to show69 that if \\(u\\) has \\(\\nu\\) derivatives, then \\(e_G \\leq Cm^{-\\frac{\\nu}{d} + \\epsilon}\\) for all \\(\\epsilon&gt;0\\).\nIf you dive deep enough into the literature, you can get similar results for the type of approximation underneath the subset of regressors approximation.\nFor the SPDE approximation, it’s all a little bit more tricky as \\(V_m \\not \\subset V\\). But ultimately, you get that, for any \\(\\epsilon &gt;0\\), \\(e_G \\leq C h^{1-\\epsilon}\\), where \\(h\\) is a measure of the mesh size70. This is roughly what you’d expect, there’s a loss of \\(\\epsilon\\) from the ordinary interpolation rate which may or may not be a result of me being a bit shit at maths.\nThe argument that gets us here is really cute so I’ll sketch it below. This is here for two reasons: firstly, because I think it’s cool and secondly because the paper is so compressed it’s hard to completely follow the argument, so I thought it would be nice to put on here. (It also took me a whole afternoon to decipher the proof in the paper, which is usually a sign that it could do with a bit of a re-write. How successfully I clarified it is something I will leave up to others to decide.)\n\n\nFinite element shit\n\nSetup. Gird yourselves!\nWe are going to bound that error rate in a way that’s relevant for the finite element method. The natural choices for the function spaces are \\(V = H^{1-\\epsilon}(\\Omega)\\) for some fixed \\(0 &lt; \\epsilon &lt; 1/2\\) (close to zero is what we want). and \\(B = L^2(\\Omega)\\). (To be honest the domain \\(\\Omega\\) isn’t changing so I’m gonna forget it sometimes.)\nOnce again, we’re going to assume that \\(L\\) is a second order uniformly elliptic PDE with no first-order terms (aka \\(b_1 = \\cdots = b_d = 0\\)) and that \\(b_0(s) &gt;0\\) on some subset of \\(\\Omega\\). We will use the symmetric, coercive bilinear form associated71 with \\(L\\), which we can define, for any \\(u,v \\in H^1\\), as \\[\na(u, v) = \\int_\\Omega (A(s)\\nabla u(s))\\cdot \\nabla v(s)\\,ds + \\int_\\Omega b_0(s) u(s)v(s)\\,ds\n\\]\nRemembering that \\(R_m = LL_m^\\dagger\\), we have \\[\n\\sup_{v \\in V}\\frac{ \\left\\|v - R_m v\\right\\|_B}{\\|v\\|_V} =\\sup_{f\\in LV}\\frac{ \\left\\|L^{-1}f - L_n^{\\dagger}f\\right\\|_B}{\\|L^{-1}f\\|_V}.\n\\]\nThe set of functions \\(f \\in LV\\) is the set of all functions \\(f = Lv\\) for some \\(v \\in V\\). It can be shown that \\(LV  = H^{-1-\\epsilon}\\), where the negative index indicates a dual Sobolev space (aka the space of continuous linear functionals on \\(H^{1+  \\epsilon}\\)).\nThis means that we are looking at the difference between the solution to \\(Lu = f\\) and \\(L_m u_m = f_m\\), where \\(f_m\\) is the \\(L^2\\)-orthogonal projection of \\(f\\) onto \\(V_m\\), which is the space of piecewise linear functions on some72 triangular mesh \\(\\mathcal{T}_m\\).\nWe define the projection of the function \\(f \\in H^{-1-\\epsilon}(\\Omega)\\) onto \\(V_m\\) as the unique function \\(f_m \\in V_m\\) such that73 \\[\n\\int_\\Omega f_n(s) v_n(s)\\,ds = \\int f(s) v_n(s)\\,ds, \\quad \\forall v_n \\in V_n.\n\\]\nNow let’s do this!\nWith all of this in place, we can actually do something. We want to bound \\[\n\\frac{\\|u - u_m\\|_{L^2}}{\\|u\\|_{H^{1+\\epsilon}}},\n\\] where74 \\(a(u, \\phi) = \\int_\\Omega f(s) \\phi(s)\\,ds\\) for all \\(\\phi \\in H^{1+\\epsilon}\\) and \\(a(u_m, \\phi_m) = \\int_\\Omega f(s) \\phi_m(s)\\,ds\\) for all \\(\\phi_m \\in V_m \\subset H^{1+\\epsilon}\\).\nThe key observation is that \\[\n\\int_\\Omega f(s) \\phi_m(s)\\,ds = \\int_\\Omega f_m(s) \\phi_m(s)\\,ds,\n\\] which suggests that \\(u_m(s)\\) is an approximation to two different problems!\nLet’s write this second problem down! We want to find \\(z^{(m)}\\) such that \\[\na({z}^{(m)}, \\phi) = \\int_\\Omega f_n(s) \\phi(s)\\,ds \\quad \\forall \\phi \\in H^{1} ,\n\\] where the \\(m\\) superscript indicates that it depends on \\(m\\) through it’s right hand side. The projection \\(f_n \\in L^2\\), which means that we are in the realm of usual PDEs and (assuming some regularity) \\(z^{(m)} \\in H^2\\).\nHence, we can write \\[\n\\|u - u_m\\|_{L^2}\\leq \\|u - z^{(m)}\\|_{L^2} + \\|z^{(m)} - u_m\\|_{L^2}.\n\\]\nWe can bound the second term almost immediately from standard finite element theory, which says that \\[\n\\|z^{(m)} - u_m\\|_{L^2} \\leq Ch^2 \\|f_n\\|_{L^2}.\n\\]\nTo estimate \\(\\|f_m\\|\\) we use the inverse estimates of Ben Belgacem and Brenner to show that, for any \\(v\\in L^2(\\Omega)\\), \\[\n\\int_\\Omega f_m(s) v(s) \\,ds = \\int_\\Omega f(s)v_m(s)  \\,ds\\leq\\|f\\|_{H^{-1-\\epsilon}}\\|v_m\\|_{H^{1+\\epsilon}} \\leq Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}} \\|v\\|_{L^2},\n\\] where \\(v_m\\) is the orthogonal projection of \\(v\\) onto \\(V_m\\).\nIf we set \\(v = f_m\\) in the above equation, we get \\(\\|f_m\\|_{L^2} \\leq Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}}\\), which combines with our previous estimate to give \\[\n\\|z^{(m)} - u_m\\|_{L^2} \\leq Ch^{1-\\epsilon} \\|f_n\\|_{L^2}.\n\\]\nFinally, to bound \\(\\|u - z^{(m)}\\|_{L^2}\\) we are going to use one of my75 favourite arguments. Fix \\(w \\in L^2\\) and let \\(W\\) be the solution of the dual equation \\(a(\\phi, W) = \\int_\\Omega \\phi(s)w(s)\\,ds\\). It then follows that, for any \\(v_m \\in V_m\\), \\[\\begin{align*}\n\\left|\\int_\\Omega (u(s) - z^{(m)}(s))w(s)\\,ds\\right| &= \\left|a(u - z^{(m)}, W)\\right| \\\\\n&= \\left|\\int_\\Omega (f(s) - f_m(s))W(s)\\,ds\\right|\\\\\n&= \\left|\\int_\\Omega (f(s) - f_m(s))(W(s) - v_m(s))\\,ds\\right|\\\\\n&\\leq\\left|\\int_\\Omega f(s)(W(s) - v_m(s))\\,ds\\right|+  \\left|\\int_\\Omega f_m(s)(W(s) - v_m(s))\\,ds\\right| \\\\\n&\\leq \\|f\\|_{H^{-1-\\epsilon}}\\|W - v_m\\|_{H^{1+\\epsilon}} + Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}} \\|W - v_m\\|_{L^2} \\\\\n&\\leq C \\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon}\\left(h^{1+\\epsilon}\\|W - v_m\\|_{H^{1+\\epsilon}} +  \\|W - v_m\\|_{L^2} \\right),\n\\end{align*}\\] where the first line uses the definition of \\(W\\); the second uses the definition of \\(u\\) and \\(z^{(m)}\\); the third uses the fact that \\((f - f_m) \\perp V_m\\) so subtracting off \\(v_m\\) doesn’t change anything; the fourth is the triangle inequality; the fifth is the Hölder inequality on the left and the estimate from half a screen up on the right; and the sixth line is clean up.\nBecause the above bound holds for any \\(v_m \\in V_m\\), we can choose the one that makes the bound the smallest. This leads to \\[\\begin{align*}\n\\left|\\int_\\Omega (u(s) - z^{(m)}(s))w(s)\\,ds\\right| &\\leq  C \\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon}\\inf_{v \\in V_m}\\left(h^{1+\\epsilon}\\|W - v_m\\|_{H^{1+\\epsilon}} +  \\|W - v_m\\|_{L^2} \\right) \\\\\n& \\leq C\\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon} h^2 \\|W\\|_{H^2}\\\\\n&\\leq C h^{1-\\epsilon} \\|w\\|_{L^2},\n\\end{align*}\\] where the last two inequalities are Theorem 14.4.2 from Brenner and Scott and a standard estimate of the solution to an elliptic PDE by it’s RHS.\nPutting this all together we get the result. Phew.\nThis whole argument was a journey, but I think it’s quite pretty. It’s clobbered together from a lot of sleepless nights and an argument inspired by strip-mining76 a Ridgeway Scott paper from 1976. Anyway, I think it’s nifty."
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#wrapping-it-up",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#wrapping-it-up",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "Wrapping it up",
    "text": "Wrapping it up\nSo. That was quite a lot. I enjoyed it, but I’m weird like that. This has mostly been me trying to remember what I did in 2015. Why? Because I felt like it.\nI also think that there’s some value in this way of thinking about Gaussian processes and it’s nice to show off some ways to use all of that weird shit in the last post.\nAll of these words can be boiled down to this take away:\n\nIf your finite dimensional GP \\(u_m\\) is linked to a GP \\(u\\) by some (potentially non-linear relationship) \\(u_m= R_m u\\), then the posterior error will be controlled by how well you can approximate a function \\(v\\) that could be a realisation of the GP by \\(R_m v\\).\n\nThis is a very intuitive result if you are already thinking of GP approximation as approximating a random function. But a lot of the literature takes a view that we are approximating a covariance matrix or a multivariate normal. This might be enough to approximate a maximum likelihood estimator, but it’s insufficient for approximating a posterior77\nFurthermore, because most of the constants in the bounds don’t depend too heavily on the specific finite dimensional approximation (except through \\(A_m\\)), we can roughly say that if we have two methods for approximating a GP, the one that does a better job at approximating functions will be the better choice.\nAs long as it was, this isn’t a complete discussion of the problem. We have not considered hyper-parameters! This is a little bit tricky because if \\(\\mu_0\\) depends on parameters \\(\\theta\\), then \\(R_m\\) will also depend on parameters (and for subset of regressors, \\(V_m\\) also depends on the parameters).\nIn theory, we could use this to bound the error in the posterior \\(p(\\theta \\mid y)\\). To see how we would do that, let’s consider the case where we have Gaussian observations.\nThen we get \\[\\begin{align*}\np(\\theta \\mid y) & \\frac{\\exp(-\\Phi(u;y))}{p(y)} \\left[\\frac{d\\mu_y}{d\\mu_0}\\right]^{-1} p(\\theta) \\\\\n&= \\frac{Z(\\theta) p(\\theta)}{\\int_\\Theta Z(\\theta)p(\\theta)\\,d\\theta},\n\\end{align*}\\] where \\(Z(\\theta) = \\mathbb{E}_{\\mu_0}\\left(e^{-\\Phi(u;y)}\\right)\\).\nWe could undoubtedly bound the error in this using similar techniques to the ones we’ve already covered (in fact, we’ve already got a bound on \\(|Z - Z_m|\\)). And then it would just be a matter of piecing it all together.\nBut I’m tired and I just want to cry for me."
  },
  {
    "objectID": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#footnotes",
    "href": "posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#footnotes",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNaively: a condescending way to say “the way you were told to use them”↩︎\nIs it better to have a large amount of crappy data or a small amount of decent data? Depends on if you’re trying to impress people by being right or by being flashy.↩︎\nWho doesn’t love a good shape. Or my personal favourite: a point pattern.↩︎\nOr, hell, this is our information about how to query the Gaussian process to get the information we need for this observation. Because, again, this does not have to be as simple as evaluating the function at a point!↩︎\nThis could be time, space, space-time, covariate space, a function space, a lattice, a graph, an orthogonal frame, a manifold, a perversion, whatever. It doesn’t matter. It’s all just Gaussian processes. Don’t let people try to tell you this shit is fancy.↩︎\nThis could be covariate information, group information, hierarchy information, causal information, survey information, or really anything else you want it to be. Take a deep breath. Locate your inner peace. Add whatever you need to the model to make it go boop.↩︎\nI will never use this assumption. Think of it like the probability space at the top of a annals of stats paper.↩︎\nSo the thing is that this is here because it was funny to me when I wrote it, but real talk: just being like “it’s iid” is some real optimism (optimism, like hope, has no place in statistics.) and pretending that this is a light or inconsequential assumption is putting some bad energy out into the world. But that said, I was once a bit drunk at a bar with a subjective Bayesian (if you want to pick your drinking Bayesian, that’s not a bad choice. They’re all from The North) and he was screaming at me for thinking about what would happen if I had more data, and I was asking him quietly and politely how the data could possibly inform models as complex as he seemed to be proposing. And he said to me: what you do is you look for structures within your data that are exchangeable in some sense (probably after conditioning) and you use those as weak replicates. And, of course, I knew that but I’d never thought about it that way. Modelling, eh. Do it properly.↩︎\nThese (and the associated parenthetical girls) were supposed to be nested footnotes but Markdown is homophobic and doesn’t allow them. I am being oppressed.↩︎\nIt’s an interesting area, but the tooling isn’t there for people who don’t want to devote a year of their lives to this to experiment.↩︎\nThis is what matrix nerds say when they mean “I love you”. Or when they mean that it’s all derived from the structure of a matrix rather than from some structural principles stolen from the underlying problem. The matrix people are complicated.↩︎\nThe reason for this is that, while there are clever methods for getting determinants of H-matrices, they don’t actually scale all that well. So Geoga, Anitescu, and Stein paper use a Hutchinson estimator of the log-determinant. This has ok relative accuracy, but unfortunately, we need it to have excellent absolute accuracy to use it in a Bayesian procedure (believe me, I have tried). On the other hand, the Hutchinson estimator of the gradient of the log-determinant is pretty stable and gives a really nice approximate gradient. This is why MLE type methods for learning the hyper-parameters of a GP can be made scalable with H-matrix techniques.↩︎\nOtherwise, why bother. Just sub-sample and get on the beers. Or the bears. Or both. Whatever floats your boat.↩︎\non \\(u\\) and probably other parameters in the model↩︎\nDual spaces, y’all. This vector was inevitable because \\(m\\)-dimensional row vectors are the dual space of \\(\\mathbb{R}^m\\), while \\(s_i \\rightarrow u(s_i)\\) is in \\(B^*\\).↩︎\nThis is not surprising if you’re familiar with the sketching-type bounds that Yang, Pilanci and Wainwright did a while back (or, for that matter, with any non-asymptotic bounds involving the the complexity of the RKHS). Isn’t maths fun.↩︎\nHölder↩︎\nThink “infinitely differentiable but more so”.↩︎\nAn analytic function is one that you know will walk straight home from the pub, whereas a \\(\\nu\\)-differentiable function might just go around the corner, hop on grindr, and get in a uber. Like he’s not going to go to the other side of the city, but he might pop over to a nearby suburb. A generalised function texts you a photo of a doorway covered by a bin bag with a conveniently placed hole at 2am with no accompanying message other than an address↩︎\nI mean, I cannot be sure, but I’m pretty sure.↩︎\nAgain, not strictly necessary but it removes a tranche of really annoying technicalities and isn’t an enormous restriction in practice.↩︎\nThe result is that there is no non-trivial translation invariant measure on a separable Banach space (aka there is no analogue of the Lebesgue measure). You can prove this by using separability to make a disjoint cover of equally sized balls, realise that they would all have to have the same measure, and then say “Fuck. I’ve got too many balls”.↩︎\nBorel. Because we have assumed \\(B\\) is separable, the cylindrical \\(\\sigma\\)-algebra is identical to the Borel \\(\\sigma\\)-algebra and \\(\\mu_0\\) is a Radon measure. Party.↩︎\nSee Andrew Stuart’s long article on formulating Bayesian problems in this context and Masoumeh Dashti and Andrew Stuart’s paper paper on (simple) finite dimensional approximations.↩︎\nThe SPDE approach. Read on Macduff.↩︎\nthe Irish National Liberation Army↩︎\nThis covers GP models, GAMs, lots of spatial models, and a bunch of other stuff.↩︎\nLike, the data is a single observation of a point pattern. Or, to put it a different way, a list of (x,y) coordinates of (a priori) unknown length.↩︎\nApproximate Markovian GPs in 2-4 dimensions. See here for some info↩︎\nRue. The king of INLA. Another all round fabulous person. And a person foolish enough to hire me twice even though I was very very useless.↩︎\nIn the interest of accuracy, Janine and I were giving back to back talks at a conference that we decided for some reason to give as a joint talk and I remember her getting more and more agitated as I was sitting in the back row of the conference desperately trying to contort the innards of INLA to the form I needed to make the damn thing work. It worked and we had results to present. We also used the INLA software in any number of ways it had not been used before that conference. The talk was pretty well received and I was very relieved. It was also my first real data analysis and I didn’t know to do things like “look at the data” to check assumptions, so it was a bit of a clusterfuck and again Janine was very patient. I was a very useless 25 year old and a truly shit statistician. But we get better if we practice and now I’m a perfectly ok statistician.↩︎\nJanine and I, with Finn Lindgren, Sigrunn Sørbye and Håvard Rue, who were all heavily involved throughout but I’m sure I’ve already exhausted people’s patience.↩︎\nIIRC, Sigrunn’s university has one of those stupid lists where venue matters more than quality. Australia is also obsessed with this. It’s dumb.↩︎\nIn hindsight, the reviewer was asking for a simulation study, which is a perfectly reasonable thing to ask for but at the time I couldn’t work out how to do that because, in my naive numerical analyst ways, I thought we would need to compare our answer to a ground truth and I didn’t know how to do that. Now I know that the statistician way is to compute the same thing two different ways on exactly one problem that’s chosen pretty carefully and saying “it looks similar”.↩︎\nConditional on the log-intensity surface, a LGCP is a Poisson process↩︎\nis it, though↩︎\nMy co-authors are all very patient.↩︎\nwith fixed hyper-parameters↩︎\nThe thing about inverse problems is that they assume \\(\\Phi(u;y)\\) is the solution of some PDE or integral equation, so they don’t make any convenient simplifying assumptions that make their results inapplicable to LGCPs!↩︎\nhttps://arxiv.org/pdf/0901.1342.pdf↩︎\nstar↩︎\nAlso weak convergence but metrized by the Wasserstein-1 distance.↩︎\nFernique’s Theorem. I am using “we” very liberally here. Fernique knew and said so in French a while back. Probably the Soviet probabilists knew too but, like, I’m not going to write a history of exponential moments.↩︎\nOn \\(L^2\\), which is a Hilbert space so the basis really is countable. The result is a shit-tonne easier to parse if we make \\(B\\) a separable Hilbert space but I’m feeling perverse. If you want the most gloriously psychotic expression of this theorem, check out Theorem 7.3 here↩︎\nThere are tonnes of examples where people do actually use the Karhunen-Loève basis or some other orthogonal basis expansion. Obviously all of this theory holds over there.↩︎\nThis has many names throughout the literature. I cannae be arsed listing them. But Quiñonero-Candela, Rasmussen, and Williams attribute it to Wahba’s book in 1990.↩︎\nFunctions of the form \\(\\sum_{i=1}^m a_j r_u(\\cdot, s_j)\\) are in the RKHS corresponding to covariance function \\(r_u\\). In fact, you can characterise the whole space as limits of sums that look like that.↩︎\nI mean, we are not going to be using the \\(A_m\\) to do anything except grow with \\(m\\), so the specifics aren’t super important. Because this is a blog post.↩︎\nNot every. You do this for nice sets. See Rozanov’s book on Markov random fields if you care.↩︎\n\\(d(s, A) = \\inf_{s'\\in A} \\|s - s'\\|\\)↩︎\nRozanov↩︎\nThe sets on which they are non-zero are different↩︎\nFor general Markov random fields, this representation still exists, but \\(L\\) is no longer a differential operator (although \\(L^*L\\) must be!). All of the stuff below follows, probably with some amount of hard work to get the theory right.↩︎\nWhat is white noise? It is emphatically not a stochastic process that has the delta function as it’s covariance function. That thing is just ugly. In order to make any of this work, we need to be able to integrate deterministic functions with respect to white noise. Hence, we view it as an independently scattered random measure that satisfies \\(W(A) \\sim N(0, |A|)\\) and \\(\\int_A f(s)W(ds) \\sim N(0, \\int_A f(s)^2\\, ds)\\). Section 5.2 of Adler and Taylor’s book Random Fields and Geometry is one place to learn more.↩︎\nThis paper is a solid review↩︎\nThis paper↩︎\nFinite element methods had been used before, especially in the splines community, with people like Tim Ramsay doing some interesting work. The key insight of Finn’s paper was to link this all to corresponding infinite dimensional Gaussian processes.↩︎\nWe’re assuming \\(V_m\\subset L^2(\\Omega)\\), which is not a big deal.↩︎\nSee the paper for details of exactly which pseudo-inverse. It doesn’t really matter tbh, it’s just we’ve got to do something with the other degrees of freedom.↩︎\nConsult your favourite finite element book and then get pissed off it doesn’t cover higher-order PDEs in any detail.↩︎\nIt looks like this is vital, but it isn’t. The main thing that changes if your PDE is hyperbolic or parabolic or hypo-elliptic is how you do the discretisation. As long as the PDE is linear, this whole thing works in principle.↩︎\nFor some \\(\\alpha&gt;0\\), \\(\\sum_{i,j=1}^d w_iw_ja_{ij}(s) \\geq \\alpha \\sum_{i=1}^d w_i^2\\) holds for all \\(s \\in \\Omega\\).↩︎\nFor this construction to work in higher dimensions, you need to use a higher-order differential operator. In particular, if you want a continuous field on some subset of \\(\\mathbb{R}^d\\), you need \\(L\\) to be a differential operator of order \\(&gt;d/2\\) or higher. So in 4 dimensions, we need the highest order derivative to be at least 4th order (technically \\(L\\) could be the square root of a 6th order operator, but that gets hairy).↩︎\nIt holds in general, but if the linear terms are dominant (a so-called advection-driven diffusion), then you will need a different numerical method to get a stable estimate.↩︎\nModulo some smoothness requirements on \\(\\Omega\\) and \\(a_{ij}(s)\\).↩︎\nIt’s very easy to model weird anisotropies and to work on manifolds↩︎\neg this comparison↩︎\nLet’s not let any of the data fly off to infinity!↩︎\nCorollary A2 in the paper we’re following↩︎\nThink of it as the triangle diameter if you want.↩︎\nIntegration by parts gives us \\(\\int_\\Omega (Lu(s))v(s)\\,ds = a(u,v)\\) if everything is smooth enough. We do this to confuse people and because it makes all of the maths work.↩︎\nnot weird↩︎\n\\(f\\) is a generalised function so we are interpreting the integrals as duality pairings. This makes sense because \\(V_m \\subset H^{1+\\epsilon}\\) if we allow for a mesh-dependent embedding constant (this is why we don’t use \\(B = H^{1+\\epsilon}\\))↩︎\nThis is how fancy people define solutions to PDEs. We’re fancy.↩︎\nAlso everyone else’s, but it’s so elegantly deployed here. This is what I stole from Scott 1976)↩︎\nReal talk. I can sorta see where this argument is in the Scott paper, but I must’ve been really in the pocket when I wrote this because phew it is not an obvious transposition.↩︎\nUnless the approximation is very, very good. If we want to be pedantic, we’re approximating everything by floating point arithmetic. But we’re usually doing a good job.↩︎"
  },
  {
    "objectID": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html",
    "href": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html",
    "title": "Barry Gibb came fourth in a Barry Gibb look alike contest (Repost)",
    "section": "",
    "text": "Every day a little death, in the parlour, in the bed. On the lips and in the eyes. In the curtains in the silver, in the buttons, in the bread, in the murmurs, in the pauses, in the gestures, in the sighs. Sondheim\nThe most horrible sound in the world is that of a reviewer asking you to compare your computational method to another, existing method. Like bombing countries in the name of peace, the purity of intent drowns out the voices of our better angels as they whisper: at what cost.\nBefore the unnecessary drama of that last sentence1 sends you running back to the still-open browser tab documenting the world’s slow slide into a deeper, danker, more complete darkness that we’ve seen before, I should say that I understand that for most people this isn’t a problem. Most people don’t do research in computational statistics. Most people are happy2.\nSo why does someone asking for a comparison of two methods for allegedly computing the same thing fill me with the sort of dread usually reserved for climbing down the ladder into my basement to discover, by the the light of a single, swinging, naked light bulb, that the evil clown I keep chained in the corner has escaped? Because it’s almost impossible to do well."
  },
  {
    "objectID": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html#option-1-we-want-to-fill-in-our-sparse-observation-by-predicting-at-more-and-more-points",
    "href": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html#option-1-we-want-to-fill-in-our-sparse-observation-by-predicting-at-more-and-more-points",
    "title": "Barry Gibb came fourth in a Barry Gibb look alike contest (Repost)",
    "section": "Option 1: We want to fill in our sparse observation by predicting at more and more points",
    "text": "Option 1: We want to fill in our sparse observation by predicting at more and more points\n(This is known as “in-fill asymptotics”). This type of question occurs when, for instance, we want to fill in the holes in satellite data (which are usually due to clouds).\nThis is the case that most closely resembles the design of the simulation study in this paper. In this case you refine your estimated coverage by computing more prediction intervals and checking if the true value lies within the interval.\nMost of the easy to find results about coverage in these is from the 1D literature (specifically around smoothing splines and non-parametric regression). In these cases, it’s known that the first option is bad, the second option will lead to conservative regions (the coverage will be too high), the third option involves some sophisticated understanding of how Gaussian random fields work, and the fourth is not something I know anything about."
  },
  {
    "objectID": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html#option-2-we-want-to-predict-at-one-point-where-the-field-will-be-monitored-multiple-times",
    "href": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html#option-2-we-want-to-predict-at-one-point-where-the-field-will-be-monitored-multiple-times",
    "title": "Barry Gibb came fourth in a Barry Gibb look alike contest (Repost)",
    "section": "Option 2: We want to predict at one point, where the field will be monitored multiple times",
    "text": "Option 2: We want to predict at one point, where the field will be monitored multiple times\nThis second option comes up when we’re looking at a long-term monitoring network. This type data is common in environmental science, where a long term network of sensors is set up to monitor, for example, air pollution. The new observations are not independent of the previous ones (there’s usually some sort of temporal structure), but independence can often be assumed if the observations are distant enough in time.\nIn this case as you are repeating observations at a single site, Option 1 will be the right way to construct your interval, option 2 will probably still be a bit broad but might be ok, and options 3 and 4 will probably be too narrow if the underlying process is smooth."
  },
  {
    "objectID": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html#option-3-mixed-asymptotics-you-do-both-at-once",
    "href": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html#option-3-mixed-asymptotics-you-do-both-at-once",
    "title": "Barry Gibb came fourth in a Barry Gibb look alike contest (Repost)",
    "section": "Option 3: Mixed asymptotics! You do both at once",
    "text": "Option 3: Mixed asymptotics! You do both at once\nSimulation studies are the last refuge of the damned."
  },
  {
    "objectID": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html#footnotes",
    "href": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost.html#footnotes",
    "title": "Barry Gibb came fourth in a Barry Gibb look alike contest (Repost)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2021: Oh my giddy aunt what even was that?!↩︎\n2021: The around that time is notable to me, but not interesting to others. So I’m sorry extent to which these blog posts captured the variations in my mental state about that. But also they give a small glimpse at just how bleak my sense of humour can be.↩︎\nNo I don’t speak Swedish, but one of my favourite songwriters/lyricists does. And sometimes I’m just that unbearable. Also the next part of this story takes place in Norway, which is near Sweden but produces worse music (Susanne Sunfør and M2M being notable exceptions)↩︎\nI once gave a truly mortifying talk called INLA: Past, Present, and Future at a conference in Dublin.↩︎\nOr, as happened one time, they compared computation for a different model with an algorithm that failed its convergence checks and assumed that all of the hyperparameters were fixed. All of that is bad but the last part is like saying lm is faster than lme4::lmer for fitting mixed effects models because we only checked when the almost always unknown variance parameters were assumed known.↩︎\nIn 2017. A long time ago.↩︎\nRepeat the same test or make a new test for different data↩︎"
  },
  {
    "objectID": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html",
    "href": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html",
    "title": "Sparse Matrices 4: Design is my passion",
    "section": "",
    "text": "This is the fourth post in a series where I try to squeeze autodiffable sparse matrices into JAX with the aim to speed up some model classes in PyMC. So far, I have:\nI am in the process of writing a blog on building new primitives1 into JAX, but as I was doing it I accidentally wrote a long section about options for exposing sparse matrices. It really didn’t fit very well into that blog, so here it is."
  },
  {
    "objectID": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#what-are-we-trying-to-do-here",
    "href": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#what-are-we-trying-to-do-here",
    "title": "Sparse Matrices 4: Design is my passion",
    "section": "What are we trying to do here?",
    "text": "What are we trying to do here?\nIf you recall from the first blog, we need to be able to compute the value and gradients of the (un-normalised) log-posterior \\[\n\\log(p(\\theta \\mid y)) = \\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TA^TW^{-1}y + \\frac{1}{2} \\log(|Q(\\theta)|) - \\frac{1}{2}\\log(|Q_{u\\mid y, \\theta}(\\theta)|) + \\text{const},\n\\] where \\(Q(\\theta)\\) is a sparse matrix, and \\[\n\\mu_{u\\mid y, \\theta}(\\theta) = \\frac{1}{\\sigma^2} Q_{u\\mid y,\\theta}(\\theta)^{-1} A^TW^{-1}y.\n\\]\nOverall, our task is to design a system where this un-normalised log-posterior can be evaluated and differentiated efficiently. As with all design problems, there are a lot of different ways that we can implement it. They share a bunch of similarities, so we will actually end up implementing the guts of all of the systems.\nTo that end, let’s think of all of the ways we can implement our target2."
  },
  {
    "objectID": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#option-1-the-direct-design",
    "href": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#option-1-the-direct-design",
    "title": "Sparse Matrices 4: Design is my passion",
    "section": "Option 1: The direct design",
    "text": "Option 1: The direct design\n\n\\(A \\rightarrow \\log(|A|)\\), for a sparse, symmetric positive definite matrix \\(A\\)\n\\((A,b) \\rightarrow A^{-1}b\\), for a sparse, symmetric positive definite matrix \\(A\\) and a vector \\(b\\)\n\nThis option is, in some sense, the most straightforward. We implement primitives for both of the major components of our target and combine them using existing JAX primitives (like addition, scalar multiplication, and dot products).\nThis is a bad idea.\nThe problem is that both primitives require the Cholesky decomposition of \\(A\\), so if we take this route we might end up computing an extra Cholesky decomposition. And you may ask yourself: what’s an extra Cholesky decomposition between friends?\nWell, Jonathan, it’s the most expensive operation we are doing for these models, so perhaps we should avoid the 1/3 increase in running time!\nThere are some ways around this. We might implement sparse, symmetric positive definite matrices as a class that, upon instantiation, computes the Cholesky factorisation.\n\nclass SPDSparse: \n  def __init__(self, A_indices, A_indptr, A_x):\n    self._perm, self._iperm = _find_perm(A_indices, A_indptr)\n    self._A_indices, self._A_indptr, self._A_x = _twist(self._perm, A_indices, A_indptr, A_x)\n    try:\n      self._L_indices, self._L_indptr, self._L_x = _compute_cholesky()\n    except SPDError:\n      print(\"Matrix is not symmetric positive definite to machine precision.\")\n  \n  def _find_perm(self, indices, indptr):\n    \"\"\"Finds the best fill-reducing permutation\"\"\"\n    raise NotImplemented(\"_find_perm\")\n  \n  def _twist(self, perm, indices, indptr, x):\n    \"\"\"Returns A[perm, perm]\"\"\"\n    raise NotImplemented(\"_twist\")\n  \n  def _compute_cholesky():\n    \"\"\"Compute the Cholesky decomposition of the permuted matrix\"\"\"\n    raise NotImplemented(\"_compute_cholesky\")\n  \n  # Not pictured: a whole forest of gets\n\nIn contexts where we need a Cholesky decomposition of every SPD matrix we instantiate, this design might be useful. It might also be useful to write a constructor that takes a jax.experimental.CSCMatrix, so that we could build a differentiable matrix and then just absolutely slam it into our filthy little Cholesky context3.\nIn order to use this type of pattern with JAX, we would need to register it as a Pytree class, which involves writing flatten and unflatten routines. The CSCSparse class is a good example of how to implement this type of thing. Some care would be needed to make sure the differentiation rules don’t try to do something stupid like differentiate with respect to self.iperm or self.L_x. This is beyond the extra autodiff sugar in the experimental sparse library.\nImplementing this would be quite an undertaking, but it’s certainly an option. The most obvious downside of this pattern (plus a fully functional sparse matrix class) is that it may end up being quite delicate to have this volume of auxillary information4 in a pytree while making everything differentiate properly. This doesn’t seem to be how most parts of JAX has been built. There are also a couple of sharp corners we could run into with instantiation.\nTo close this out, it’s worth noting a variation on this pattern that comes up: the optional Cholesky. The idea is that rather than compute the permutations and the Cholesky factorisation on initialisation, we store a boolean flag in the class is_cholesky and, whenever we need a Cholesky factor we check is_cholesky and if it’s True we use the computed Cholesky factor and otherwise we compute it and set is_cholesky = True.\nThis pattern introduces state to the object: it is no longer set and forget. This will not work within JAX5, where objects need to be immutable. It’s also not an exceptional pattern in general: it is considerably easier to debug code with stateless objects."
  },
  {
    "objectID": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#option-2-implement-all-of-the-combinations-of-functions-that-we-need",
    "href": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#option-2-implement-all-of-the-combinations-of-functions-that-we-need",
    "title": "Sparse Matrices 4: Design is my passion",
    "section": "Option 2: Implement all of the combinations of functions that we need",
    "text": "Option 2: Implement all of the combinations of functions that we need\nRather than dicking around with classes, we could just implement primitives that compute\n\n\\(A \\rightarrow \\log(|A|)\\), for a sparse, symmetric positive definite matrix \\(A\\)\n\\((A,b, c) \\rightarrow \\log(|A|) + c^TA^{-1}b\\), for a sparse, symmetric positive definite matrix \\(A\\) and vectors \\(b\\) and \\(c\\).\n\nThis is exactly what we need to do our task and nothing more. It won’t result in any unnecessary Cholesky factors. It doesn’t need us to store computed Cholesky factors. We can simply eat, prey, love.\nThe obvious downside to this option is it’s going to just massively expand the codebase if there are more things that we want to do. It’s also not obvious why we would do this instead of just making \\(\\log p(\\theta \\mid y)\\) a primitive6."
  },
  {
    "objectID": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#option-3-just-compute-the-cholesky",
    "href": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#option-3-just-compute-the-cholesky",
    "title": "Sparse Matrices 4: Design is my passion",
    "section": "Option 3: Just compute the Cholesky",
    "text": "Option 3: Just compute the Cholesky\nOur third option is to simply compute (and differentiate) the Cholesky factor directly. We can then compute \\(\\log(|A|)\\) and \\(A^{-1}b\\) through a combination of differentiable operations on the elements of the Cholesky factor (for \\(\\log(|A|)\\)) and triangular linear solves \\(L^{-1}b\\) and \\(L^{-T}c\\) (for \\(A^{-1}b\\)).\nHence we require the following two7 JAX primitives:\n\n\\(A \\rightarrow \\operatorname{chol}(A)\\), where \\(\\operatorname{chol}(A)\\) is the Cholesky factor of \\(A\\),\n\\((L, b) \\rightarrow L^{-1} b\\) and \\((L, b) \\rightarrow L^{-T}b\\) for lower-triangular sparse matrix \\(L\\).\n\nThis is pretty close to how the dense version of this function would be implemented.\nThere are two little challenges with this pattern:\n\nWe are adding another large-ish node \\(L\\) to our autodiff tree. As we saw in other patterns, this is unnecessary storage for our problem at hand.\nThe number of non-zeros in \\(L\\) is a function of the non-zero pattern of \\(A\\). This means the Cholesky will need to be implemented very carefully to ensure that its traceable enough.\n\nThe second point here might actually be an issue. To be honest, I have no idea. I think maybe it’s fine? But I need to do a close read on the adding primitives doc. Essentially, as long as the abstract traces just need shapes but not dimensions, we should be ok.\nFor adding this to something like Stan, however, we will likely need to do some extra work to make sure we know the number of parameters.\nThe advantage of this type of design pattern is that it gives users the flexibility to do whatever perverted thing they want to do with the Cholesky triangle. For example, they might want to do a centring/non-centring transformation. In Option 1, we would need to write explicit functions to let them do that (not difficult, but there’s a lot of code to write, which has the annoying tendency to increases the maintainence burden)."
  },
  {
    "objectID": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#option-4-functors",
    "href": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#option-4-functors",
    "title": "Sparse Matrices 4: Design is my passion",
    "section": "Option 4: Functors!",
    "text": "Option 4: Functors!\nA slightly wilder design pattern would be to abandon sparse matrices and just make functions A(theta, ...) that return a sparse matrix. If that function is differentiable wrt its first argument, then we can build this whole thing up that way.\nIn reality, the only way I can think of to implement this pattern would be to implement a whole differentiable sparse matrix arithmetic (make operations like alpha * A + beta * B, C * D work for sparse matrices). At which point, we’ve basically just recreated option 1.\nI’m really only bringing up functors because unlike sparse matrices, it is actually a pretty good model for implementing Gaussian Processes with general covariance functions. There’s a little bit of the idea in this Stan issue that, to my knowledge, hasn’t gone anywhere. More recently, a variant has been used successfully in the (as yet un-merged) Laplace approximation feature in Stan."
  },
  {
    "objectID": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#which-one-should-we-use",
    "href": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#which-one-should-we-use",
    "title": "Sparse Matrices 4: Design is my passion",
    "section": "Which one should we use?",
    "text": "Which one should we use?\nWe don’t really need to make that choice yet. So we won’t.\nBut personally, I like option 1. I expect everyone else on earth would prefer option 3. For densities that see a lot of action, it would make quite a bit of sense to consider making that density a primitive when it has a complex derivative (à la option 2).\nBut for now, let’s park this and start getting in on the implementations."
  },
  {
    "objectID": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#footnotes",
    "href": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/design-is-my-passion-sparse-matrices-part-four.html#footnotes",
    "title": "Sparse Matrices 4: Design is my passion",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfunctions that have explicit transformations written for them (eg explicit instruction on how to JIT or how to differentiate)↩︎\nI get sick of typing “unnormalised log-posterior”↩︎\nI am sorry. I have had some wine.↩︎\nPermuations, cholesky, etc↩︎\nThis also won’t work in Stan, because all Stan objects are stateless.↩︎\nThis is actually what Stan has done for a bunch of its GLM-type models. It’s very efficient and fast. But with a maintainance burden.↩︎\nor three, but you can implement both triangular solves in one function↩︎"
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "",
    "text": "I guess I’m going to talk about Gaussian processes now. This wasn’t the plan but who really expected a) there to be a plan or b) me to stick to the plan. I feel like writing about Gaussian processes and so I shall! It will be grand."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#what-is-a-gaussian-process",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#what-is-a-gaussian-process",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "What is a Gaussian process?",
    "text": "What is a Gaussian process?\nWell I could tell you that a Gaussian process is defined by its joint distribution \\[\nu \\sim N(\\mu, \\Sigma),\n\\] where \\(u_i = u(s_i)\\), \\(\\mu_i = \\mu(s_i)\\) and \\(\\Sigma_{ij} = c(s_i, s_j)\\) for some positive definite covariance (or kernel) function \\(c(\\cdot, \\cdot)\\).\nBut that would be about as useful as presenting you with a dog that can bark “she’s a grand old flag”: perhaps good enough for a novelty hit, but there’s just no longevity in it.\nTo understand a Gaussian process you need to feel it deep down within you where the fear and the detailed mathematical concepts live.\nSo let’s try again."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#were-gonna-have-a-you-know-what.-im-not-gonna-do-that.-but-i-am-going-to-define-this-stuff-three-times.-once-for-mum-once-for-dad-and-once-for-the-country.",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#were-gonna-have-a-you-know-what.-im-not-gonna-do-that.-but-i-am-going-to-define-this-stuff-three-times.-once-for-mum-once-for-dad-and-once-for-the-country.",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "We’re gonna have a … you know what. I’m not gonna do that. But I am going to define this stuff three times. Once for mum, once for dad, and once for the country.",
    "text": "We’re gonna have a … you know what. I’m not gonna do that. But I am going to define this stuff three times. Once for mum, once for dad, and once for the country.\nYou’ve got to wonder why anyone would introduce something three ways. There are some reasons. The first is, of course, that each definition gives you a different insight into different aspects of Gaussian processes (the operational, the boundless generality, the functional). And the second is because I’ve had to use all three of these ideas (and several more) over the years in order to understand how Gaussian processes work.\nI learnt about GPs from several sources (listed not in order):\n\nA Swede1 (so I will rant about random fields in the footnotes eventually);\nA book2 that was introducing GPs in a very general way because they needed the concept in outrageous generality to answer questions about the distribution of the maximum of a Gaussian process;\nA book3 written by a Russian who’s really only into measure theory and doesn’t believe anything is real if it isn’t at least happening on a Frechet space;\nAnd a book4 by a different Russian who’s really only into generalised Markov properties and needed to work with Gaussian processes that are defined over functions.\n\nOf these, the most relevant is probably the first one. I was primarily taught this stuff by Finn Lindgren, who had the misfortune of having the office next to mine when we worked together in Trondheim a very long time ago. (We both had a lot more hair then.)\nOne of the things that I learnt from him is that Gaussian processes can appear in all kinds of contexts, which means you need to understand them as a model for an unknown function rather than as a tool to be used in a specific context (like for Gaussian process regression or Gaussian process classification).\nIt’s some effort to really get a good grip on the whole “Gaussian processes as a model for an unknown function” thing but once you relax into it5, it stops being alarming to see models where you are observing things that aren’t just \\(u(s_k)\\). It is not alarming when you are observing integrals of the GP over regions, or derivatives. And you (or your methods) don’t fall apart when presented with complex non-linear functions on the GP (as happens if you look at\nBayesian inverse problems literature6)."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#what-is-a-gaussian-process-version-1",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#what-is-a-gaussian-process-version-1",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "What is a Gaussian process? (Version 1)",
    "text": "What is a Gaussian process? (Version 1)\nI’m going to start with the most common definition of a Gaussian process7. This is the definition that was alluded to in the first section and it’s also the definition operationalised in books like Rasmussen and Williams’8, which is a bread and butter reference for most machine learners interested in GPs, use.\nThe idea is pretty straightforward: I need to define a stochastic model for an unknown function \\(u(s)\\) and I want it to be, in some sense, Gaussian. So how do I go about doing this?\nFirstly, I probably don’t care too much about the function as an abstract object. For example, if I’m using the Gaussian process to model something like temperature, I am only going to observe it at a fairly small number of places (even though I could choose any set of places I want). This means that for some arbitrary set set of \\(K\\) locations \\(s_1, s_2, \\ldots, s_K\\), I am most interested9 in understanding the joint distribution10 \\[\n(u(s_1), \\dots, u(s_K))^T.\n\\]\nSo how would we model the joint distribution? If we want the model to be tractable, we probably want a nice distribution. This is where the Gaussian part comes in. The Gaussian distribution is an extremely tractable11 distribution in medium-to-high dimensions. So the choice to model our joint distribution (which could be any size \\(K\\)) as \\[\n(u(s_1), \\dots, u(s_K))^T \\sim N\\left(\\mu_{s_1, \\ldots, s_K}, \\Sigma_{s_1, \\ldots, s_K}\\right),\n\\] makes sense from a purely mercenary position12.\nSo how do we choose the mean and the covariance function? We will see that the mean can be selected as \\([\\mu_{s_1, \\ldots, s_K}]_{k} = \\mu(s_k)\\) for pretty much any function13 \\(\\mu(\\cdot)\\), but, when we come to write \\[\n[\\Sigma_{s_1, \\ldots, s_K}]_{ij} = c(s_i, s_j),\n\\] there will be some very strong restrictions on the covariance function \\(c(\\cdot, \\cdot)\\).\nSo where do these restrictions come from?\n\nOh those (gay) Russians!\nAs with all things in probability, all the good shit comes from the Soviets. Kolmogorov14 was a leading light in the Soviet push to formalise probability and one of his many many many contributions is something called the Kolmogorov extension theorem, which gives the exact conditions under which we can go from declaring that the distributions of \\((u(s_1), \\ldots, u(s_K))^T\\) (these are called finite dimensional distributions) are Gaussian to describing a legitimate random function \\(u(s)\\).\nThere are essentially two conditions:\n\nThe order of the observations doesn’t matter in a material way. In our case changing the order just permutes the rows and columns of the mean vector and covariance matrix, which is perfectly ok.\nThere is a consistent way to map between the distributions of \\((u(s_1), \\ldots, u(s_K), u(s_{K+1}))^T\\) and \\((u(s_1), \\ldots, u(s_K))^T\\). This is the condition that puts a strong restriction on the covariance function.\n\nEssentially, we need to make sure that we have a consistent way to add rows and columns to our covariance matrix while ensuring that stays positive definite (that is, while all of the eigenvalues stay non-negative, which is the condition required for a multivariate normal distribution15). The condition—which is really gross—is that for every positive integer \\(K\\) and every set of points \\(s_1, \\ldots, s_k\\), and for every \\(a_1, \\ldots, a_K\\) not all equal to zero, we require that \\[\n\\sum_{i=1}^K \\sum_{j = 1}^K a_ia_j c(s_i, s_j) \\geq 0.\n\\]\nThis condition is obviously very difficult to check. This is why people typically choose their covariance function from a very short list16 that is typically found in a book on Gaussian processes."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#but-kolmogorov-said-a-little-bit-more",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#but-kolmogorov-said-a-little-bit-more",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "But Kolmogorov said a little bit more",
    "text": "But Kolmogorov said a little bit more\nThere’s a weird thing in grad school in North America where they insist on teaching measure theoretic probability theory and then never ever ever ever ever using any of the subtleties. But Gaussian processes (and, in general, stochastic processes on uncountable index spaces) are a great example of when you need these details.\nWhy? Because unlike discrete probability (where the set of events that we can compute the probability of is obvious) or even continuous random variables (where the events that we can’t compute the probability of are so weird we can truly just ignore them unless we are doing something truly exotic), for Gaussian processes,17 the set of allowable events is considerably smaller than the set of all things you might want probabilities of.\nThe gist of it is that we have built up a random function \\(u(s)\\) from a bunch of finite random vectors. This means that we can only assign probabilities to events that can be built up from events on finite random vectors. The resulting set of events (or \\(\\sigma\\)-algebra to use the adult term) is called the cylindrical18 \\(\\sigma\\)-algebra and can be roughly19 thought of as the set of all events that can be evaluated by evaluating \\(u(\\cdot)\\) at most a countable number of times."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#things-that-arent-measurable",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#things-that-arent-measurable",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "Things that aren’t measurable",
    "text": "Things that aren’t measurable\nThis will potentially become a problem if, for instance, you are working with a Gaussian process in a model that uses a Gaussian process in a weird way. When this happens, it is not guaranteed that, for instance, your likelihood is a measurable function, which would mean that you can’t normalise your probability distribution! (I mean, don’t worry. Unless you’re doing something fairly wild it will be, but it has come up especially in the inverse problems literature!)\nThis limited set of measurable events even seems to preclude well studied “events” like “\\(u\\) is continuous” or “\\(u\\) is twice continuously differentiable” or “\\(u\\) has a finite supremum”. All things that we a) want to know about a Gaussian process and b) things people frequently say about Gaussian processes. It is common for people to say that “Brownian motion is continuous” and similar things.\nAs with all of mathematics, there are a lot of work arounds that we can use. For those three statements in particular, there is some really elegant mathematical work (due, again, to Kolmogorov and extended greatly by others). The idea is that we can build another function \\(\\tilde u(s)\\) such that \\(\\Pr(u(s) = \\tilde u(s)) = 1\\) for all20 \\(s\\) such that \\(\\tilde u(s)\\) is continuous (or differentiable or bounded).\nIn the language of stochastic processes, \\(\\tilde u(s)\\) is called a version of \\(u(s)\\) and the more correct, temperate language (aka the one least likely to find in the literature) is that \\(u(s)\\) has a continuous/differentiable/bounded version.\nIf you’re interested in seeing how a differentiable version of a Gaussian process is constructed, you basically have to dick around with dyads for a while. Martin Hairer’s lecture notes21 is a nice clear example.\n\nWhere are the limitations of this definition?\nThere are a few. These are, of course, in the eye of the beer holder. The definition is workable in a lot of situations and, with some explanation can be broadened out a bit more. It’s less of a great definition when you’re trying to manipulate Gaussian processes as mathematical objects, but that’s what the next one is for.\nThe first limitation is maybe not so much a limit of the definition as a bit of work you have to do to make it applicable. And that is: what happens if I am observing (or my likelihood depends on) averages like \\[\n\\left(\\int_S \\ell_1(s) u(s)\\,ds, \\ldots, \\int_S \\ell_K(s) u(s)\\,ds\\right)^T\n\\] instead of simple point evaluations22.\nThis might seem like a massively different problem, until we remember that integrals are just sums dressed up for Halloween, so we can approximate the integrals arbitrarily well by sums23. In fact, if we squint24 a bit, we can see that the above vector will also be multivariate Gaussian with mean vector \\[\n[\\mu]_k = \\int_S \\ell_k(s) \\mu(s)\\,ds\n\\] and covariance matrix with entries \\[\n[\\Sigma]_{ij} = \\int_{S \\times S} \\ell_i(s) \\ell_j(s')c(s, s')\\,dsds'.\n\\] Similar formulas hold for derivative observations.\nProbably the bigger limitation is that in this way of seeing things, your view is tied very tightly to the covariance function. While it is a natural object for defining Gaussian processes, it is fucking inconvenient if you want to understand things like how well approximate Gaussian processes work.\nAnd let’s face it, a big chunk of Gaussian processes we see in practice are approximate because the computational burden on large data sets is too big to do anything but approximate.\n(Fun fact, when I was much much younger I wrote a paper that was a better title than a paper25 called26 In order to make spatial statistics computationally feasible, we need to abandon the covariance function. I copped a lot of shit for it at the time [partly because the title was better than the paper, but partly because some people are dicks], but I think the subsequent 10 years largely proved me (or at least my title) right27.)\nThe focus on the covariance function also hides the strong similarity between Gaussian process literature and the smoothing splines literature starting from Grace Wahba in the 1970s. It’s not that nobody notices this, but it’s work to get there!\nIn a similar way, it hides the fundamental role the reproducing kernel Hilbert space (or Cameron-Martin space) is doing and the ways that Gaussian process regression is (and is not) like kernel smoothing in RKHSs. This, again, isn’t a secret per se—you can find this information if you want it—but it’s confusing to people and the lack of clarity leads to people missing useful connections (or sometimes leads to them drawing mistaken parallels).\nHow many times have you seen someone say that realisations of a Gaussian process are in the RKHS associated with the covariance function? They are not. In fact, every realisation of a Gaussian process is rougher than any function in the RKHS (with probability 1)! Unfortunately, this means that your reason for choosing the kernel in a RKHS regression and for choosing the covariance function in a Gaussian process prior need to be subtly different. Or, to put it differently, a penalty is not a log-prior and interpreting the maximum a penalised likelihood is, in high dimensions, a very distant activity from interpreting a posterior distribution (even when the penalty is the log of the prior)."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#what-is-a-gaussian-process-version-2",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#what-is-a-gaussian-process-version-2",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "What is a Gaussian process? (Version 2)",
    "text": "What is a Gaussian process? (Version 2)\nOk. Let’s do this again. This definition lives in a considerably more mathematical space and while I’m gonna try to explain the key terms, I will fail. But hey. Who doesn’t like googling weird terms?\nA Gaussian process is a collection of random variables \\(u(s)\\), where \\(s \\in S\\) and \\(S\\) is some set of things that isn’t too topologically disastrous28.\nBut what makes it Gaussian? Here’s the general definition.\n\nA stochastic process/random field is Gaussian if and only if every continuous linear functional has a univariate Gaussian distribution.\n\n\nWell that’s very useful Daniel. What the hell is a linear functional?\nGreat question angry man who lives inside my head! It is any function \\(\\ell(\\cdot)\\) that takes the Gaussian process \\(u(s)\\) and an input and spits out a real number that is is\n\nLinear. Aka \\(\\alpha \\ell(u) + \\beta\\ell(v) = \\ell(\\alpha u + \\beta v)\\)\nBounded29.\n\nGreat. Love a definition. Shall we try something more concrete?\nPoint evaluation \\(u(s_j)\\) (aka evaluating the function at a point) is a linear functional (\\((u + v)(s)_j = u(s_j) + v(s_j)\\)). As is a definite integral over a set \\(\\int_A u(s)\\,ds\\).\nIt’s a fun little exercise to convince yourself that this all implies that for any collection \\(\\ell_1(\\cdot), \\ldots, \\ell_J(\\cdot)\\) of continuous linear functionals, then \\(u(s)\\) is a Gaussian process means that the vector \\[\n(\\ell_1(u), \\ldots \\ell_J(u))^T\n\\] is multivariate Gaussian.\nYour idea of fun is not my idea of fun. Anyway. Keep talking.\nIf \\(u\\) lives in a Banach space30 \\(B\\), then the set of all continuous/bounded linear functionals on \\(B\\) is called the dual space and is denoted \\(B^*\\).         \n\n\nI mean, cool I guess but where the merry hell is the covariance function\nIn this context, the most important thing about \\(B^*\\) is it does double duty: it is both a space of linear functionals and a space that can be identified with random variables.\nHow the fuck do you do that?\nWell, the trick is to remember the definition! If \\(\\ell \\in B^*\\), then \\(\\ell(u)\\) is a Gaussian. Similarly, if we have two functionals \\(\\ell, \\ell' \\in B^*\\) we consider the covariance of their associated random variables \\[\nC_u(\\ell, \\ell') = \\mathbb{E}(\\ell(u)\\ell'(u)).\n\\]\n\\(C_u(\\ell, \\ell')\\) is a symmetric, positive definite bilinear form (aka good candidate for an inner product)!\nWe can use this to add more functions to \\(B^*\\), particularly for any sequence \\(b_n \\in B^*\\) that is Cauchy with respect to the norm \\(\\|\\ell\\|_{R_u} = \\sqrt{C_u(\\ell, \\ell)}\\) we append the limit to \\(B^*\\) to complete the space. Once we take equivalence classes, we end up with a Hilbert space \\(R_u\\) that, very unfortunately, probabilists have a tendency to call the reproducing kernel Hilbert space associated with \\(u\\).\nWhy is this unfortunate? Well primarily because it’s not the exact same space that machine learners call the reproducing kernel Hilbert space, which is, to put it mildly, confusing. But we can build the machine learner’s RKHS (known to probabilists as the Cameron-Martin space).\nWhy are you even telling me this? Is this a digression?\nHonestly. Yes. But regardless the space \\(R_u\\) is quite useful to understand what’s going on. To start off, let’s do one example that shows just how different a Gaussian process is from a multivariate normal random vector. We will show that if we multiply a GP by a constant, we completely change its support31! Many a computational and inferential ship have come to grief on these sharp rocks.\nTo do this, though, we need32 to make an assumption on \\(B\\): We assume that \\(B\\) is separable33. This isn’t an vacuous assumption, but in a lot of cases of practical interest, this is basically the same thing as assuming the set \\(S\\) is a nice bounded domain or a friendly compact manifold (and not something like \\(\\mathbb{R}^d\\))34.\nSo. How do we use \\(R_u\\) to show that Gaussian processes are evil? Well we begin by noting that \\(R_u\\) is a separable35 Hilbert space it contains an orthonormal basis \\(e_n\\), \\(n=1, \\ldots, \\infty\\) (that is \\(\\|e_n\\|_{R_u} = 1\\) and \\(\\langle e_n, e_m\\rangle_{R_u} = 0\\) if \\(n\\neq m\\)). We can use this basis to show some really really weird stuff about \\(u(s)\\).\nIn particular, consider another Gaussian process \\(v(s) = c u(s)\\), where \\(c\\) is a non-zero constant. For this process we can build \\(R_v\\) in an analogous way. The \\(e_n\\) are still orthogonal in \\(R_v\\) but now \\(\\|e_n\\|_{R_v} = c^2\\).\nNow consider the functional \\(X_K(\\cdot) = K^{-1}\\sum_{k = 1}^Ke_i(\\cdot)^2\\). We are going to use this function to break stuff! To do this, we are going to define two disjoint sets of functions \\(A_1 = \\{u: \\lim_{K\\rightarrow \\infty} X_K(u) = 1\\}\\) and \\(A_2 =  \\{u: \\lim_{K\\rightarrow \\infty} X_K(u) = c^2\\}\\). Clearly \\(A_1\\) and \\(A_2\\) are disjoint if \\(|c|\\neq 1\\).\nBecause \\(e_n(\\cdot)\\) are orthonormal in \\(R_u\\), it follows that that \\(u_n = e_n(u) \\sim N(0,1)\\) are iid. Similarly, \\(v_n = e_n(v) \\sim N(0, c^2)\\) are also independent. Hence it follows from the properties of \\(\\chi^2\\) random variables (aka the mean plus the strong law of large numbers) that \\(X_K(u) \\rightarrow 1\\) and hence \\(\\Pr(u \\in A_1) = 1\\). On the other hand, \\(X_K(v) \\rightarrow c^2\\), so \\(\\Pr(v \\in A_2) = 1\\). As \\(A_1\\) and \\(A_2\\) are disjoint, this means that unless \\(|c|=1\\), the processes \\(u\\) and \\(v\\) are mutually singular (aka they have no overlapping support).\nWhat does this mean? This means the distributions of \\(u\\) and \\(v\\) (which remember is just \\(u\\) multiplied by a constant) are as different from each other as a normal distribution truncated to \\((-\\infty, 1)\\) and another normal distribution truncated to \\((1, \\infty)\\)! Or, more realistically36, as disjoint as a distribution over \\(2\\mathbb{Z}\\) and \\((2\\mathbb{Z} - 1)\\).\nThis is an example of the most annoying phenomena in Gaussian processes37: the slightest change in a Gaussian process can lead to a mutually singular process. In fact, this is not a particularly strange example. It can be shown that Gaussian processes over uncountable index spaces are either absolutely continuous or mutually singular. There is no half-arsing it!\nThis has a lot of implications when it comes to computing38, setting priors on the parameters that control the properties of the covariance function39, and just generally inference40.\n\n\nYes but where’s our reproducing kernel Hilbert space\nWe just saw that if \\(u\\) is a Gaussian process than \\(c u\\) will be a singular GP if \\(|c| \\neq 1\\). What happens if we add things? Well, a result known as the Cameron-Martin theorem says that, for a deterministic \\(h(s) \\in B\\), \\(u(s) + h(s)\\) is absolutely continuous wrt \\(u(s)\\) if and only if \\(h(s)\\) is in the Cameron-Martin space \\(H_u\\) (this is the one that machine learners call the RKHS!).\nBut how do we find this mythical space? I find this quite stressful!\nLike, honey I do not know. But when a probabilist is in distress, we can calm them by screaming characteristic function at the top of our lungs right into their ear. Try it. It definitely works. You won’t be arrested.\nSo let’s do that. The characteristic function of a univariate random variable \\(X\\) is \\[\n\\phi_X(t) = \\mathbb{E}\\left(e^{itX}\\right),\n\\] which doesn’t seem like it’s going to be an amazingly useful thing, but it actually is. It’s how you prove the central limit theorem41, and a few other shiny things.\nWhen we are dealing with more complex random things, like random vectors and Gaussian processes, we can use characteristic functions, but we need to extend beyond the fact that they’re currently only defined for univariate random variables. Conveniently, we have some lying around. In particular, if \\(\\ell \\in B^*\\), we have the associated random variable \\(\\ell(u)\\) and we can compute its characteristic function42, which leads to the definition of a characteristic function of a stochastic process on \\(B\\) \\[\n\\phi_u(\\ell) = \\mathbb{E}(e^{i\\ell(u)}), \\quad \\ell \\in B^*.\n\\]\nNow this feels quite different. It’s no longer a function of some real number \\(t\\) but is instead a function of a linear functional \\(\\ell\\), which feels weird but isn’t.\nCharacteristic functions are immensely useful because if two Gaussian processes have same characteristic function they have the same distribution43.\nBecause \\(u(s)\\) is a Gaussian process, we can compute its characteristic function! We know that \\(\\ell(u)\\) is Gaussian so we can look up its characteristic function on Wikipedia and get that \\[\n\\mathbb{E}(e^{i\\ell(u)}) = \\exp\\left[{i \\mu(\\ell) - \\frac{\\sigma^2(\\ell)}{2}}\\right],\n\\] where \\(\\mu(\\ell) = \\mathbb{E}(\\ell(u))\\) and \\(\\sigma^2(\\ell) = \\mathbb{E}(\\ell(u) - \\mu(\\ell))^2\\).\nWe know that \\[\n\\mu(\\ell) = \\mathbb{E}(\\ell(u))\n\\] and \\[\n\\sigma^2(\\ell) = \\mathbb{E}\\left[(\\ell(u) - \\mu(\\ell)^2\\right],\n\\] the latter of which can be extended naturally to the aforementioned positive definite quadratic form \\[\nC_u(\\ell, \\ell') = \\mathbb{E}\\left[(\\ell(u) - \\mu(\\ell)(\\ell'(u) - \\mu(\\ell'))\\right], \\quad \\ell, \\ell' \\in B^*.\n\\]\nThis leads to the exact form of the characteristic function and to this theorem, which is true.\n\nTheorem 1 A stochastic process \\(u(\\cdot)\\) is a Gaussian process if and only if \\[\n\\phi_u(\\ell) = \\exp\\left[i\\mu(\\ell) - \\frac{1}{2}C_u(\\ell, \\ell)\\right].\n\\]\n\nSo Alf is back. In pog form.\nYes.\nIn this case, we can define the covariance operator \\(C_u: B^* \\rightarrow B\\) as44 \\[\n(C_u \\ell) (\\ell') = \\mathbb{E}\\left[(\\ell(u) - \\mu(\\ell)(\\ell'(u) - \\mu(\\ell'))\\right].\n\\] The definition is cleaner when \\(\\mu(\\ell) = 0\\) (which is why people tend to assume that when writing this shit down45), in which case we get \\[\nC_u\\ell = \\mathbb{E}(u\\ell(u))\n\\] and \\[\nC_u(\\ell, \\ell') = \\ell'(C_u\\ell)\n\\]\nGreat gowns, beautiful gowns.\nWow. Shady.\nAnyway, the whole reason to introduce this is the following:\n\nTheorem 2 Let \\(v = x + h\\). Then \\[\n\\phi_v(\\ell) = e^{i\\ell(h)}\\phi_u(\\ell).\n\\]\n\nThis does not not help us answer the question of whether or not \\(v\\) has the same support as \\(u\\). To do this, we construct a variable that is absolutely continuous with respect to \\(u\\) (we guarantee this because we specify its density46 wrt \\(u\\)).\nTo this end, take some \\(g \\in R_u\\) and define a stochastic process \\(w\\) with density wrt47 u \\[\n\\rho(u) = \\exp\\left[iC_u(g, u) - \\frac{1}{2}C_u(g,g)\\right].\n\\]\nFrom this, we can compute48 the characteristic function of \\(w\\) \\[\\begin{align*}\n\\phi_w(\\ell) &= \\mathbb{E}_w\\left(e^{i\\ell(w)}\\right) \\\\\n&= \\mathbb{E}_u\\left(\\rho(u) e^{i\\ell(u)}\\right) \\\\\n&= \\exp\\left[iC_u(g,\\ell) + i \\mu(\\ell)  - \\frac{1}{2}C_u(\\ell, \\ell)\\right]\n\\end{align*}\\]\nSo we are fine if we can find some \\(h \\in B\\) such that \\[\nC_u(g, \\ell) = \\ell(h).\n\\]\nTo do this, we note that \\[\nC_u(g, \\ell) = \\ell(C_u g),\n\\] so for any \\(g\\) we can find a \\(h \\in B\\) such that \\(h = C_ug\\) and for such a \\(h\\) \\(v(s) = u(s) + h(s)\\) is absolutely continuous with respect to \\(u(s)\\).\nThis gives us our definition of the Cameron-Martin space (aka the RKHS) associated with \\(u\\).\n\nDefinition 1 The Cameron-Martin space (or reproducing kernel Hilbert space, if you must) associated with a Gaussian process \\(u\\) is the Hilbert space \\(H_u = \\{h\\in B: h = C_uh^* \\text{ for some } h^* \\in R_u\\}\\) equipped with the inner product \\[\n\\langle h, h'\\rangle_{H_u} = C_u(h^*, (h')^*)\n\\]\n\nA fun note is that the reason the probabilists don’t call the Cameron-Martin space the reproducing kernel Hilbert space is that there is no earthly reason to think that point evaluation will be bounded in general. So it become a problematique name. (And no, I don’t know why they’re ok with calling \\(R_u\\) that some things are just mysterious.)\nLord in heaven. Any chance of being a bit more concrete?\nSure! Let’s consider the case where \\(u \\in \\mathbb{R}^n\\) is a Gaussian random vector \\[\nu \\sim N(\\mu, \\Sigma).\n\\] While all of this is horribly over-powered for this case, it does help get a grip on what the inner product on \\(H_u\\) is.\nIn this case, \\(B^*\\) is row vectors like \\(f^T\\), \\(f\\in \\mathbb{R}^n\\) and \\[\nC_u(f^T, g^T) = \\operatorname{Cov}(f^Tu, g^Tu) = f^T\\Sigma g.\n\\]\nFurthermore,     the operator \\(C_u = \\Sigma f\\) satisfies \\(g^T(\\Sigma f) = C_u(f^T,g^T)\\).\nSo what is \\(H_u\\)? Well, every \\(n\\) dimensional vector space can be represented as an \\(n\\)-dimensional vector, so what we really need to do is identify \\(h^*\\) from \\(h\\). To do this, we use the relationship \\(C(h^*, \\ell) = \\ell(h)\\) for all \\(\\ell \\in B^*\\). Translating that to our finite dimensional case we get that \\[\n(h^*)^T\\Sigma g = h^T g,\\qquad g \\in \\mathbb{R}^n,\n\\] from which it follows that \\(h^* = \\Sigma^{-1}h\\). Hence we get the inner product between \\(h, k \\in H_u\\) \\[\\begin{align*}\n\\langle h, k\\rangle_{H_u} &= \\langle h^*, k^*\\rangle_{R_h} \\\\\n&= (\\Sigma^{-1} h)^T \\Sigma (\\Sigma^{-1 k}) \\\\\n&= h^T \\Sigma^{-1} k.\n\\end{align*}\\]\nOk! That’s cool!\nYes! And the same thing holds in general, if you squint49. Just replace the covariance matrix \\(\\Sigma\\) with the covariance operator \\[\n(\\mathcal{C}f)(s) = \\int_S c(s, s') f(s') \\, ds'.\n\\]\nThis operator has (in a suitable sense) a symmetric50 non-negative definite (left) (closed) inverse operator \\(\\mathcal{Q}\\), which defines the RKHS inner product by \\[\n\\langle f, g \\rangle_{H_u} = \\int_{S} f(s) (\\mathcal{Q} g)(s) \\,ds,\n\\] where \\(f\\) and \\(g\\) are smooth enough functions for this to make sense. In general, \\(\\mathcal{Q}\\) will be a (very) singular integral operator, but when \\(u(s)\\) has the Markov property, \\(\\mathcal{Q}\\) is a differential operator. In all of these cases the RKHS is the set of functions that are smooth enough that \\(\\langle f, f \\rangle_{H_u} &lt; \\infty\\).\nWe sometimes call the operator \\(\\mathcal{Q}\\) the precision operator and it’s fundamental to thin plate spline theory as well as some nice ways to approximate GPs in 1-4 dimensions. I will blog about this later, probably, but for now if you’re interested Finn Lindgren, Håvard Rue, and David Bolin just released a really nice survey paper about the technique.\n\n\nTell me some things about the Cameron-Martin space\nNow that we’ve gone to the effort of finding it, I should probably tell you why it’s so important. So here are a collection of facts!\nFact 1: The Cameron-Martin space (the set of functions and the inner product) determines a51 Gaussian process, in that if two Gaussian processes have the same mean and the the same Cameron-Martin space, they have the same distribution. In fact, the next definition of a Gaussian process is going to show this constructively.\nThis is nice because it means you can define a Gaussian process without needing to specify its covariance function. You just (just!) need to specify a Hilbert space. It turns out that this is a considerably easier task than trying to find a positive definite covariance function if the domain \\(S\\) is weird.\nFact 2: \\(u(s)\\) is never in the RKHS. That is, \\(\\Pr(u \\in H_u) = 0\\). But52 if, for any \\(\\epsilon&gt;0\\), \\(A_\\epsilon \\subset B\\) is any measurable set of functions with \\(\\Pr(u \\in A) = \\epsilon\\), then \\(\\Pr(u \\in A_\\epsilon + H_u) = 1\\), where \\(A_\\epsilon+H_u = \\{a + h \\in B: a\\in A_\\epsilon, h \\in H_u\\}\\). Or to say it in words, although \\(u\\) is never in \\(H_u\\), if you find a set \\(A_\\epsilon\\) that \\(u\\) could be in (even if it’s extremely unlikely to be there), then \\(u\\) is almost surely made up of a function in \\(A_\\epsilon\\) plus a function in \\(H_u\\).\nThis is wild. It means that while \\(u(\\cdot)\\) is never in the RKHS, all you need to do is add a bit of rough to get all of the stuff out. Another characterisation of the RKHS that are related to this is that it is the intersection of all subsets of \\(B\\) that have full measure under \\(u\\) (aka all sets \\(A\\subset B\\) such that \\(\\Pr(u \\in A) = 1\\)).\nFact 3: If we observe some data \\(y = N(Tu, \\Sigma_y)\\), where \\(Tu = (\\ell_1(u),\\ldots, \\ell_n(u))^T\\) is some observation vector, then the posterior mean \\(\\mathbb{E}(u \\mid y)\\) is in the RKHS and that posterior distribution of \\(u\\mid y\\) is a Gaussian process that’s absolutely continuous with respect to the prior GP u(s). This means that the posterior mean, which is our best point prediction under squared error loss, is always smoother than any of the posterior draws.\nThis kinda makes sense: averaging things smooths out the rough edges. And so when we average a Gaussian process in this way, we make it smoother. But this is a thing that we need to be aware of! Our algorithms, our reasoning for choosing a kernel, and our interpretations of the posterior need to be aware that the space of posterior realizations \\(B\\) is rougher than the space that contains the posterior mean.\nFrequentists / people who penalise likelihoods don’t have to worry about this shit."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#so-what-have-we-learnt",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#so-what-have-we-learnt",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "So what have we learnt?",
    "text": "So what have we learnt?\nSo so so so so so so much notation and weird maths shit.\nBut there are three take aways here:\n\nThe importance of the Fourier transform (aka the characteristic function) when it comes to understanding Gaussian processes.\nThe maths buys us understanding of some of the more delicate properties of a Gaussian process as a random object (in particular it’s joint properties)\nYou can define a Gaussian process exclusively using the RKHS inner product. (You can also do all of the computations that way too, but we’ll cover that later). So you do not need to explicitly specify a covariance function. Grace Wahba started doing this with thin plate splines (and \\(L\\)-splines) in 1974 and it worked out pretty well for her.\n\nSo to finish off this post, let’s show one more way of constructing a Gaussian process. This time we will explicitly start from the RKHS."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#what-is-a-gaussian-process-version-3",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#what-is-a-gaussian-process-version-3",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "What is a Gaussian process? (Version 3)",
    "text": "What is a Gaussian process? (Version 3)\nOur final Gaussian process definition is going to centre the RKHS53 as the fundamental object. This construction, which is known as an abstract Wiener space54 is less general55 than our previous definition, but it covers most of the processes we are going to encounter in applications.\nThis construction is by far the most abstract of the three (it is in the name after all). So buckle up.\nThe jumping off point here is a separable Hilbert space \\(H\\). This has an inner-product \\(\\langle\\cdot, \\cdot \\rangle_H\\) on it, and the associated notion of orthogonality and an orthogonal projector. Consider an \\(n\\)-dimensional subspace \\(V_n \\subset H_u\\). We can, without any trouble, define a Gaussian process on \\(V_n\\) \\(\\tilde u_n\\) with characteristic function \\[\n\\phi_{\\tilde u_n}(h) = \\exp\\left(-\\frac{1}{2}\\langle h,h\\rangle_H\\right).\n\\] We hit no mathematical problems because \\(V_n\\) is finite dimensional and nothing weird happens to Gaussians in finite dimensions.\nThe thing is, we can do this for any finite dimensional subspace \\(V_n\\) and, in particular, if we have a sequence of subspace \\(V_1 \\subset V_2 \\subset \\ldots\\), where \\(\\operatorname{dim}(V_n) =n\\), then we can build a sequence of finite dimensional Gaussian processes \\(\\tilde u_n\\) that are each supported in their respective \\(V_n\\).\nThe question is: can we construct a Gaussian process \\(\\tilde{u}\\) supported56 on \\(H\\) such that \\(P_n \\tilde u \\stackrel{d}{=} \\tilde u_n\\), where \\(P_n\\) is the orthogonal projector from \\(H\\) to \\(V_n\\)?\nYou would think the answer is yes. It is not. In fact, Komolgorov’s extension theorem says that we can build a Gaussian process this way, but it does not guarantee that the process will be supported on \\(H\\). And it is not.\nTo see why this is, we need to look a bit more carefully at the covariance operator of a Gaussian process on a separable Hilbert space. The key mathematical feature of a separable Hilbert space is that it has an57 orthonormal58 basis \\(e_n\\). We can use the orthonormal basis to do a tonne of things, but the one we need right now is the idea of a trace59 \\[\n\\operatorname{tr}(C_u) = \\sum_{n = 1}^\\infty C_u(e_i, e_i).\n\\]\nFor a (zero mean) Gaussian process \\(u\\) supported on \\(H\\), we can see that \\[\\begin{align*}\n\\operatorname{tr}(C_u) &= \\sum_{n = 1}^\\infty \\mathbb{E}\\left[(\\langle e_n, u\\rangle)^2\\right] \\\\\n&= \\mathbb{E}\\left[ \\sum_{n = 1}^\\infty\\langle e_n, u\\rangle_H^2\\right] \\\\\n&= \\mathbb{E}\\left[\\langle u, u\\rangle_H\\right] &lt; \\infty,\n\\end{align*}\\] where the second line is just true because I say it is and the third line is Pythagoras’ theorem writ large (and is finite because Gaussian processes have a lot of moments60!).\nIf we were to say this in words, we would say that the covariance operator of a Gaussian process supported on a separable Hilbert space is a trace-class operator (or has a finite trace).\nAnd this is where we rejoin the main narrative. You see, if \\(\\tilde{u}\\) was a stochastic process on \\(H\\), then its characteristic function would be \\[\n\\phi_{\\tilde u}(h) = \\exp\\left(-\\frac{1}{2}\\langle h, h \\rangle_H\\right).\n\\] But it can’t be! Because \\(H\\) is infinite dimensional and the proposed covariance operator is the identity on \\(H\\), which is not trace class (its trace is clearly infinite).\nSo whatever \\(\\tilde u\\) is61, it is emphatically not a Gaussian process on \\(H\\).\n\nThat doesn’t seem like a very useful trip through abstract land\nWell, while we did not successful make a Gaussian process on \\(H\\) we did actually build the guts of a Gaussian process on a different space. The trick is to use the same idea in reverse. We showed that \\(\\tilde u\\) was not a Gaussian process because its covariance operator wasn’t on trace class. It turns out that the reverse also holds: if \\[\n\\phi_u(h) = \\exp\\left(-\\frac{1}{2}\\langle C_uh, h\\rangle_{H'}\\right)\n\\] and \\(C_u\\) is trace class on \\(H'\\), then \\(u\\) is a Gaussian process supported on \\(H'\\).\nThe hard part is going to be finding another Hilbert space \\(H' \\supset H\\).\nTo do this, we need to recall a definition of a separable Hilbert space \\(H\\) with orthonormal basis \\(e_n\\), \\(n=1, \\ldots, \\infty\\): \\[\nH = \\left\\{\\sum_{n=1}^\\infty a_n e_n: \\sum_{n=1}^\\infty a_n^2 &lt; \\infty\\right\\}.\n\\] From this, we can build a larger separable Hilbert space \\(H'\\) as \\[\nH' = \\left\\{\\sum_{n=1}^\\infty a_n e_n: \\sum_{n=1}^\\infty \\frac{a_n^2}{n^2} &lt; \\infty\\right\\}.\n\\] This is larger because there are sequences of \\(a_n\\)s that are admissible for \\(H'\\) that aren’t admissible for \\(H\\) (for example62, \\(a_n = \\sqrt{n}\\)).\nWe let \\(j:H \\rightarrow H'\\) be the linear embedding that we get by considering an element \\(h \\in H\\) as an element of \\(H'\\). If we let \\(e_n'\\) be an orthonormal basis on \\(H'\\) (note: this is not the same as \\(e_n\\) as it needs to be re-scaled to have unit norm in \\(H'\\)), then we get \\[\nj\\left(\\sum_{n=1}^\\infty \\alpha_n e_n\\right) = \\sum_{n=1}^\\infty  \\frac{\\alpha_n}{n} e_n'.\n\\] Why? Because \\(\\|e_n\\|_{H'} = n^{-1}\\) which means that \\(e_n' = n e_n\\) is an orthonormal basis for \\(H'\\). This means we have to divide the coefficients by \\(n\\) when we move from \\(H\\) to \\(H'\\), otherwise we wouldn’t be representing the same function.\nWith this machinery set up, we can ask if \\(\\tilde u\\) is a Gaussian process on \\(H'\\). Or, more accurately, we can ask if \\(u = j(\\tilde u)\\) is a Gaussian process on \\(H\\).\nWell.\nLet’s compute its characteristic function. \\[\\begin{align*}\n\\phi_u(h') &= \\mathbb{E}\\left(e^{i\\left\\langle u, h' \\right\\rangle_{H'}}\\right) \\\\\n&= \\mathbb{E}\\left[\\exp\\left(i\\left\\langle \\sum_{n=1}^\\infty \\frac{\\langle \\tilde u, e_n\\rangle_H}{n}e_n', \\sum_{n=1}^\\infty h_n'e_n' \\right\\rangle_{H'}\\right) \\right] \\\\\n&= \\mathbb{E}\\left[\\exp\\left(i \\sum_{n=1}^\\infty \\frac{\\langle \\tilde u, e_n\\rangle}{n} h_n\\right) \\right] \\\\\n&= \\exp\\left(-\\frac{1}{2} \\sum_{n=1}^\\infty \\frac{h_n^2}{n^2}\\right).\n\\end{align*}\\] It follows that \\(\\phi_u(e_n') = e^{-1/(2n^2)}\\) and so63 \\[\n\\operatorname{tr}(C_u) = -2\\sum_{n=1}^\\infty \\log \\phi_u(e_n') = \\sum_{n=1}^\\infty \\frac{1}{n^2} &lt; \\infty,\n\\] \\(C_u\\) is a trace class operator on \\(H'\\) and, therefore, \\(u\\) is a Gaussian process on \\(u\\).\nBut wait, there is more! To do the calculation above, we identified elements of \\(H'\\) as infinite sequences \\(h' = (h'_1, h'_2, \\ldots)\\) that satisfy \\(\\sum_{n=1}^\\infty n^{-2}h_n^2 &lt; \\infty\\). In this case the covariance operator is \\(C_{u}\\) is diagonal, so the \\(n\\)th entry of \\(C_u h' = n^{-2}h'_n\\). From this, and the reasoning in the previous section, we see that the Cameron-Martin space can be thought of as a subset of \\(H'\\). The Cameron-Martin inner product can be constructed from the inverse of \\(C_u\\), which gives \\[\n\\langle a, b\\rangle_{H_u} = \\sum_{i=1}^\\infty n^2 a_n b_n.\n\\] Clearly, this will not be finite unless we put much much stronger restrictions on \\(a_n\\) and \\(b_n\\) than that \\(\\sum_{n\\geq 1} n^{-2}a_n^2 &lt; \\infty\\).\nThe Cameron Marin space is the subspace of \\(H'\\) consisting of all functions \\(h' = \\sum_{n=1}^\\infty a_n e_n'\\) such that \\[\n\\sum_{n=1}^\\infty n^2a_n^2 &lt; \\infty.\n\\] This is (isomorphic to) \\(H\\)!\nTo see this, we note that the condition is only going to hold if \\(a_n = n^{-1}\\alpha_n\\) for some sequence \\(\\alpha_n\\) such that \\(\\sum_{n\\geq 1} \\alpha_n^2 &lt; \\infty\\). Remembering that \\(e_n' = n e_n\\), it follows that \\(h \\in H_u\\) if and only if \\[\\begin{align*}\nh &= \\sum_{n=1}^n\\frac{\\alpha_n}{n} e_n' \\\\\n&=\\sum_{n=1}^n\\frac{\\alpha_n}{n} n e_n \\\\\n&=\\sum_{n=1}^n \\alpha_n e_n,\n\\end{align*}\\] which is exactly the definition of \\(H\\).\n\n\nAre you actually trying to kill me?\nYes.\nSo let’s recap what we just did: We took a separable Hilbert space \\(H\\) and used it to construct a Gaussian process on a larger space \\(H'\\) with \\(H\\) as its Cameron-Martin space. And we did all of this without ever touching a covariance function. This is an abstract Wiener space construction of a Gaussian process.\nThe thing is that this construction is a lot more general than this. The following is a (simplified64) version of the abstract Wiener space theorem.\n\nTheorem 3 Let \\(H\\) be a separable Hilbert space and let \\(B\\) be a separable Banach space. Furthermore, we assume that \\(H\\) is dense in \\(B\\). Then there is a unique Gaussian process \\(u\\) with \\(\\Pr(u \\in B) = 1\\) and \\(H_u = H\\). It can be constructed from the canonical cylindrical Gaussian process \\(\\tilde u\\) on \\(H\\) by \\(u = j(\\tilde u)\\), where \\(j:H \\rightarrow E\\) is the natural embedding.\n\n\n\nWas there any point to doing that?\nI mean, probably not. The main thing we did here was see that you can take the RKHS as the primal object when building a Gaussian process. Why that may be a useful observation was not covered.\nWe also saw that there are some restrictions required on the covariance operator to ensure that a Gaussian process is a proper stochastic process on a given space. (For the tech-heads, the problem with \\(\\tilde u\\) is that it’s associated probability measure is not countably additive. That is a bad thing, so we do not allow it.)\n\nThe restrictions are very clear for covariance operators on separable Hilbert spaces (they must be trace class). Unfortunately, there isn’t any clean characterization of all allowable covariance operators on more complex spaces like Banach spaces65."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#where-do-we-go-now-but-nowhere",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#where-do-we-go-now-but-nowhere",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "Where do we go now but nowhere",
    "text": "Where do we go now but nowhere\nAnd with that I have finished my task. I have defined Gaussian processes three different ways and if anyone is still reading at this point: you’re a fucking champion.\nI probably want to talk about other stuff eventually:\n\nUsing all this technology to work out what happens to a posterior when we approximate a Gaussian process (which we usually do for computational reasons)\nUnderstanding how singularity/absolute continuity of Gaussian measures can help you set priors for the parameters in a covariance function\nThe Markov property in space: what is it and how do you use it\nShow how we can use methods for solving PDEs to approximate Gaussian processes.\n\nThe last one has gotten a lot less urgent because Finn, David and Håvard just released a lovely survey paper.\nMaybe by the time I am finished with these things (if that ever happens, I don’t rate my chances), I will have justified all of this technicality. But for now, I am done."
  },
  {
    "objectID": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#footnotes",
    "href": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html#footnotes",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot the root vegetable.↩︎\nThe first 5 chapters of Adler and Taylor’s masterpiece s are glorious↩︎\nNot gonna lie. Bogachev’s Gaussian Measures is only recommended if you believe in intercessory prayer.↩︎\nRozanov’s Markov Random Fields, which is freely available from that link and is so beautiful you will cry when it turns the whole question into one about function space embeddings. It will be a moist old time. Bring tissues.↩︎\nI recommend some video head cleaner↩︎\nwhich spent an embarrassing amount of time essentially divorced from the mainstream statistical literature↩︎\nThis is the nomenclature that machine learners thrust upon us and it’s annoying and I hate it. Traditionally, a stochastic process was indexed by time (so in this case it would be a one-dimensional Gaussian process and when it was indexed by any other set it was referred to as a random field. So I would much rather be talking about Gaussian random fields. Why? Because there’s a bunch of shit that is only true in 1D and I’m not interested in talking about that)↩︎\nGreat book. Great reference. No shade whatsoever↩︎\nMaybe? But maybe I’m most interested in the average temperature over a region. This is why we are going to need to think about things more general than just evaluating Gaussian processes at a location.↩︎\nWhy the joint? Well because it’s likely that nearby temperature measurements will be similar, while measurements that are far apart are more likely to be (almost) independent (maybe after adjusting for season, time of day, etc).↩︎\nin the sense that we have formulas for almost everything we want to have formulas for↩︎\nWe can also play games with multivariate Gaussians (like building deep Gaussian processes or putting stochastic models on the covariance structure) that markedly increase their flexibility.↩︎\nUsually this involves covariates!↩︎\nWikipedia edit war aside (have a gander, it’s a blast), there’s evidence that Kolmogorov had a long-term relationship with Aleksandrov that was a) known at the time and b) used by the Soviets to blackmail them. So that’s fun.↩︎\nto be proper on some subspace. We are allowed zero eigenvalues for technical reasons and it actually turns out to be useful later, making things like thin plate splines a type of Gaussian process. Grace Wahba had to do all this without Google.↩︎\nExponential, Mat'{e}rn, and squared-exponential are the common ones on \\(\\mathbb{R}^d\\). After that shit gets exotic.↩︎\nand any other process built from Kolmogorov’s extension theorem↩︎\nso named because a set \\(A = \\{u(\\cdot): u(s_1, \\ldots, u(s_K)) \\in B;\\; B\\in \\mathcal{B}(\\mathbb{R}^K)\\}\\) is called a cylinder set↩︎\nthe exact definition is the smallest \\(\\sigma\\)-algebra for which all continuous linear functionals are measurable↩︎\nThe all part is important here. Consider the function \\(u(s) = 1_{A}(s)\\) where \\(A\\) is uniformly distributed on \\(S\\) and \\(1_A(s)\\) is 1 when \\(s=A\\) and zero otherwise. This function is equal to \\(0\\) for almost every \\(s\\) (rather than for every \\(s\\)), but the random function \\(u(s)\\) is definitely not the zero function (it is always non-zero at exactly one point).↩︎\nBottom of page 12 through page 14 here↩︎\nThis is a straight generalisation. If \\(\\ell_k(s) = \\delta_{s_k}(s)\\) then it’s the exact situation we were in before.↩︎\nIn the technical language of the next section, the set of delta functions is dense in the space of bounded linear functionals↩︎\naka replace integrals with sums, compute the joint distribution of the sums, and then send everything to infinity, which is ok when \\(\\ell_k\\) are bounded↩︎\nThe paper is pretty good and I think it’s a nice contribution. But the title was perfect.↩︎\nSorry for the pay wall. It’s from so long ago it’s not on arXiv.↩︎\nYes. NN-GPs, Vecchia approximations, fixed-rank Kriging, variational GPs, and all of the methods I haven’t specifically done work on, all abandon some or all of the covariance function. Whether the people who work on those methods think they’re abandoning the covariance function is between them an Cher.↩︎\nI say this, but you can make this work over pretty bonkers spaces. If we want to be general, if \\(E\\) is a linear space and \\(F\\) is a space of functionals on \\(E\\) that separates the points of \\(F\\), then \\(u(s)\\) is defined as a Gaussian process (wrt the appropriate cylindrical \\(\\sigma\\)-algebra) if \\(f(u)\\) is Gaussian for all \\(f\\in F\\). Which is fairly general but also, like, at this point I am just really showing off my maths degree.↩︎\nIt is very convenient that continuous linear functionals and bounded linear functionals are the same thing.↩︎\nit’s the one with a norm↩︎\nThe support of \\(u\\) is a set \\(A \\subset B\\) such that \\(\\Pr(u \\in A) = 1\\).↩︎\nneed is a big word here. We don’t need to do this, but not doing it makes things more technical. The assumption we are about to make let’s us breeze past a lot of edge cases as we sail from the unfettered Chapter 2 of Bogachev to the more staid and calm Chapter 3 of Bogachev.↩︎\nThat it contains a countable dense set. Somewhat surprisingly, this implies that the Gaussian process is separable (or alternatively that it’s law is a Radon measure), which is a wildly technical condition that just makes everything about 80% less technical↩︎\nThere are separable spaces on the whole space too, but, like, leave me alone.↩︎\nI’ve made a↩︎\nThese sets don’t overlap, but they’re probably not very far apart from each other? Honestly I can’t be arsed checking but this is my feeling.↩︎\nand continuously index stochastic processes/random fields in general↩︎\nsee Simon Cotter and Friends↩︎\nsee Geir-Arne Fuglstad and friends↩︎\nZhang, H. (2004). Inconsistent estimation and asymptotically equal interpolations in model-based geostatistics. Journal of the American Statistical Association, 99(465):250–261.↩︎\nEveryone who’s ever suffered through that inexplicable grad-level measure-valued probability course that builds up this really fucking intense mathematical system and then essentially never uses it to do anything interesting should be well aware of the many many many many ways to prove the central limit theorem.↩︎\nwell, the characteristic function when \\(t=1\\) because if \\(\\ell \\in B^*\\), \\(t\\ell \\in B^*\\).↩︎\nIn a locally convex space, this is true as measures over the cylindrical \\(\\sigma\\)-algebra, but for separable spaces it’s true over the Borel \\(\\sigma\\)-algebra (aka all open sets), which is an enormous improvement. (This happens because for separable spaces these two \\(\\sigma\\)-algebras coincide.) That we have to make these sorts of distinctions (between Baire and Borel measures) at all is an important example of when you really need the measure theoretic machinery to do probability theory. Unfortunately, this is beyond the machinery that’s typically covered in that useless fucking grad probability course.↩︎\nIt is not at all clear that the range of this operator is contained in \\(B\\). It should be mapping to \\(B^{**}\\), but that separability really really helps! Check out the Hairer notes.↩︎\nOr they define it on the set \\(\\{\\ell - \\mu(\\ell): \\ell \\in B^*\\}\\), the completion of which is the general definition of \\(R_u\\).↩︎\nRadon-Nikodym derivative↩︎\nA true pain in the arse when working on infinite dimensional spaces is that there’s no natural equivalent of a Lebesgue measure, so we don’t have a universal default measure to take the density against. So we have to take it against an existing probability measure. In this case, the most convenient one is the distribution of \\(u\\). In finite dimensions, the density \\(\\rho(u)\\) would satisfy \\(p_w(x) = \\rho(x)p_u(x)\\) where \\(p_w(\\cdot)\\) is the density of \\(w\\).↩︎\nI’m skipping the actual computation because I’m lazy.↩︎\nor if you’re working on a separable Hilbert space↩︎\nself-adjoint↩︎\nseparable↩︎\nThis next thing is a consequence of Borel’s inequality: \\(\\mathbb{B}(t, H_u)\\) is the \\(t\\) ball in \\(H_u\\) and a \\(A\\) is any measurable subset of \\(B\\) with \\(\\Pr(u \\in A) = \\Phi(\\alpha)\\), then \\(\\Pr(u \\in A + \\mathbb{B}(t, H_u)) \\geq \\Phi(\\alpha + t)\\), where \\(\\Phi\\) is the CDF of the standard normal distribution. Just take \\(t\\rightarrow \\infty\\).↩︎\nAt some point while writing this I’ve started using RKHS and Cameron-Martin space interchangeably for the one that is a subset of \\(B\\). We’re all just gonna have to be ok with that.↩︎\nYou can get references for this from the Bogachev book, but I actually quite like this survey from Jan van Neervaen, even though it’s almost comically general.↩︎\nAlthough I made the big, ugly assumption that \\(B\\) was separable halfway through the last definition, almost everything is true without that. Just with more caveats. Whereas, the abstract Wiener space construction really fundamentally uses the separability of \\(H_u\\) and \\(B\\) as a place to start.↩︎\nie with \\(\\Pr(\\tilde u \\in H) = 1\\)↩︎\nIt has lots of them but everything we’re about to talk about is independent of the choice of orthonormal basis.↩︎\n\\(\\|e_n\\|_H = 1\\) and \\(\\langle e_n, e_m \\rangle = 0\\) for \\(m\\neq n\\).↩︎\nThis is the trace of the operator \\(C_u\\) and I would usually write this as \\(\\sum_{n\\geq 1} \\langle Ce_i, e_i\\rangle\\), but it makes no difference here.↩︎\nThere’s a result called Fernique’s theorem that implies that Gaussian processes have all polynomial and exponential moments.↩︎\nIt’s called an iso-normal process and is strongly related to the idea of white noise and I’ll probably talk about that at some point. But the key thing is it is definitely not a Gaussian process in the ordinary sense on \\(H\\). We typically call it a generalized Gaussian process or a Generalized Gaussian random field and it is a Gaussian process indexed by \\(H\\). Life is pain.↩︎\nchaos_reins.gif↩︎\nYou can convince yourself this is true. I’m not doing all the work for you↩︎\nIf you want more, read Bogachev or that Radonification paper↩︎\nThe best reference I have is this survey↩︎"
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "",
    "text": "Welcome to part six!!! of our ongoing series on making sparse linear algebra differentiable in JAX with the eventual hope to be able to do some cool statistical shit. We are nowhere near done.\nLast time, we looked at making JAX primitives. We built four of them. Today we are going to implement the corresponding differentiation rules! For three1 of them.\nSo strap yourselves in. This is gonna be detailed.\nIf you’re interested in the code2, the git repo for this post is linked at the bottom and in there you will find a folder with the python code in a python file."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#she-is-beauty-and-she-is-grace.-she-is-queen-of-50-states.-she-is-elegance-and-taste.-she-is-miss-autodiff",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#she-is-beauty-and-she-is-grace.-she-is-queen-of-50-states.-she-is-elegance-and-taste.-she-is-miss-autodiff",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "She is beauty and she is grace. She is queen of 50 states. She is elegance and taste. She is miss autodiff",
    "text": "She is beauty and she is grace. She is queen of 50 states. She is elegance and taste. She is miss autodiff\nDerivatives are computed in JAX through the glory and power of automatic differentiation. If you came to this blog hoping for a great description of how autodiff works, I am terribly sorry but I absolutely do not have time for that. Might I suggest google? Or maybe flick through this survey by Charles Margossian..\nThe most important thing to remember about algorithmic differentiation is that it is not symbolic differentiation. That is, it does not create the functional form of the derivative of the function and compute that. Instead, it is a system for cleverly composing derivatives in each bit of the program to compute the value of the derivative of the function.\nBut for that to work, we need to implement those clever little mini-derivatives. In particular, every function \\(f(\\cdot): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) needs to have a function to compute the corresponding Jacobian-vector product \\[\n(\\theta, v) \\rightarrow J(\\theta) v,\n\\] where the \\(n \\times m\\) matrix \\(J(\\theta)\\) has entries \\[\nJ(\\theta)_{ij} = \\frac{\\partial f_j }{\\partial \\theta_j}.\n\\]\nOk. So let’s get onto this. We are going to derive and implement some Jacobian-vector products. And all of the assorted accoutrement. And by crikey. We are going to do it all in a JAX-traceable way."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#jvp-number-one-the-linear-solve.",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#jvp-number-one-the-linear-solve.",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "JVP number one: The linear solve.",
    "text": "JVP number one: The linear solve.\nThe first of the derivatives that we need to work out is the derivative of a linear solve \\(A^{-1}b\\). Now, intrepid readers, the obvious thing to do is look the damn derivative up. You get exactly no hero points for computing it yourself.\nBut I’m not you, I’m a dickhead.\nSo I’m going to derive it. I could pretend there are reasons3, but that would just be lying. I’m doing it because I can.\nBeyond the obvious fun of working out a matrix derivative from first principles, this is fun because we have two arguments instead of just one. Double the fun.\nAnd we really should make sure the function is differentiated with respect to every reasonable argument. Why? Because if you write code other people might use, you don’t get to control how they use it (or what they will email you about). So it’s always good practice to limit surprises (like a function not being differentiable wrt some argument) to cases4 where it absolutely necessary. This reduces the emails.\nTo that end, let’s take an arbitrary SPD matrix \\(A\\) with a fixed sparsity pattern. Let’s take another symmetric matrix \\(\\Delta\\) with the same sparsity pattern and assume that \\(\\Delta\\) is small enough5 that \\(A + \\Delta\\) is still symmetric positive definite. We also need a vector \\(\\delta\\) with a small \\(\\|\\delta\\|\\).\nNow let’s get algebraing. \\[\\begin{align*}\nf(A + \\Delta, b + \\delta) &= (A+\\Delta)^{-1}(b + \\delta) \\\\\n&= (I + A^{-1}\\Delta)^{-1}A^{-1}(b + \\delta) \\\\\n&= (I - A^{-1}\\Delta + o(\\|\\Delta\\|))A^{-1}(b + \\delta) \\\\\n&= A^{-1}b + A^{-1}(\\delta - \\Delta A^{-1}b ) + o(\\|\\Delta\\| + \\|\\delta\\|)\n\\end{align*}\\]\nEasy6 as.\nWe’ve actually calculated the derivative now, but it’s a little more work to recognise it.\nTo do that, we need to remember the practical definition of the Jacobian of a function \\(f(x)\\) that takes an \\(n\\)-dimensional input and produces an \\(m\\)-dimensional output. It is the \\(n \\times m\\) matrix \\(J_f(x)\\) such that \\[\nf(x + \\delta)  = f(x) + J_f(x)\\delta + o(\\|\\delta\\|).\n\\]\nThe formulas further simplify if we write \\(c = A^{-1}b\\). Then, if we want the Jacobian-vector product for the first argument, it is \\[\n-A^{-1}\\Delta c,\n\\] while the Jacobian-vector product for the second argument is \\[\nA^{-1}\\delta.\n\\]\nThe only wrinkle in doing this is we need to remember that we are only storing the lower triangle of \\(A\\). Because we need to represent \\(\\Delta\\) the same way, it is represented as a vector Delta_x that contains only the lower triangle of \\(\\Delta\\). So we need to make sure we remember to form the whole matrix before we do the matrix-vector product \\(\\Delta c\\)!\nBut otherwise, the implementation is going to be pretty straightforward. The Jacobian-vector product costs one additional linear solve (beyond the one needed to compute the value \\(c = A^{-1}b\\)).\nIn the language of JAX (and autodiff in general), we refer to \\(\\Delta\\) and \\(\\delta\\) as tangent vectors. In search of a moderately coherent naming convention, we are going to refer to the tangent associated with the variable x as xt.\nSo let’s implement this. Remember: it needs7 to be JAX traceable."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-two-the-triangular-solve",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-two-the-triangular-solve",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Primitive two: The triangular solve",
    "text": "Primitive two: The triangular solve\nFor some sense of continuity, we are going to keep the naming of the primitives from the last blog post, but we are not going to attack them in the same order. Why not? Because we work in order of complexity.\nSo first off we are going to do the triangular solve. As I have yet to package up the code (I promise, that will happen next8), I’m just putting it here under the fold.\n\n\nThe primal implementation\n\n\nfrom scipy import sparse\nimport numpy as np\nfrom jax import numpy as jnp\nfrom jax import core\nfrom jax._src import abstract_arrays\nfrom jax import core\n\nsparse_triangular_solve_p = core.Primitive(\"sparse_triangular_solve\")\n\ndef sparse_triangular_solve(L_indices, L_indptr, L_x, b, *, transpose: bool = False):\n  \"\"\"A JAX traceable sparse  triangular solve\"\"\"\n  return sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose = transpose)\n\n@sparse_triangular_solve_p.def_impl\ndef sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, *, transpose = False):\n  \"\"\"The implementation of the sparse triangular solve. This is not JAX traceable.\"\"\"\n  L = sparse.csc_array((L_x, L_indices, L_indptr)) \n  \n  assert L.shape[0] == L.shape[1]\n  assert L.shape[0] == b.shape[0]\n  \n  if transpose:\n    return sparse.linalg.spsolve_triangular(L.T, b, lower = False)\n  else:\n    return sparse.linalg.spsolve_triangular(L.tocsr(), b, lower = True)\n\n@sparse_triangular_solve_p.def_abstract_eval\ndef sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, *, transpose = False):\n  assert L_indices.shape[0] == L_x.shape[0]\n  assert b.shape[0] == L_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n\n\n\nThe Jacobian-vector product\n\nfrom jax._src import ad_util\nfrom jax.interpreters import ad\nfrom jax import lax\nfrom jax.experimental import sparse as jsparse\n\ndef sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent, *, transpose):\n  \"\"\"\n  A jax-traceable jacobian-vector product. In order to make it traceable, \n  we use the experimental sparse CSC matrix in JAX.\n  \n  Input:\n    arg_values:   A tuple of (L_indices, L_indptr, L_x, b) that describe\n                  the triangular matrix L and the rhs vector b\n    arg_tangent:  A tuple of tangent values (same lenght as arg_values).\n                  The first two values are nonsense - we don't differentiate\n                  wrt integers!\n    transpose:    (boolean) If true, solve L^Tx = b. Otherwise solve Lx = b.\n  Output:         A tuple containing the maybe_transpose(L)^{-1}b and the corresponding\n                  Jacobian-vector product.\n  \"\"\"\n  L_indices, L_indptr, L_x, b = arg_values\n  _, _, L_xt, bt = arg_tangent\n  value = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose=transpose)\n  if type(bt) is ad.Zero and type(L_xt) is ad.Zero:\n    # I legit do not think this ever happens. But I'm honestly not sure.\n    print(\"I have arrived!\")\n    return value, lax.zeros_like_array(value) \n  \n  if type(L_xt) is not ad.Zero:\n    # L is variable\n    if transpose:\n      Delta = jsparse.CSC((L_xt, L_indices, L_indptr), shape = (b.shape[0], b.shape[0])).transpose()\n    else:\n      Delta = jsparse.CSC((L_xt, L_indices, L_indptr), shape = (b.shape[0], b.shape[0]))\n\n    jvp_Lx = sparse_triangular_solve(L_indices, L_indptr, L_x, Delta @ value, transpose = transpose) \n  else:\n    jvp_Lx = lax.zeros_like_array(value) \n\n  if type(bt) is not ad.Zero:\n    # b is variable\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, bt, transpose = transpose)\n  else:\n    jvp_b = lax.zeros_like_array(value)\n\n  return value, jvp_b - jvp_Lx\n\nad.primitive_jvps[sparse_triangular_solve_p] = sparse_triangular_solve_value_and_jvp\n\nBefore we see if this works, let’s first have talk about the structure of the function I just wrote. Generally speaking, we want a function that takes in the primals and tangents at tuples and then returns the value and the9 Jacobian-vector product.\nThe main thing you will notice in the code is that there is a lot of checking for ad.Zero. This is a special type defined in JAX that is, essentially, telling the autodiff system that we are not differentiating wrt that variable. This is different to a tangent that just happens to be numerically equal to zero. Any code for a Jacobian-vector product needs to handle this special value.\nAs we have two arguments, we have 3 interesting options:\n\nBoth L_xt and bt are ad.Zero: This means the function is a constant and the derivative is zero. I am fairly certain that we do not need to manually handle this case, but because I don’t know and I do not like surprises, it’s in there.\nL_xt is not ad.Zero: This means that we need to differentiate wrt the matrix. In this case we need to compute \\(\\Delta c\\) or \\(\\Delta^T c\\), depending on the transpose argument. In order to do this, I used the jax.experimental.sparse.CSC class, which has some very limited sparse matrix support (basically matrix-vector products). This is extremely convenient because it means I don’t need to write the matrix-vector product myself!\nbt is not ad.Zero: This means that we need to differentiate wrt the rhs vector. This part of the formula is pretty straightforward: just an application of the primal.\n\nIn the case that either L_xt or bt are ad.Zero, we simply set the corresponding contribution to the jvp to zero.\nIt’s worth saying that you can bypass all of this ad.Zero logic by writing separate functions for the JVP contribution from each input and then chaining them together using10 ad.defjvp2() to chain them together. This is what the lax.linalg.triangular_solve() implementation does.\nSo why didn’t I do this? I avoided this because in the other primitives I have to implement, there are expensive computations (like Cholesky factorisations) that I want to share between the primal and the various tangent calculations. The ad.defjvp frameworks don’t allow for that. So I decided not to demonstrate/learn two separate patterns.\n\n\nTransposition\nNow I’ve never actively wanted a Jacobian-vector product in my whole life. I’m sorry. I want a gradient. Gimme a gradient. I am the Veruca Salt of gradients.\nIn may autodiff systems, if you want11 a gradient, you need to implement vector-Jacobian products12 explicitly.\nOne of the odder little innovations in JAX is that instead of forcing you to implement this as well13, you only need to implement half of it.\nYou see, some clever analysis that, as far as I far as I can tell14, is detailed in this paper shows that you only need to form explicit vector-Jacobian products for the structurally linear arguments of the function.\nIn JAX (and maybe elsewhere), this is known as a transposition rule. The combination of a transopition rule and a JAX-traceable Jacobian-vector product is enough for JAX to compute all of the directional derivatives and gradients we could ever hope for.\nAs far as I understand, it is all about functions that are structurally linear in some arguments. For instance, if \\(A(x)\\) is a matrix-valued function and \\(x\\) and \\(y\\) are vectors, then the function \\[\nf(x, y) = A(x)y + g(x)\n\\] is structurally linear in \\(y\\) in the sense that for every fixed value of \\(x\\), the function \\[\nf_x(y) = A(x) y + g(x)\n\\] is linear in \\(y\\). The resulting transpositon rule is then\n\ndef f_transpose(x, y):\n  Ax = A(x)\n  gx = g(x)\n  return (None, Ax.T @ y + gx)\n\nThe first element of the return is None because \\(f(x,y)\\) is not15 structurally linear in \\(x\\) so there is nothing to transpose. The second element simply takes the matrix in the linear function and transposes it.\nIf you know anything about autodiff, you’ll think “this doesn’t feel like enough” and it’s not. JAX deals with the non-linear part of \\(f(x,y)\\) by tracing the evaluation tree for its Jacobian-vector product and … manipulating16 it.\nWe already built the abstract evaluation function last time around, so the tracing part can be done. All we need is the transposition rule.\nThe linear solve \\(f(A, b) = A^{-1}b\\) is non-linear in the first argument but linear in the second argument. So we only need to implement \\[\nJ^T_b(A,b)w = A^{-T}w,\n\\] where the subscript \\(b\\) indicates we’re only computing the Jacobian wrt \\(b\\).\nInitially, I struggled to work out what needed to be implemented here. The thing that clarified the process for me was looking at JAX’s internal implementation of the Jacobian-vector product for a dense matrix. From there, I understood what this had to look like for a vector-valued function and this is the result.\n\ndef sparse_triangular_solve_transpose_rule(cotangent, L_indices, L_indptr, L_x, b, *, transpose):\n  \"\"\"\n  Transposition rule for the triangular solve. \n  Translated from here https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747.\n  Inputs:\n    cotangent: Output cotangent (aka adjoint). (produced by JAX)\n    L_indices, L_indptr, L_x: Represenation of sparse matrix. L_x should be concrete\n    b: The right hand side. Must be an jax.interpreters.ad.UndefinedPrimal\n    transpose: (boolean) True: solve $L^Tx = b$. False: Solve $Lx = b$.\n  Output:\n    A 4-tuple with the adjoints (None, None, None, b_adjoint)\n  \"\"\"\n  assert not ad.is_undefined_primal(L_x) and ad.is_undefined_primal(b)\n  if type(cotangent) is ad_util.Zero:\n    cot_b = ad_util.Zero(b.aval)\n  else:\n    cot_b = sparse_triangular_solve(L_indices, L_indptr, L_x, cotangent, transpose = not transpose)\n  return None, None, None, cot_b\n\nad.primitive_transposes[sparse_triangular_solve_p] = sparse_triangular_solve_transpose_rule\n\nIf this doesn’t make a lot of sense to you, that’s because it’s confusing.\nOne way to think of it is in terms of the more ordinary notation. Mike Giles has a classic paper that covers these results for basic linear algebra. The idea is to imagine that, as part of your larger program, you need to compute \\(c = A^{-1}b\\).\nForward-mode autodiff computes the sensitivity of \\(c\\), usually denoted \\(\\dot c\\) from the sensitivies \\(\\dot A\\) and \\(\\dot b\\). These have already been computed. The formula in Giles is \\[\n\\dot c = A^{-1}(\\dot b - \\dot A c).\n\\] The canny reader will recognise this as exactly17 the formula for the Jacobian-vector product.\nSo what does reverse-mode autodiff do? Well it moves through the program in the other direction. So instead of starting with the sensitivities \\(\\dot A\\) and \\(\\dot b\\) already computed, we instead start with the18 adjoint sensitivity \\(\\bar c\\). Our aim is to compute \\(\\bar A\\) and \\(\\bar b\\) from \\(\\bar c\\).\nThe details of how to do this are19 beyond the scope, but without tooooooo much effort you can show that \\[\n\\bar b = A^{-T} \\bar c,\n\\] which you should recognise as the equation that was just implemented.\nThe thing that we do not have to implement in JAX is the other adjoint that, for dense matrices20, is \\[\n\\bar{A} = -\\bar{b}c^T.\n\\] Through the healing power of … something?—Truly I do not know.— JAX can work that bit out itself. woo.\n\n\nTesting the numerical implementation of the Jacobian-vector product\nSo let’s see if this works. I’m not going to lie, I’m flying by the seat of my pants here. I’m not super familiar with the JAX internals, so I have written a lot of test cases. You may wish to skip this part. But rest assured that almost every single one of these cases was useful to me working out how this thing actually worked!\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n)).tocsc()\n    A_lower = sparse.tril(A, format = \"csc\")\n    A_index = A_lower.indices\n    A_indptr = A_lower.indptr\n    A_x = A_lower.data\n    return (A_index, A_indptr, A_x, A)\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\n\nThis is the same test case as the last blog. We will just use the lower triangle of \\(A\\) as the test matrix.\nFirst things first, let’s check out the numerical implementation of the function. We will do that by comparing the implemented Jacobian-vector product with the definition of the Jacobian-vector product (aka the forward21 difference approximation).\nThere are lots of things that we could do here to turn these into actual tests. For instance, the test suite inside JAX has a lot of nice convenience functions for checking implementations of derivatives. But I went with homespun because that was how I was feeling.\nYou’ll also notice that I’m using random numbers here, which is fine for a blog. Not so fine for a test that you don’t want to be potentially22 flaky.\nThe choice of eps = 1e-4 is roughly23 because it’s the square root of the single precision machine epsilon24. A very rough back of the envelope calculation for the forward difference approximation to the derivative shows that the square root of the machine epislon is about the size you want your perturbation to be.\n\nb = np.random.standard_normal(100)\n\nbt = np.random.standard_normal(100)\nbt /= np.linalg.norm(bt)\n\nA_xt = np.random.standard_normal(len(A_x))\nA_xt /= np.linalg.norm(A_xt)\n\narg_values = (A_indices, A_indptr, A_x, b )\n\narg_tangent_A = (None, None, A_xt, ad.Zero(type(b)))\narg_tangent_b = (None, None, ad.Zero(type(A_xt)), bt)\narg_tangent_Ab = (None, None, A_xt, bt)\n\np, t_A = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose = False)\n_, t_b = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose = False)\n_, t_Ab = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_Ab, transpose = False)\npT, t_AT = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose = True)\n_, t_bT = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose = True)\n\neps = 1e-4\ntt_A = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b) - p) /eps\ntt_b = (sparse_triangular_solve(A_indices, A_indptr, A_x, b + eps * bt) - p) / eps\ntt_Ab = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b + eps * bt) - p) / eps\ntt_AT = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b, transpose = True) - pT) / eps\ntt_bT = (sparse_triangular_solve(A_indices, A_indptr, A_x, b + eps * bt, transpose = True) - pT) / eps\n\nprint(f\"\"\"\nTranspose = False:\n  Error A varying: {np.linalg.norm(t_A - tt_A): .2e}\n  Error b varying: {np.linalg.norm(t_b - tt_b): .2e}\n  Error A and b varying: {np.linalg.norm(t_Ab - tt_Ab): .2e}\n\nTranspose = True:\n  Error A varying: {np.linalg.norm(t_AT - tt_AT): .2e}\n  Error b varying: {np.linalg.norm(t_bT - tt_bT): .2e}\n\"\"\")\n\n\nTranspose = False:\n  Error A varying:  1.08e-07\n  Error b varying:  0.00e+00\n  Error A and b varying:  4.19e-07\n\nTranspose = True:\n  Error A varying:  1.15e-07\n  Error b varying:  0.00e+00\n\n\n\nBrilliant! Everythign correct withing single precision!\n\n\nChecking on the plumbing\nMaking the numerical implementation work is only half the battle. We also have to make it work in the context of JAX.\nNow I would be lying if I pretended this process went smoothly. But the first time is for experience. It’s mostly a matter of just reading the documentation carefully and going through similar examples that have already been implemented.\nAnd testing. I learnt how this was supposed to work by testing it.\n(For full disclosure, I also wrote a big block f-string in the sparse_triangular_solve() function at one point that told me the types, shapes, and what transpose was, which was how I worked out that my code was breaking because I forgot the first to None outputs in the transposition rule. When it doubt, print shit.)\nAs you will see from my testing code, I was not going for elegance. I was running the damn permutations. If you’re looking for elegance, look elsewhere.\n\nfrom jax import jvp, grad\nfrom jax import scipy as jsp\n\ndef f(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[20]].add(theta[0])\n  Ax_theta = Ax_theta.at[A_indptr[50]].add(theta[1])\n  b = jnp.ones(100)\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = True)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  Ax_theta = Ax_theta.at[20,20].add(theta[0])\n  Ax_theta = Ax_theta.at[50,50].add(theta[1])\n  b = jnp.ones(100)\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"T\")\n\ndef g(theta):\n  Ax_theta = jnp.array(A_x)\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = True)\n\ndef g_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"T\")\n\ndef h(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[20]].add(theta[0]) \n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = False)\n\ndef h_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  Ax_theta = Ax_theta.at[20,20].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"N\")\n\ndef no_diff(theta):\n  return sparse_triangular_solve(A_indices, A_indptr, A_x, jnp.ones(100), transpose = False)\n\ndef no_diff_jax(theta):\n  return jsp.linalg.solve_triangular(jnp.array(sparse.tril(A).todense()), jnp.ones(100), lower = True, trans = \"N\")\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\nprimal1, jvp1 = jvp(f, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad1 = grad(lambda x: jnp.mean(f(x)))(jnp.array([-142., 342.]))\ngrad2 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([-142., 342.]))\n\nprimal3, jvp3 = jvp(g, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal4, jvp4 = jvp(g_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad3 = grad(lambda x: jnp.mean(g(x)))(jnp.array([-142., 342.]))\ngrad4 = grad(lambda x: jnp.mean(g_jax(x)))(jnp.array([-142., 342.]))  \n\nprimal5, jvp5 = jvp(h, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal6, jvp6 = jvp(h_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad5 = grad(lambda x: jnp.mean(h(x)))(jnp.array([-142., 342.]))\ngrad6 = grad(lambda x: jnp.mean(h_jax(x)))(jnp.array([-142., 342.]))\n\nprimal7, jvp7 = jvp(no_diff, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal8, jvp8 = jvp(no_diff_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad7 = grad(lambda x: jnp.mean(no_diff(x)))(jnp.array([-142., 342.]))\ngrad8 = grad(lambda x: jnp.mean(no_diff_jax(x)))(jnp.array([-142., 342.]))\n\nprint(f\"\"\"\nVariable L:\n  Primal difference: {np.linalg.norm(primal1 - primal2): .2e}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2): .2e}\n  Gradient difference: {np.linalg.norm(grad1 - grad2): .2e}\n\nVariable b:\n  Primal difference: {np.linalg.norm(primal3 - primal4): .2e}\n  JVP difference: {np.linalg.norm(jvp3 - jvp4): .2e}\n  Gradient difference: {np.linalg.norm(grad3 - grad4): .2e} \n\nVariable L and b:\n  Primal difference: {np.linalg.norm(primal5 - primal6): .2e}\n  JVP difference: {np.linalg.norm(jvp5 - jvp6): .2e}\n  Gradient difference: {np.linalg.norm(grad5 - grad6): .2e}\n\nNo diff:\n  Primal difference: {np.linalg.norm(primal7 - primal8)}\n  JVP difference: {np.linalg.norm(jvp7 - jvp8)}\n  Gradient difference: {np.linalg.norm(grad7 - grad8)}\n\"\"\")\n\n\nVariable L:\n  Primal difference:  1.98e-07\n  JVP difference:  2.58e-12\n  Gradient difference:  0.00e+00\n\nVariable b:\n  Primal difference:  7.94e-06\n  JVP difference:  1.83e-08\n  Gradient difference:  3.29e-10 \n\nVariable L and b:\n  Primal difference:  2.08e-06\n  JVP difference:  1.08e-08\n  Gradient difference:  2.33e-10\n\nNo diff:\n  Primal difference: 2.2101993124579167e-07\n  JVP difference: 0.0\n  Gradient difference: 0.0\n\n\n\nStunning!"
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-one-the-general-a-1b",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-one-the-general-a-1b",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Primitive one: The general \\(A^{-1}b\\)",
    "text": "Primitive one: The general \\(A^{-1}b\\)\nOk. So this is a very similar problem to the one that we just solved. But, as fate would have it, the solution is going to look quite different. Why? Because we need to compute a Cholesky factorisation.\nFirst things first, though, we are going to need a JAX-traceable way to compute a Cholesky factor. This means that we need25 to tell our sparse_solve function the how many non-zeros the sparse Cholesky will have. Why? Well. It has to do with how the function is used.\nWhen sparse_cholesky() is called with concrete inputs26, then it can quite happily work out the sparsity structure of \\(L\\). But when JAX is preparing to transform the code, eg when it’s building a gradient, it calls sparse_cholesky() using abstract arguments that only share the shape information from the inputs. This is not enough to compute the sparsity structure. We need the indices and indptr arrays.\nThis means that we need sparse_cholesky() to throw an error if L_nse isn’t passed. This wasn’t implemented well last time, so here it is done properly.\n(If you’re wondering about that None argument, it is the identity transform. So if A_indices is a concrete value, ind = A_indices. Otherwise an error is called.)\n\nsparse_cholesky_p = core.Primitive(\"sparse_cholesky\")\n\ndef sparse_cholesky(A_indices, A_indptr, A_x, *, L_nse: int = None):\n  \"\"\"A JAX traceable sparse cholesky decomposition\"\"\"\n  if L_nse is None:\n    err_string = \"You need to pass a value to L_nse when doing fancy sparse_cholesky.\"\n    ind = core.concrete_or_error(None, A_indices, err_string)\n    ptr = core.concrete_or_error(None, A_indptr, err_string)\n    L_ind, _ = _symbolic_factor(ind, ptr)\n    L_nse = len(L_ind)\n  \n  return sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse = L_nse)\n\n\n\nThe rest of the Choleksy code\n\n\n@sparse_cholesky_p.def_impl\ndef sparse_cholesky_impl(A_indices, A_indptr, A_x, *, L_nse):\n  \"\"\"The implementation of the sparse cholesky This is not JAX traceable.\"\"\"\n  \n  L_indices, L_indptr= _symbolic_factor(A_indices, A_indptr)\n  if L_nse is not None:\n    assert len(L_indices) == L_nse\n    \n  L_x = _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)\n  L_x = _sparse_cholesky_impl(L_indices, L_indptr, L_x)\n  return L_indices, L_indptr, L_x\n\ndef _symbolic_factor(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] &gt; j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) &gt; 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\n\n\ndef _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\ndef _sparse_cholesky_impl(L_indices, L_indptr, L_x):\n  n = len(L_indptr) - 1\n  descendant = [[] for j in range(0, n)]\n  for j in range(0, n):\n    tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n    for bebe in descendant[j]:\n      k = bebe[0]\n      Ljk= L_x[bebe[1]]\n      pad = np.nonzero(                                                       \\\n          L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n      update_idx = np.nonzero(np.in1d(                                        \\\n                    L_indices[L_indptr[j]:L_indptr[j+1]],                     \\\n                    L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n      tmp[update_idx] = tmp[update_idx] -                                     \\\n                        Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n    diag = np.sqrt(tmp[0])\n    L_x[L_indptr[j]] = diag\n    L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n    for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n      descendant[L_indices[idx]].append((j, idx))\n  return L_x\n\n@sparse_cholesky_p.def_abstract_eval\ndef sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, *, L_nse):\n  return core.ShapedArray((L_nse,), A_indices.dtype),                   \\\n         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             \\\n         core.ShapedArray((L_nse,), A_x.dtype)\n\n\n\nWhy do we need a new pattern for this very very similar problem?\nOk. So now on to the details. If we try to repeat our previous pattern it would look like this.\n\ndef sparse_solve_value_and_jvp(arg_values, arg_tangents, *, L_nse):\n  \"\"\" \n  Jax-traceable jacobian-vector product implmentation for sparse_solve.\n  \"\"\"\n  \n  A_indices, A_indptr, A_x, b = arg_values\n  _, _, A_xt, bt = arg_tangents\n\n  # Needed for shared computation\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n\n  # Make the primal\n  primal_out = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose = False)\n  primal_out = sparse_triangular_solve(L_indices, L_indptr, L_x, primal_out, transpose = True)\n\n  if type(A_xt) is not ad.Zero:\n    Delta_lower = jsparse.CSC((A_xt, A_indices, A_indptr), shape = (b.shape[0], b.shape[0]))\n    # We need to do Delta @ primal_out, but we only have the lower triangle\n    rhs = Delta_lower @ primal_out + Delta_lower.transpose() @ primal_out - A_xt[A_indptr[:-1]] * primal_out\n    jvp_Ax = sparse_triangular_solve(L_indices, L_indptr, L_x, rhs)\n    jvp_Ax = sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_Ax, transpose = True)\n  else:\n    jvp_Ax = lax.zeros_like_array(primal_out)\n\n  if type(bt) is not ad.Zero:\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, bt)\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_b, transpose = True)\n  else:\n    jvp_b = lax.zeros_like_array(primal_out)\n\n  return primal_out, jvp_b - jvp_Ax\n\nThat’s all well and good. Nothing weird there.\nThe problem comes when you need to implement the transposition rule. Remembering that \\(\\bar b = A^{-T}\\bar c = A^{-1}\\bar c\\), you might see the issue: we are going to need the Cholesky factorisation. But we have no way to pass \\(L\\) to the transpose function.\nThis means that we would need to compute two Cholesky factorisations per gradient instead of one. As the Cholesky factorisation is our slowest operation, we do not want to do extra ones! We want to compute the Cholesky triangle once and pass it around like a party bottom27. We do not want each of our functions to have to make a deep and meaningful connection with the damn matrix28.\n\n\nA different solution\nSo how do we pass around our Cholesky triangle? Well, I do love a good class so my first thought was “fuck it. I’ll make a class and I’ll pass it that way”. But the developers of JAX had a much better idea.\nTheir idea was to abstract the idea of a linear solve and its gradients. They do this through lax.custom_linear_solve. This is a function that takes all of the bits that you would need to compute \\(A^{-1}b\\) and all of its derivatives. In particular it takes29:\n\nmatvec: A function that matvec(x) that computes \\(Ax\\). This might seem a bit weird, but it’s the most common atrocity committed by mathematicians is abstracting30 a matrix to a linear mapping. So we might as well just suck it up.\nb: The right hand side vector31\nsolve: A function that takes takes the matvec and a vector so that32 solve(matvec, matvec(x)) == x\nsymmetric: A boolean indicating if \\(A\\) is symmetric.\n\nThe idea (happily copped from the implementation of jax.scipy.linalg.solve) is to wrap our Cholesky decomposition in the solve function. Through the never ending miracle of partial evaluation.\n\nfrom functools import partial\n\ndef sparse_solve(A_indices, A_indptr, A_x, b, *, L_nse = None):\n  \"\"\"\n  A JAX-traceable sparse solve. For this moment, only for vector b\n  \"\"\"\n  assert b.shape[0] == A_indptr.shape[0] - 1\n  assert b.ndim == 1\n  \n  L_indices, L_indptr, L_x = sparse_cholesky(\n    lax.stop_gradient(A_indices), \n    lax.stop_gradient(A_indptr), \n    lax.stop_gradient(A_x), L_nse = L_nse)\n  \n  def chol_solve(L_indices, L_indptr, L_x, b):\n    out = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose = False)\n    return sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose = True)\n  \n  def matmult(A_indices, A_indptr, A_x, b):\n    A_lower = jsparse.CSC((A_x, A_indices, A_indptr), shape = (b.shape[0], b.shape[0]))\n    return A_lower @ b + A_lower.transpose() @ b - A_x[A_indptr[:-1]] * b\n\n  solver = partial(\n    lax.custom_linear_solve,\n    lambda x: matmult(A_indices, A_indptr, A_x, x),\n    solve = lambda _, x: chol_solve(L_indices, L_indptr, L_x, x),\n    symmetric = True)\n\n  return solver(b)\n\nThere are three things of note in that implementation.\n\nThe calls to lax.stop_gradient(): These tell JAX to not bother computing the gradient of these terms. The relevant parts of the derivatives are computed explicitly by lax.custom_linear_solve in terms of matmult and solve, neither of which need the explicit derivative of the cholesky factorisation.!\nThat definition of matmult()33: Look. I don’t know what to tell you. Neither addition nor indexing is implemented for jsparse.CSC objects. So we did it the semi-manual way. (I am thankful that matrix-vector multiplication is available)\nThe definition of solver(): Partial evaluation is a wonderful wonderful thing. functools.partial() transforms lax.custom_linear_solve() from a function that takes 3 arguments (and some keywords), into a function solver() that takes one34 argument35 (b, the only positional argument of lax.custom_linear_solve() that isn’t specified).\n\n\n\nDoes it work?\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x)\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  b = jnp.ones(100)\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense())\n  Ax_theta = Ax_theta.at[np.arange(100),np.arange(100)].add(theta[1])\n  b = jnp.ones(100)\n  return jsp.linalg.solve(Ax_theta, b)\n\ndef g(theta):\n  Ax_theta = jnp.array(A_x)\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef g_jax(theta):\n  Ax_theta = jnp.array(A.todense())\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve(Ax_theta, b)\n\ndef h(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef h_jax(theta):\n  Ax_theta = jnp.array(A.todense())\n  Ax_theta = Ax_theta.at[np.arange(100),np.arange(100)].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve(Ax_theta, b)\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\ngrad1 = grad(lambda x: jnp.mean(f(x)))(jnp.array([2., 3.]))\ngrad2 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([2., 3.]))\n\n\nprimal3, jvp3 = jvp(g, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal4, jvp4 = jvp(g_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad3 = grad(lambda x: jnp.mean(g(x)))(jnp.array([-142., 342.]))\ngrad4 = grad(lambda x: jnp.mean(g_jax(x)))(jnp.array([-142., 342.]))\n\nprimal5, jvp5 = jvp(h, (jnp.array([2., 342.]),), (jnp.array([1., 2.]),))\nprimal6, jvp6 = jvp(h_jax, (jnp.array([2., 342.]),), (jnp.array([1., 2.]),))\ngrad5 = grad(lambda x: jnp.mean(f(x)))(jnp.array([2., 342.]))\ngrad6 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([2., 342.]))\n\nprint(f\"\"\"\nCheck the plumbing!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2): .2e}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2): .2e}\n  Gradient difference: {np.linalg.norm(grad1 - grad2): .2e}\n  \nVariable b:\n  Primal difference: {np.linalg.norm(primal3 - primal4): .2e}\n  JVP difference: {np.linalg.norm(jvp3 - jvp4): .2e}\n  Gradient difference: {np.linalg.norm(grad3 - grad4): .2e} \n    \nVariable A and b:\n  Primal difference: {np.linalg.norm(primal5 - primal6): .2e}\n  JVP difference: {np.linalg.norm(jvp5 - jvp6): .2e}\n  Gradient difference: {np.linalg.norm(grad5 - grad6): .2e}\n  \"\"\")\n\n\nCheck the plumbing!\nVariable A:\n  Primal difference:  1.98e-07\n  JVP difference:  1.43e-07\n  Gradient difference:  0.00e+00\n  \nVariable b:\n  Primal difference:  4.56e-06\n  JVP difference:  6.52e-08\n  Gradient difference:  9.31e-10 \n    \nVariable A and b:\n  Primal difference:  8.10e-06\n  JVP difference:  1.83e-06\n  Gradient difference:  1.82e-12\n  \n\n\nYes.\n\n\nWhy is this better than just differentiating through the Cholesky factorisation?\nThe other option for making this work would’ve been to implement the Cholesky factorisation as a primitive (~which we are about to do!~ which we will do another day) and then write the sparse solver directly as a pure JAX function.\n\ndef sparse_solve_direct(A_indices, A_indptr, A_x, b, *, L_nse = None):\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  out = sparse_triangular_solve(L_indices, L_indptr, L_x, b)\n  return sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose = True)\n\nThis function is JAX-traceable36 and, therefore, we could compute the gradient of it directly. It turns out that this is going to be a bad idea.\nWhy? Because the derivative of sparse_cholesky, which we would have to chain together with the derivatives from the solver, is pretty complicated. Basically, this means that we’d have to do a lot more work37 than we do if we just implement the symbolic formula for the derivatives."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-three-the-dreaded-log-determinant",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-three-the-dreaded-log-determinant",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Primitive three: The dreaded log determinant",
    "text": "Primitive three: The dreaded log determinant\nOk, so now we get to the good one. The log-determinant of \\(A\\). The first thing that we need to do is wrench out a derivative. This is not as easy as it was for the linear solve. So what follows is a modification for sparse matrices from Appendix A of Boyd’s convex optimisation book.\nIt’s pretty easy to convince yourself that \\[\\begin{align*}\n\\log(|A + \\Delta|) &= \\log\\left( \\left|A^{1/2}(I + A^{-1/2}\\Delta A^{-1/2})A^{1/2}\\right|\\right) \\\\\n&= \\log(|A|) + \\log\\left( \\left|I + A^{-1/2}\\Delta A^{-1/2}\\right|\\right).\n\\end{align*}\\]\nIt is harder to convince yourself how this could possibly be a useful fact.\nIf we write \\(\\lambda_i\\), \\(i = 1, \\ldots, n\\) as the eigenvalues of \\(A^{-1/2}\\Delta A^{-1/2}\\), then we have \\[\n\\log(|A + \\Delta |) = \\log(|A|) + \\sum_{i=1}^n \\log( 1 + \\lambda_i).\n\\] Remembering that \\(\\Delta\\) is very small, it follows that \\(A^{-1/2}\\Delta A^{-1/2}\\) will also be small. That translates to the eigenvalues of \\(A^{-1/2}\\Delta A^{-1/2}\\) all being small. Therefore, we can use the approximation \\(\\log(1 + \\lambda_i)  = \\lambda_i  + \\mathcal{O}(\\lambda_i^2)\\).\nThis means that38 \\[\\begin{align*}\n\\log(|A + \\Delta |) &= \\log(|A|) + \\sum_{i=1}^n  \\lambda_i + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right) \\\\\n&=\\log(|A|) + \\operatorname{tr}\\left(A^{-1/2} \\Delta A^{-1} \\right) + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right) \\\\\n&= \\log(|A|) + \\operatorname{tr}\\left(A^{-1} \\Delta \\right) + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right),\n\\end{align*}\\] which follows from the cyclic property of the trace.\nIf we recall the formula from the last section defining the Jacobian-vector product, in our context \\(m = 1\\), \\(x\\) is the vector of non-zero entries of the lower triangle of \\(A\\) stacked by column, and \\(\\delta\\) is the vector of non-zero entries of the lower triangle of \\(\\Delta\\). That means the Jacobian-vector product is \\[\nJ(x)\\delta = \\operatorname{tr}\\left(A^{-1} \\Delta \\right) = \\sum_{i=1}^n\\sum_{j=1}^n[A^{-1}]_{ij} \\Delta_{ij}.\n\\]\nRemembering that \\(\\Delta\\) is sparse with the same sparsity pattern as \\(A\\), we see that the Jacobian-vector product requires us to know the values of \\(A^{-1}\\) that correspond to non-zero elements of \\(A\\). That’s good news because we will see that these entries are relatively cheap and easy to compute. Whereas the full inverse is dense and very expensive to compute.\nBut before we get to that, I need to point out a trap for young players39. Lest your implementations go down faster than me when someone asks politely.\nThe problem comes from how we store our matrix. A mathematician would suggest that it’s our representation. A physicist40 would shit on about being coordinate free with such passion that he41 will keep going even after you quietly leave the room.\nThe problem is that we only store the non-zero entries of the lower-triangular part of \\(A\\). This means that we need to be careful that when we compute the Jacobian-vector product that we properly compute the Matrix-vector product.\nLet A_indices and A_indptr define the sparsity structure of \\(A\\) (and \\(\\Delta\\)). Then if \\(A_x\\) is our input and \\(v\\) is our vector, then we need to do the follow steps to compute the Jacobian-vector product:\n\nCompute Ainv_x (aka the non-zero elements of \\(A^{-1}\\) that correspond to the sparsity pattern of \\(A\\))\nCompute the matrix vector product as\n\n\njvp = 2 * sum(Ainv_x * v) - sum(Ainv_x[A_indptr[:-1]] * v[A_indptr[:-1]])\n\nWhy does it look like that? Well we need to add the contribution from the upper triangle as well as the lower triangle. And one way to do that is to just double the sum and then subtract off the diagonal terms that we’ve counted twice.\n(I’m making a pretty big assumption here, which is fine in our context, that \\(A\\) has a non-zero diagonal. If that doesn’t hold, it’s just a change of the indexing in the second term to just pull out the diagonal terms.)\nUsing similar reasoning, we can compute the Jacobian as \\[\n[J_f(x)]_{i1} = \\begin{cases}\n\\operatorname{partial-inverse}(x)_i, \\qquad & x_i  \\text{ is a diagonal element of }A \\\\\n2\\operatorname{partial-inverse}(x)_i, \\qquad & \\text{otherwise},\n\\end{cases}\n\\] where \\(\\operatorname{partial-inverse}(x)\\) is the vector that stacks the columns of the elements of \\(A^{-1}\\) that correspond to the non-zero elements of \\(A\\). (Yikes!)\n\nComputing the partial inverse\nSo now we need to actually work out how to compute this partial inverse of a symmetric positive definite matrix \\(A\\). To do this, we are going to steal a technique that goes back to Takahashi, Fagan, and Chen42 in 1973. (For this presentation, I’m basically pillaging Håvard Rue and Sara Martino’s 2007 paper.)\nTheir idea was that if we write \\(A = VDV^T\\), where \\(V\\) is a lower-triangular matrix with ones on the diagonal and \\(D\\) is diagonal. This links up with our usual Cholesky factorisation through the identity \\(L = VD^{1/2}\\). It follows that if \\(S = A^{-1}\\), then \\(VDV^TS = I\\). Then, we make some magic manipulations43. \\[\\begin{align*}\nV^TS &= D^{-1}V^{-1} \\\\\nS + V^TS &= S + D^{-1}V^{-1} \\\\\nS &= D^{-1}V^{-1} + (I - V^T)S.\n\\end{align*}\\]\nOnce again, this does not look super-useful. The trick is to notice 2 things.\n\nBecause \\(V\\) is lower triangular, \\(V^{-1}\\) is also lower triangular and the elements of \\(V^{-1}\\) are the inverse of the diagonal elements of \\(V\\) (aka they are all 1). Therefore, \\(D^{-1}V^{-1}\\) is a lower triangular matrix with a diagonal given by the diagonal of \\(D^{-1}\\).\n\\(I - V^T\\) is an upper triangular matrix and \\([I - V^T]_{nn} = 0\\).\n\nThese two things together lead to the somewhat unexpected situation where the upper triangle of \\(S = D^{-1}V^{-1} + (I-  V^T)S\\) defines a set of recursions for the upper triangle of \\(S\\). (And, therefore, all of \\(S\\) because \\(S\\) is symmetric!) These are sometimes referred to as the Takahashi recursions.\nBut we don’t want the whole upper triangle of \\(S\\), we just want the ones that correspond to the non-zero elements of \\(A\\). Unfortunately, the set of recursions are not, in general, solveable using only that subset of \\(S\\). But we are in luck: they are solveable using the elements of \\(S\\) that correspond to the non-zeros of \\(L + L^T\\), which, as we know from a few posts ago, is a superset of the non-zero elements of \\(A\\)!\nFrom this, we get the recursions running from \\(i = n, \\ldots, 1\\), \\(j = n, \\ldots, i\\) (the order is important!) such that \\(L_{ji} \\neq 0\\) \\[\nS_{ji} =   \\begin{cases}\n\\frac{1}{L_{ii}^2} - \\frac{1}{L_{ii}}\\sum_{k=i+1}^{n} L_{ki} S_{kj} \\qquad&  \\text{if } i=j, \\\\         \n- \\frac{1}{L_{ii}}\\sum_{k=i+1}^{n} L_{ki} S_{kj}  & \\text{otherwise}.\n\\end{cases}\n\\]\nIf you recall our discussion way back when about the way the non-zero structure of the \\(j\\) the column of \\(L\\) relates to the non-zero structure of the \\(i\\) th column for \\(j \\geq i\\), it’s clear that we have computed enough44 of \\(S\\) at every step to complete the recursions.\nNow we just need to Python it. (And thanks to Finn Lindgren who helped me understand how to implement this, which he may or may not remember because it happened about five years ago.)\nActually, we need this to be JAX-traceable, so we are going to implement a very basic primitive. In particular, we don’t need to implement a derivative or anything like that, just an abstract evaluation and an implementation.\n\nsparse_partial_inverse_p = core.Primitive(\"sparse_partial_inverse\")\n\ndef sparse_partial_inverse(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  \"\"\"\n  Computes the elements (out_indices, out_indptr) of the inverse of a sparse matrix (A_indices, A_indptr, A_x)\n   with Choleksy factor (L_indices, L_indptr, L_x). (out_indices, out_indptr) is assumed to be either\n   the sparsity pattern of A or a subset of it in lower triangular form. \n  \"\"\"\n  return sparse_partial_inverse_p.bind(L_indices, L_indptr, L_x, out_indices, out_indptr)\n\n@sparse_partial_inverse_p.def_abstract_eval\ndef sparse_partial_inverse_abstract_eval(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  return abstract_arrays.ShapedArray(out_indices.shape, L_x.dtype)\n\n@sparse_partial_inverse_p.def_impl\ndef sparse_partial_inverse_impl(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  n = len(L_indptr) - 1\n  Linv = sparse.dok_array((n,n), dtype = L_x.dtype)\n  counter = len(L_x) - 1\n  for col in range(n-1, -1, -1):\n    for row in L_indices[L_indptr[col]:L_indptr[col+1]][::-1]:\n      if row != col:\n        Linv[row, col] = Linv[col, row] = 0.0\n      else:\n        Linv[row, col] = 1 / L_x[L_indptr[col]]**2\n      L_col  = L_x[L_indptr[col]+1:L_indptr[col+1]] / L_x[L_indptr[col]]\n \n      for k, L_kcol in zip(L_indices[L_indptr[col]+1:L_indptr[col+1]], L_col):\n         Linv[col,row] = Linv[row,col] =  Linv[row, col] -  L_kcol * Linv[k, row]\n        \n  Linv_x = sparse.tril(Linv, format = \"csc\").data\n  if len(out_indices) == len(L_indices):\n    return Linv_x\n\n  out_x = np.zeros(len(out_indices))\n  for col in range(n):\n    ind = np.nonzero(np.in1d(L_indices[L_indptr[col]:L_indptr[col+1]],\n      out_indices[out_indptr[col]:out_indptr[col+1]]))[0]\n    out_x[out_indptr[col]:out_indptr[col+1]] = Linv_x[L_indptr[col] + ind]\n  return out_x\n\nThe implementation makes use of the45 dictionary of keys representation of a sparse matrix from scipy.sparse. This is an efficient storage scheme when you need to modify the sparsity structure (as we are doing here) or do a lot of indexing. It would definitely be possible to implement this directly on the CSC data structure, but it gets a little bit tricky to access the elements of L_inv that are above the diagonal. The resulting code is honestly a mess and there’s lots of non-local memory access anyway, so I implemented it this way.\nBut let’s be honest: this thing is crying out for a proper symmetric matrix class with sensible reverse iterators. But hey. Python.\nThe second chunk of the code is just the opposite of our _structured_copy() function. It takes a matrix with the sparsity pattern of \\(L\\) and returns one with the sparsity pattern of out (which is assumed to be a subset, and is usually the sparsity pattern of \\(A\\) or a diagonal matrix).\nLet’s check that it works.\n\nA_indices, A_indptr, A_x, A = make_matrix(15)\nn = len(A_indptr) - 1\n\n\nL_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n\na_inv_L = sparse_partial_inverse(L_indices, L_indptr, L_x, L_indices, L_indptr)\n\ncol_counts_L = [L_indptr[i+1] - L_indptr[i] for i in range(n)]\ncols_L = np.repeat(range(n), col_counts_L)\n\ntrue_inv = np.linalg.inv(A.todense())\ntruth_L = true_inv[L_indices, cols_L]\n\na_inv_A = sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)\ncol_counts_A = [A_indptr[i+1] - A_indptr[i] for i in range(n)]\ncols_A = np.repeat(range(n), col_counts_A)\ntruth_A = true_inv[A_indices, cols_A]\n\nprint(f\"\"\"\nError in partial inverse (all of L): {np.linalg.norm(a_inv_L - truth_L): .2e}\nError in partial inverse (all of A): {np.linalg.norm(a_inv_A - truth_A): .2e}\n\"\"\")\n\n\nError in partial inverse (all of L):  1.57e-15\nError in partial inverse (all of A):  1.53e-15\n\n\n\n\n\nPutting the log-determinant together\nAll of our bits are in place, so now all we need is to implement the primitive for the log-determinant. One nice thing here is that we don’t need to implement a transposition rule as the function is not structurally linear in any of its arguments. At this point we take our small wins where we can get them.\nThere isn’t anything particularly interesting in the implementation. But do note that the trace has been implemented in a way that’s aware that we’re only storing the bottom triangle of \\(A\\).\n\nsparse_log_det_p = core.Primitive(\"sparse_log_det\")\n\ndef sparse_log_det(A_indices, A_indptr, A_x):\n  return sparse_log_det_p.bind(A_indices, A_indptr, A_x)\n\n@sparse_log_det_p.def_impl\ndef sparse_log_det_impl(A_indices, A_indptr, A_x):\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  return 2.0 * jnp.sum(jnp.log(L_x[L_indptr[:-1]]))\n\n@sparse_log_det_p.def_abstract_eval\ndef sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):\n  return abstract_arrays.ShapedArray((1,), A_x.dtype)\n\ndef sparse_log_det_value_and_jvp(arg_values, arg_tangent):\n  A_indices, A_indptr, A_x = arg_values\n  _, _, A_xt = arg_tangent\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  value = 2.0 * jnp.sum(jnp.log(L_x[L_indptr[:-1]]))\n  Ainv_x = sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)\n  jvp = 2.0 * sum(Ainv_x * A_xt) - sum(Ainv_x[A_indptr[:-1]] * A_xt[A_indptr[:-1]])\n  return value, jvp\n\nad.primitive_jvps[sparse_log_det_p] = sparse_log_det_value_and_jvp\n\nFinally, we can test it out.\n\nld_true = np.log(np.linalg.det(A.todense())) #np.sum(np.log(lu.U.diagonal()))\nprint(f\"Error in log-determinant = {ld_true - sparse_log_det(A_indices, A_indptr, A_x): .2e}\")\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x) / n\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  return sparse_log_det(A_indices, A_indptr, Ax_theta)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense()) / n \n  Ax_theta = Ax_theta.at[np.arange(n),np.arange(n)].add(theta[1])\n  L = jnp.linalg.cholesky(Ax_theta)\n  return 2.0*jnp.sum(jnp.log(jnp.diag(L)))\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\n\neps = 1e-4\njvp_fd = (f(jnp.array([2.,3.]) + eps * jnp.array([1., 2.]) ) - f(jnp.array([2.,3.]))) / eps\n\ngrad1 = grad(f)(jnp.array([2., 3.]))\ngrad2 = grad(f_jax)(jnp.array([2., 3.]))\n\nprint(f\"\"\"\nCheck the Derivatives!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2)}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2)}\n  JVP difference (FD): {np.linalg.norm(jvp1 - jvp_fd)}\n  Gradient difference: {np.linalg.norm(grad1 - grad2)}\n\"\"\")\n\nError in log-determinant =  0.00e+00\n\n\n\nCheck the Derivatives!\nVariable A:\n  Primal difference: 0.0\n  JVP difference: 0.000885009765625\n  JVP difference (FD): 0.221893310546875\n  Gradient difference: 1.526623782410752e-05\n\n\n\nI’m not going to lie, I am not happy with that JVP difference. I was somewhat concerned that there was a bug somewhere in my code. I did a little bit of exploring and the error got larger as the problem got larger. It also depended a little bit more than I was comfortable on how I had implemented46 the baseline dense version.\nThat second fact suggested to me that it might be a floating point problem. By default, JAX uses single precision (32-bit) floating point. Most modern systems that don’t try and run on GPUs use double precision (64-bit) floating point. So I tried it with double precision and lo and behold, the problem disappears.\nMatrix factorisations are bloody hard in single precision.\n\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\nld_true = np.log(np.linalg.det(A.todense())) #np.sum(np.log(lu.U.diagonal()))\nprint(f\"Error in log-determinant = {ld_true - sparse_log_det(A_indices, A_indptr, A_x): .2e}\")\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x, dtype = jnp.float64) / n\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  return sparse_log_det(A_indices, A_indptr, Ax_theta)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense(), dtype = jnp.float64) / n \n  Ax_theta = Ax_theta.at[np.arange(n),np.arange(n)].add(theta[1])\n  L = jnp.linalg.cholesky(Ax_theta)\n  return 2.0*jnp.sum(jnp.log(jnp.diag(L)))\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.], dtype = jnp.float64),), (jnp.array([1., 2.], dtype = jnp.float64),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.], dtype = jnp.float64),), (jnp.array([1., 2.], dtype = jnp.float64),))\n\neps = 1e-7\njvp_fd = (f(jnp.array([2.,3.], dtype = jnp.float64) + eps * jnp.array([1., 2.], dtype = jnp.float64) ) - f(jnp.array([2.,3.], dtype = jnp.float64))) / eps\n\ngrad1 = grad(f)(jnp.array([2., 3.], dtype = jnp.float64))\ngrad2 = grad(f_jax)(jnp.array([2., 3.], dtype = jnp.float64))\n\nprint(f\"\"\"\nCheck the Derivatives!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2)}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2)}\n  JVP difference (FD): {np.linalg.norm(jvp1 - jvp_fd)}\n  Gradient difference: {np.linalg.norm(grad1 - grad2)}\n\"\"\")\n\nError in log-determinant =  0.00e+00\n\n\n\nCheck the Derivatives!\nVariable A:\n  Primal difference: 0.0\n  JVP difference: 8.526512829121202e-13\n  JVP difference (FD): 4.171707900013644e-06\n  Gradient difference: 8.881784197001252e-16\n\n\n\nMuch better!"
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#wrapping-up",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#wrapping-up",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Wrapping up",
    "text": "Wrapping up\nAnd that is where we will leave it for today. Next up, I’m probably going to need to do the autodiff for the Cholesky factorisation. It’s not hard, but it is tedious47 and this post is already very long.\nAfter that we need a few more things:\n\nCompilation rules for all of these things. For the most part, we can just wrap the relevant parts of Eigen. The only non-trivial code would be the partial inverse. That will allow us to JIT shit.\nWe need to beef up the sparse matrix class a little. In particular, we are going to need addition and scalar multiplication at the very minimum to make this useful.\nWork out how Aesara works so we can try to prototype a PyMC model.\n\nThat will be a lot more blog posts. But I’m having fun. So why the hell not."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#footnotes",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#footnotes",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am sorry Cholesky factorisation, this blog is already too long and there is simply too much code I need to make nicer to even start on that journey. So it will happen in a later blog.↩︎\nWhich I have spent zero effort making pretty or taking to any level above scratch code↩︎\nLike making it clear how this works for a sparse matrix compared to a general one↩︎\nTo the best of my knowledge, for example, we don’t know how to differentiate with respect to the order parameter \\(\\nu\\) in the modified Bessel function of the second kind \\(K_\\nu(x)\\). This is important in spatial statistics (and general GP stuff).↩︎\nYou may need to convince yourself that this is possible. But it is. The cone of SPD matrices is very nice.↩︎\nDon’t despair if you don’t recognise the third line, it’s the Neumann series, which gives an approximation to \\((I + B)^{-1}\\) whenever \\(\\|B\\| \\ll 1\\).↩︎\nI recognise that I’ve not explained why everything needs to be JAX-traceable. Basically it’s because JAX does clever transformations to the Jacobian-vector product code to produce things like gradients. And the only way that can happen is if the JVP code can take abstract JAX types. So we need to make it traceable because we really want to have gradients!↩︎\nWhy not now, Daniel? Why not now? Well mostly because I might need to do some tweaking down the line, so I am not messing around until I am done.↩︎\nThis is the primary difference between implementing forward mode and reverse mode: there is only one output here. When we move onto reverse mode, we will output a tuple Jacobian-transpose-vector products, one for each input. You can see the structure of that reflected in the transposition rule we are going to write later.↩︎\nSome things: Firstly your function needs to have the correct signature for this to work. Secondly, you could also use ad.defjvp() if you didn’t need to use the primal value to define the tangent (recall one of our tangents is \\(A^{-1}\\Delta c\\), where \\(c = A^{-1}b\\) is the primal value).↩︎\nThis is because it is the efficient way of computing a gradient. Forward-mode autodiff chains together Jacobian-vector products in such a way that a single sweep of the entire function computes a single directional derivative. Reverse-mode autodiff chains together Jacobian-transpose-vector products (aka vector-Jacobian products) in such a way that a single sweep produces an entire gradient. (This happens at the cost of quite a bit of storage.) Depending on what you are trying to do, you usually want one or the other (or sometimes a clever combination of both).↩︎\nor gradients or some sort of thing.↩︎\nto be honest, in Stan we sometimes just don’t dick around with the forward-mode autodiff, because gradients are our bread and butter.↩︎\nI mean, love you programming language people. But fuck me this paper could’ve been written in Babylonic cuneiform for all I understood it.↩︎\nThat is, if you fix a value of \\(y\\), \\(f_y(x) = f(x, y)\\) is not an affine function.↩︎\nDetails bore me.↩︎\nIn general, there might need to be a little bit of reshaping, but it’s equivalent.↩︎\nHave you noticed this is like the third name I’ve used for this equivalent concept. Or the fourth? The code calls it a cotangent because that’s another damn synonym. I’m so very sorry.↩︎\nnot difficult, I’m just lazy and Mike does it better that I can. Read his paper.↩︎\nFor sparse matrices it’s just the non-zero mask of that.↩︎\nYes. I know. Central differences. I am what I am.↩︎\nSome of the stuff I’ve done like normalising all of the inputs would help make these tests more stable. You should also just pick up Nick Higham’s backwards error analysis book to get some ideas of what your guarantees actually are in floating point, but I truly cannot be bothered. This is scratch code.↩︎\nIt should be slightly bigger, it isn’t.↩︎\nThe largest number \\(\\epsilon\\) such that float(1.0) == float(1.0 + machine_eps) in single precision floating point.↩︎\nFun fact: I implemented this and the error never spawned, so I guess JAX is keeping the index arrays concrete, which is very nice of it!↩︎\nactual damn numbers↩︎\nWe want that auld triangle to go jingle bloody jangle↩︎\nWe definitely do not want someone to write an eight hour, two part play that really seems to have the point of view that our Cholesky triangle deserved his downfall. Espoused while periodically reading deadshit tumblr posts. I mean, it would win a Tony. But we still do not want that.↩︎\nThere are more arguments. Read the help. This is what we need↩︎\nWhat if I told you that this would work perfectly well if \\(A\\) was a linear partial differential operator or an integral operator? Probably not much because why would you give a shit?↩︎\nIt can be more general, but it isn’t↩︎\nI think there is a typo in the docs↩︎\nFull disclosure: I screwed this up multiple times today and my tests caught it. What does that look like? The derivatives for \\(A\\) being off, but everything else being good.↩︎\nAnd some optional keyword arguments, but we don’t need to worry about those↩︎\nThis is not quite the same but similar to something that functional programming people call currying, which was named after famous Australian Olympic swimmer Lisa Curry.↩︎\nand a shitload simpler!↩︎\nAnd we have to store a bunch more. This is less of a big deal when \\(L\\) is sparse, but for an ordinary linear solve, we’d be hauling around an extra \\(\\mathcal{O}(n^2)\\) floats containing tangents for no good reason.↩︎\nIf you are worrying about the suppressed constant, remember that \\(A\\) (and therefore \\(n\\) and \\(\\|A\\|\\)) is fixed.↩︎\nI think I’ve made this mistake about four times already while writing this blog. So I am going to write it out.↩︎\nNot to “some of my best friends are physicists”, but I do love them. I just wished a man would talk about me the way they talk about being coordinate free. Rather than with the same ambivalence physicist use when speaking about a specific atlas. I’ve been listening to lesbian folk music all evening. I’m having feelings.↩︎\npronoun on purpose↩︎\nTakahashi, K., Fagan, J., Chen, M.S., 1973. Formation of a sparse bus impedance matrix and its application to short circuit study. In: Eighth PICA Conference Proceedings.IEEE Power Engineering Society, pp. 63–69 (Papers Presented at the 1973 Power Industry Computer Application Conference in Minneapolis, MN).↩︎\nThanks to Jerzy Baranowski for finding a very very bad LaTeX error that made these questions quite wrong!↩︎\nIndeed, in the notation of post two \\(\\mathcal{L}_i \\cap \\{i+1, \\dots, n\\} \\subseteq \\mathcal{L}_j\\) for all \\(i \\leq j\\), where \\(\\mathcal{L}_i\\) is the set of non-zeros in the \\(i\\)th column of \\(L\\).↩︎\nThe sparse matrix is stored as a dictionary {(i,j): value}, which is a very natural way to build a sparse matrix, even if its quite inefficient to do anything with it in that form.↩︎\nYou can’t just use jnp.linalg.det() because there’s a tendency towards nans. (The true value is something like r exp(250.49306761204593)!)↩︎\nWould it be less tedious if my implementation of the Cholesky was less shit? Yes. But hey. It was the first non-trivial piece of python code I’d written in more than a decade (or maybe ever?) so it is what it is. Anyway. I’m gonna run into the same problem I had in Part 3↩︎"
  },
  {
    "objectID": "posts/2024-05-08-laplace/laplace.html",
    "href": "posts/2024-05-08-laplace/laplace.html",
    "title": "An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won’t you buy me a Laplace approximation",
    "section": "",
    "text": "I am, once again, in a bit of a mood. And the only thing that will fix my mood is a good martini and a Laplace approximation. And I’m all out of martinis.\nTo be honest I started writing this post in February 2023, but then got distracted by visas and jobs and all that jazz. But I felt the desire to finish it, so here we are. I wonder how much I will want to re-write1\nThe post started as a pedagogical introduction to Laplace approximations (for reasons I don’t fully remember), but it rapidly went off the rails. So strap yourself in2 for a tour through the basics of sparse autodiff and a tour through manipulating the jaxpr intermediate representation in order to make one very simple logistic regression produce autodiff code that is almost as fast as a manually programmed gradient."
  },
  {
    "objectID": "posts/2024-05-08-laplace/laplace.html#the-laplace-approximation",
    "href": "posts/2024-05-08-laplace/laplace.html#the-laplace-approximation",
    "title": "An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won’t you buy me a Laplace approximation",
    "section": "The Laplace approximation",
    "text": "The Laplace approximation\nOne of the simplest approximations to a distribution is the Laplace approximation. It be defined as the Gaussian distribution that matches the location and the curvature at the mode of the target distribution. It lives its best life when the density is of the form \\[\np(x) \\propto \\exp(-nf_n(x)),\n\\] where \\(f_n\\) is a sequence of functions3. Let’s imagine that we want to approximate the normalized density \\(p(x)\\) near the mode \\(x^*\\). We can do this by taking the second order Taylor expansion of \\(f_n\\) around \\(x=x_0\\), which is \\[\nf_n = f_n(x^*) + (x-x^*)^TH(x^*)(x-x^*)  + \\mathcal{O}((x-x^*)^3),\n\\] where4 \\[\n[H(x^*)]_{ij} = \\frac{\\partial^2 f_n}{\\partial x_i \\partial x_j}\n\\] is the Hessian matrix.\nIf we replace \\(f_n\\) by its quadratic approximation we get \\[\np(x) \\approx  C\\exp(-n(x- x^*)^TH(x^*)(x-x^*)),\n\\] where \\(C\\) is a constant.\nAfter normalizing the approximation to make sure that we get a proper density, we get the Laplace approximation \\[\np(x) \\approx N(x^*, n^{-1}H(x^*)^{-1}).\n\\]\nThe Laplace approximation can be justified rigorously and has a well-studied error and it’s known to work quite well when \\(p(x)\\) is a) unimodal5 and b) isn’t tooooo non-Gaussian.\nIn practice, people have found that Laplace approximations do a reasonable6 job quantifying uncertainty even in complex neural network models and it is at the heart of any number of classical estimators in statistics.\nFrom an implementation perspective, the Laplace approximation is pretty simple. It’s just a two step process:\n\n\nFind the mode \\(x^* = \\arg \\max_x f_n(x)\\) using your favorite optimizer\nCompute the Hessian \\(H(x^*)\\).\n\n\nIn a Bayesian context, we typically take \\[\nf_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\log p(y_i \\mid x) + \\frac{1}{n} \\log p(x),\n\\] which will lead to a Gaussian approximation to the posterior distribution. But this post really isn’t about Bayes. It’s about Laplace approximations.\n\nComputing the Laplace approximation in JAX\nThis is a two step process and, to be honest, all of the steps are pretty standard. So (hopefully) this will not be too tricky to implement. For simplicity, I’m not going to bother with the dividing and multiplying by \\(n\\), although for very large data it could be quite\n\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\nfrom jax.scipy.special import expit\nfrom jax import jacfwd, grad\nfrom jax import Array\nfrom typing import Callable, Tuple, List, Set, Dict\n\ndef laplace(f: Callable, x0: Array) -&gt; Array:\n    nx = x0.shape[0]\n    mode, *details = minimize(lambda x: -f(x), x0, method = \"BFGS\")\n    H =  -1.0 * jacfwd(grad(f))(mode)\n    return mode, H\n\nThere are a few things worth noting here. There’s not really much in this code, except to note that jax.scipy.optimize.minimize finds the minimum of \\(f\\), so I had to pass through the negative of the function. This change also propagates to the computation of the Hessian, which is computed as the Jacobian of the gradient of f. \nDepending on what needs to be done with the Laplace approximation, it might be more appropriate to output the log-density rather than just the mode and the Hessian, but for the moment we will keep this signature.\nLet’s try it out. First of all, I’m going to generate some random data from a logistic regression model. This is going to use Jax’s slightly odd random number system where you need to manually update the state of the pseudo-random number generator. This is beautifully repeatable7 unlike, say, R or standard numpy, where you’ve got to pay a lot of attention to the state of the random number generator to avoid oddities.\n\nfrom jax import random as jrandom\n\ndef make_data(key, n: int, p: int) -&gt; Tuple[Array, Array]:\n  key, sub = jrandom.split(key)\n  X = jrandom.normal(sub, shape = (n,p)) /jnp.sqrt(p)\n\n  key, sub = jrandom.split(key)\n  beta = 0.5 * jrandom.normal(sub, shape = (p,))\n  key, sub = jrandom.split(key)\n  beta0 = jrandom.normal(sub)\n\n\n  key, sub = jrandom.split(key)\n  y = jrandom.bernoulli(sub, expit(beta0 + X @ beta))\n\n  return (y, X)\n\nAn interesting side-note here is that I’ve generated the design matrix \\(X\\) to have standard Gaussian columns. This is not a benign choice as \\(n\\) gets big. With very high probability, the columns of \\(X\\) will be almost8 orthonormal, which means that this is the best possible case for logistic regression. Generally speaking, design matrices from real9 data have a great deal of co-linearity in them and so algorithms that perform well on random design matrices may perform less well on real data.\nOk, so let’s fit the model! I’m just going to use \\(N(0,1)\\) priors on all of the \\(\\beta\\)s.\n\nfrom functools import partial\nn = 100\np = 5\n\nkey = jrandom.PRNGKey(30127)\ny, X = make_data(key, n, p)\n\ndef log_posterior(beta: Array, X: Array, y: Array) -&gt; Array:\n    assert beta.shape[0] == X.shape[1] + 1\n\n    prob = expit(beta[0] + X @ beta[1:])\n    \n    return (\n      jnp.sum(y * jnp.log(prob) + \n      (1-y) * jnp.log1p(-prob)) - \n      0.5 * jnp.dot(beta, beta)\n    )\n\n\npost_mean, H = laplace(\n  partial(log_posterior, X = X, y = y),\n  x0 =jnp.zeros(X.shape[1] + 1)\n)\n\npost_cov = jnp.linalg.inv(H)\n\nLet’s see how this performs relative to MCMC. To do that, I’m going to build and equivalent PyMC model.\n\nimport numpy as np\nimport pymc as pm\nimport pandas as pd\n\nwith pm.Model() as logistic_reg:\n  beta = pm.Normal('beta', 0, 1, shape = (p+1,))\n  linpred = beta[0] + pm.math.dot(np.array(X), beta[1:])\n  \n  pm.Bernoulli(\n    \"y\", \n    p = pm.math.invlogit(linpred),\n    observed = np.array(y)\n  )\n  posterior = pm.sample(\n    tune=1000, \n    draws=1000, \n    chains=4, \n    cores = 1)\n\n# I would like to apologize for the following pandas code.\ntmp = pm.summary(posterior)\ntmp = tmp.assign(\n  laplace_mean = post_mean, \n  laplace_sd = np.sqrt(np.diag(post_cov)), \n  Variable = tmp.index\n)[[\"Variable\", \"mean\", \"laplace_mean\", \"sd\", \"laplace_sd\"]]\n\nwith pd.option_context('display.precision', 3):\n    print(tmp)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (4 chains in 1 job)\nNUTS: [beta]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 5 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 2, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 3, 0 divergences]\n    \n    \n\n\n        Variable   mean  laplace_mean     sd  laplace_sd\nbeta[0]  beta[0]  0.249         0.234  0.235       0.229\nbeta[1]  beta[1] -0.964        -0.914  0.435       0.428\nbeta[2]  beta[2] -1.710        -1.616  0.490       0.470\nbeta[3]  beta[3] -0.975        -0.926  0.423       0.416\nbeta[4]  beta[4] -0.739        -0.716  0.470       0.457\nbeta[5]  beta[5]  0.637         0.609  0.481       0.475\n\n\nWell that’s just dandy! Everything is pretty10 close. With 1000 observations it’s identical to within 3 decimal places.\n\n\nSpeeding up the computation\nSo that is all well and dandy. Let’s see how long it takes. I am interested in big models, so for this demonstration, I’m going to take \\(p = 5000\\). That said, I’m not enormously interested in seeing how this scales in \\(n\\) (linearly), so I’m going to keep that at the fairly unrealistic value of \\(n=1000\\).\n\nimport timeit\ndef hess_test(key, n, p):\n  y, X = make_data(key, n , p)\n  inpu = jnp.ones(p+1)\n  def hess():\n    f = partial(log_posterior, X = X, y = y)\n    return -1.0 * jacfwd(grad(f))(inpu)\n  return hess\n\nn = 1000\np = 5000\nkey, sub = jrandom.split(key)\nhess = hess_test(sub, n , p)\ntimes = timeit.repeat(hess, number = 5, repeat = 5)\nprint(f\"Autodiff: The average time with p = {p} is {np.mean(times): .3f}(+/-{np.std(times): .3f})\")\n\nAutodiff: The average time with p = 5000 is  3.222(+/- 0.379)\n\n\nThat doesn’t seem too bad, but the thing is that I know quite a lot about logistic regression. It is, after all, logistic regression. In particular, I know that the Hessian has the form \\[\nH = X^T D(\\beta) X,\n\\] where \\(D(\\beta)\\) is a diagonal \\(n \\times n\\) matrix that has a known form.\nThis means that the appropriate comparison is between the speed of the autodiff Hessian and how long it takes to compute \\(X^TDX\\) for some diagonal matrix X.\nNow you might be worried here that I didn’t explicitly save \\(X\\) and \\(y\\), so the comparison might not be fair. But my friends, I have good news! All of that awkward key, sub = jrandom.split(key) malarkey has the singular advantage that if I pass the same key into make_data that I used for hess_test, I will get the exact same generated data! So let’s do that. For \\(D\\) I’m just going to pick a random matrix. This will give a minimum achievable time for computing the Hessian (as it doesn’t do the extra derivatives to compute \\(D\\) properly).\nIf you look at that code and say but Daniel you used the wrong multiplication operator, you can convince yourself that X * d[:, None] gives the same result as jnp.diag(d) @ X. But it will be faster. And it uses such beautiful11 broadcasting rules.\n\ny, X = make_data(key, n , p)\nkey, sub = jrandom.split(key)\nd = jrandom.normal(sub, shape = (n,))\nmm = lambda: X.T @ (X * d[:, None])\ntimes = timeit.repeat(mm, number = 5, repeat = 5)\nprint(f\"Symbolic (minimum possible): The average time with p = {p} is {np.mean(times): .3f}(+/-{np.std(times): .3f})\")\n\nSymbolic (minimum possible): The average time with p = 5000 is  0.766(+/- 0.014)\n\n\nOh dear. The symbolic derivative12 is a lot faster.\nSpeeding this up is going to take a little work. The first thing we can try is to explicitly factor out the linear transformation. Instead of passing in the function \\(f\\), we could pass in \\(g\\) such that \\[\nf(x) = g(Ax),\n\\] for some matrix \\(A\\). In our case \\(g\\) would have a diagonal Hessian. Let’s convince ourselves of that with a small example. As well as dropping the intercept, I’ve also dropped the prior term.\n\ng = lambda prob: jnp.sum(y * jnp.log(prob) + (1-y) * jnp.log1p(-prob))\nkey, sub2 = jrandom.split(key)\ny, X = make_data(sub2, 5, 3)\nb = X @ jnp.ones(3)\nD = -1.0 * jacfwd(grad(g))(b)\nprint(np.round(D, 1))\n\n[[0.7 0.  0.  0.  0. ]\n [0.  3.7 0.  0.  0. ]\n [0.  0.  7.8 0.  0. ]\n [0.  0.  0.  4.9 0. ]\n [0.  0.  0.  0.  0.3]]\n\n\nWonderfully diagonal!\n\ndef hess2(g, A, x):\n  # \n  b = A @ x\n  D = -1.0 * jacfwd(grad(g))(b)\n  H = A.T @ (A * jnp.diag(D)[:, None])\n  return H\n\ny, X = make_data(sub, n, p)\ng = lambda prob: jnp.sum(y * jnp.log(prob) + (1-y) * jnp.log1p(-prob))\nx0 = jnp.ones(p)\nh2 = lambda: hess2(g, X, x0)\ntimes = timeit.repeat(h2, number = 5, repeat = 5)\nprint(f\"Separated Hessian: The average time with p = {p} is {np.mean(times): .3f}(+/-{np.std(times): .3f})\")\n\nSeparated Hessian: The average time with p = 5000 is  0.975(+/- 0.163)\n\n\nWell that’s definitely better.\nNow, we might be able to do even better than that if we notice that if we know that \\(D\\) is diagonal, then we don’t need to compute the entire Hessian, we can simply compute the Hessian-vector product \\[\n\\operatorname{diag}(H) = H 1 \\qquad \\text{iff }H\\text{ is diagonal},\n\\] where \\(1\\) is the vector of ones. Just as we computed the Hessian by computing the Jacobian of the gradient, it turns out that we can compute a Hessian-vector product by computing a Jacobian-vector product jvp of the gradient. The syntax in JAX is, honestly, a little bit gross here13, but if you want to read up about how it works the docs are really nice14.\nThis observation is going to be useful because jacfwd computes the Jacobian by computing \\(n\\) Jacobian-vector products. So this observation is saving us a lot of work.\n\nfrom jax import jvp\ndef hess3(g, A, x):\n  # \n  b = A @ x\n  D = -1.0 * jvp(grad(g), (b,), (jnp.ones(n),))[1]\n  H = A.T @ (A * D[:, None])\n  return H\n\nh3 = lambda: hess3(g, X, x0)\ntimes = timeit.repeat(h3, number = 5, repeat = 5)\nprint(f\"Compressed Hessian: The average time with p = {p} is {np.mean(times): .3f}(+/-{np.std(times): .3f})\")\n\nCompressed Hessian: The average time with p = 5000 is  0.879(+/- 0.082)\n\n\nThis is very nearly as fast as the lower bound for the symbolic Hessian. There must be a way to use this."
  },
  {
    "objectID": "posts/2024-05-08-laplace/laplace.html#can-we-automate-this-parsing-jax-expressions",
    "href": "posts/2024-05-08-laplace/laplace.html#can-we-automate-this-parsing-jax-expressions",
    "title": "An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won’t you buy me a Laplace approximation",
    "section": "Can we automate this? Parsing JAX expressions",
    "text": "Can we automate this? Parsing JAX expressions\nSo that was all lovely and shiny. But the problem is that it was very labor intensive. I had to recognize both that you could write \\(f(x) = g(Ax)\\) and that \\(g\\) would have a diagonal Hessian. That is, frankly, hard to do in general.\nIf I was building a system like bambi or brms or INLA15, where the model classes are relatively constrained, it’s possible to automate both of these steps by analyzing the formula. But all I get is a function. So I need to work out how I can automatically parse the code for \\(f\\) to find \\(g\\) and \\(A\\) (if they exist) and to determine if \\(g\\) would have a sparse Hessian.\nWe can’t do this easily with a standard Python program, but we can do it with JAX because it traces through the code and provides an intermediate representation (IR)of the code. This is, incidentally, the first step that any code compiler uses. The beauty of an IR is that it abstracts away all of the specific user choices and provides a clean, logical representation of the program that can then be executed or, in our case, manipulated. These manipulations are, for example, key to how JAX computes gradients, how it JIT-compiles code, and how it does vmap and pmap operations.\nBut we can do more types of manipulations. In particular, we can take the IR and transform it into another IR that produces the same output in a more efficient way. Anyone who’s familiar with compiled programming languages should know that this happens under that hood. They also probably know that compiler writers are small gods and I’m definitely not going to approach anywhere near that level of complexity in a blog post.\nSo what are our tasks. First of all we need to trace our way through the JAX code. We can do this by using the intermediate representation that JAX uses when transforming functions: the jaxprs.\n\nGetting to know jaxprs\nA jaxpr is a transformation of the python code for evaluating a JAX function into a human-readable language that maps types primitives through the code. We can view it using the jax.make_jaxpr function.\nLet’s look at the log-posterior function after partial evaluation to make it a single-input function.\n\nfrom jax import make_jaxpr\n\nlp = partial(log_posterior, X=X, y=y)\nprint(make_jaxpr(lp)(jnp.ones(p+1)))\n\n{ lambda a:f32[1000,5000] b:bool[1000]; c:f32[5001]. let\n    d:f32[1] = dynamic_slice[slice_sizes=(1,)] c 0\n    e:f32[] = squeeze[dimensions=(0,)] d\n    f:f32[5000] = dynamic_slice[slice_sizes=(5000,)] c 1\n    g:f32[1000] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a f\n    h:f32[1000] = add e g\n    i:f32[1000] = logistic h\n    j:f32[1000] = log i\n    k:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b\n    l:f32[1000] = mul k j\n    m:i32[1000] = convert_element_type[new_dtype=int32 weak_type=True] b\n    n:i32[1000] = sub 1 m\n    o:f32[1000] = neg i\n    p:f32[1000] = log1p o\n    q:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] n\n    r:f32[1000] = mul q p\n    s:f32[1000] = add l r\n    t:f32[] = reduce_sum[axes=(0,)] s\n    u:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] c c\n    v:f32[] = mul 0.5 u\n    w:f32[] = sub t v\n  in (w,) }\n\n\nThis can be a bit tricky to read the first time you see it, but it’s waaaay easier that X86-Assembly or the LLVM-IR. Basically it says that to compute lp(jnp.ones(p+1)) you need to run through this program. The first line gives the inputs (with types and shapes). Then after the let statement, there are a the commands that need to be executed in order. A single execution looks like\nd:f32[1] = dynamic_slice[slice_sizes=(1,)] c 0\nThis can be read as take a slice of vector c starting at 0 of shape (1,) and store it in d, which is a 1-dimensional 32bit float array. (The line after turns it into a scalar.)\nAll of the other lines can be read similarly. A good trick, if you don’t recognize the primitive16, is to look it up in the jax.lax sub-module.\nEven a cursory read of this suggests that we could probably save a couple of tedious operations by passing in an integer y, rather than a Boolean y, but hey. That really shouldn’t cost much.\nWhile the jaxpr is lovely, it’s a whole lot easier to reason about if you see it graphically. We can plot the expression graph using17 the haiku18 package from DeepMind.\n\nfrom haiku.experimental import to_dot\nimport graphviz\nimport re\nf = partial(log_posterior, X = X, y = y)\ndot = to_dot(f)(jnp.ones(p+1))\n#Strip out an obnoxious autogen title\ndot = re.sub(\"&lt;&lt;.*&gt;&gt;;\",\"\\\" \\\"\", dot, count = 1, flags=re.DOTALL)\ngraphviz.Source(dot)\n\n\n\n\n\n\n\n\nTo understand this graph, the orange-y boxes represent the input for lp. In this case it’s an array of floating point digits with \\(p+1 = 5001\\). The purple boxes are constants that are used in the function. Some of these are signed integers (s32), there’s a matrix (f32[1000, 5000]), and there is even a literal (0.5). The blue box is the output. That leaves the yellow boxes, which have all of the operations, with inward arrows indicating the inputs and outward arrows indicating the outputs.\n\n\nSplitting the expression graph into linear and non-linear subgraphs\nLooking at the graph, we can split it into three sub-graphs. The first sub-graph can be found by tracing an input value through the graph until it hits either a non-linear operation or the end of the graph. The sub-graph is created by making the penultimate node in that sequence an output node. This sub-graph represents a linear transformations.\n\n\n\n\n\n\n\n\n\nOnce we have reached the end of the linear portion, we can link the output from this operation to the input of the non-linear sub-graph.\n\n\n\n\n\n\n\n\n\nFinally, we have one more trace of \\(\\beta\\) through the graph that is non-linear. We could couple this into the non-linear graph at the cost of having to reason about a bivariate Hessian (which will become complex).\n\n\n\n\n\n\n\n\n\nThe two non-linear portions of the graph are merged through a trivial linear combination.\n\n\n\n\n\n\n\n\n\n\n\nStep right up to play the game of the year: Is it linear?\nSo we need to trace through these jaxprs and keep a record of which of the sub-graphs they are in (and we do not know how many sub-graphs there will be!). We also need to note if an operation is linear or not. This is not something that is automatically provided. We need to store this information ourselves.\nThe only way I can think to do this is to make a set of all of the JAX operations that I know to be linear. Many of them are just index or type stuff. Unfortunately, there is a more complex class of operation, which are only sometimes linear.\nThe first example we see of this is\ng:f32[1000] = dot_general[\n      dimension_numbers=(((1,), (0,)), ((), ()))\n      precision=None\n      preferred_element_type=None\n    ] a f\nThis line represents the general tensor dot product between a and f. In this case, a is constant input (the matrix \\(X\\)) while f is a linear transformation of the input (beta[1:]), so the resulting step is linear. However, there is a second dot_general in the code, which occurs at\nu:f32[] = dot_general[\n      dimension_numbers=(((0,), (0,)), ((), ()))\n      precision=None\n      preferred_element_type=None\n    ] c c\nIn this case, c is a linear transformation of the input (it’s just beta), but dot(c,c) is a quadratic function. Hence in this case, dot_general is not linear.\nWe are going to need to work out how to handle this case. In the folded code is a partial19 list of the jax.lax primitives that are linear or occasionally linear. All in all there are 69 linear or no-op primitives and 7 sometimes linear primitives.\n\n\njax.lax linear and sometimes linear primitives\njax_linear = {\n  'add',\n  'bitcast_convert_type',\n  'broadcast',\n  'broadcast_in_dim',\n  'broadcast_shapes',\n  'broadcast_to_rank',\n  'clz',\n  'collapse',\n  'complex',\n  'concatenate',\n  'conj',\n  'convert_element_type',\n  'dtype',\n  'dtypes',\n  'dynamic_slice',\n  'expand_dims',\n  'full',\n  'full_like',\n  'imag',\n  'neg',\n  'pad',\n  'padtype_to_pads',\n  'real',\n  'reduce',\n  'reshape',\n  'rev',\n  'rng_bit_generator',\n  'rng_uniform',\n  'select',\n  'select_n',\n  'squeeze',\n  'sub',\n  'transpose',\n  'zeros_like_array',\n  'GatherDimensionNumbers',\n  'GatherScatterMode',\n  'ScatterDimensionNumbers',\n  'dynamic_index_in_dim',\n  'dynamic_slice',\n  'dynamic_slice_in_dim',\n  'dynamic_update_index_in_dim',\n  'dynamic_update_slice',\n  'dynamic_update_slice_in_dim',\n  'gather',\n  'index_in_dim',\n  'index_take',\n  'reduce_sum',\n  'scatter',\n  'scatter_add',\n  'slice',\n  'slice_in_dim',\n  'conv',\n  'conv_dimension_numbers',\n  'conv_general_dilated',\n  'conv_general_permutations',\n  'conv_general_shape_tuple',\n  'conv_shape_tuple',\n  'conv_transpose',\n  'conv_transpose_shape_tuple',\n  'conv_with_general_padding',\n  'cumsum',\n  'fft',\n  'all_gather',\n  'all_to_all',\n  'axis_index',\n  'ppermute',\n  'pshuffle',\n  'psum',\n  'psum_scatter',\n  'pswapaxes',\n  'xeinsum'\n}\n\njax_sometimes_linear = { \n  'batch_matmul',\n  'dot',\n  'dot_general',\n  'mul'\n }\njax_first_linear = {\n  'div'\n }\njax_last_linear = {\n  'custom_linear_solve',\n  'triangular_solve',\n  'tridiagonal_solve'\n }\n\n\nAll of the sometimes linear operations are linear as long as only one of their arguments depends on the function inputs. For both div and the various linear solves, the position of the input-dependent argument is restricted to one of the two positions.\n\n\n\n\n\n\nA more JAX-native way to deal with this is to think of how the transpose operation works. Essentially, it has the same dimension as the function argument, but evaluates to None when the operation isn’t linear in that variable. But I had already done all of this before I got there and at some point truly you’ve gotta stop making your blog post more complicated.\n\n\n\n\n\nTracing through the jaxprs\nIn order to split our graph into appropriate sub-graphs we need to trace through the jaxpr and keep track of every variable and if it depends on linear or non-linear parts.\nFor simplicity, consider the following expression graph for computing lambda x, y: 0.5*(x+y).\n\n\n\nAn expression graph for computing lambda x, y: 0.5*(x+y). The blue rectangles are input variables, the rectangle square is a literal constants, and the green oval is the output node. (Yes I know the haiku colours are different. Sue me.)\n\n\nThis figure corresponds roughly to the jaxpr\n\n\n{ lambda ; a:f32[] b:f32[]. let c:f32[] = add a b; d:f32[] = mul 0.5 c in (d,) }\n\n\nFor each node, the graph tells us\n\nits unique identifier (internally20 JAX uses integers)\nwhich equation generated the value\nwhich nodes are its parents in the graph (the input(s) to the equation)\nwhether or not this node depends on the inputs. This is useful for ignoring non-linearities that just apply to the constants bound to the jaxpr.\n\nWe can record this information in a dataclass.\n\nimport dataclasses as dc\n@dc.dataclass\nclass Node:\n  number: int = None\n  eqn: int = None\n  parents: List[int] = dc.field(default_factory=list)\n  depends_on_input: bool = True\n\nNow we can build up our graph with all of the side information we need. The format of a jaxpr places the constant inputs in the first node, followed by the non-constant inputs (which I’m calling the input variables). For simplicity, I am assuming that there is only one input variable.\n\n\n\n\n\n\nYou’re going to look at this code and say girl why are you using a dictionary, this is clearly a list. And you would be correct except for one little thing: I can’t guarantee that the count variables begin at 0. They usually do. But one time they didn’t. What is probably true is that we could subtract off the first count from constvars or invars and we would have an ordinary list with the count variable corresponding to the input. But I’m not spelunking in the source code to ensure that Literal Vars can’t be reused etc. And anyway, this is not a performance-critical data structure.\nI’m also relying heavily on dictionaries remembering key entry order, as the nodes are topographically sorted.\n\n\n\n\nimport jax.core as jcore\nfrom jax import make_jaxpr\n\njpr = make_jaxpr(lp)(jnp.ones(p+1))\n\nnode_list = {\n  const.count: Node(\n    number=const.count, \n    depends_on_input=False\n  ) for const in jpr.jaxpr.constvars\n}\n\nnode_list |= {\n  inval.count: Node(number=inval.count) \n  for inval in jpr.jaxpr.invars\n}\n\n## For later, we need to know the node numbers that correspond\n## to the constants and inputs\n\nconsts_and_inputs = {node.number for node in node_list.values()}\n\nnode_list |= {\n  node.count: Node(\n    number=node.count,\n    eqn=j,\n    parents=[\n      invar.count for invar in eqn.invars if not isinstance(invar, jcore.Literal)\n    ],\n  )\n  for j, eqn in enumerate(jpr.jaxpr.eqns)\n  for node in eqn.outvars\n}\n\nfor node in node_list.values():\n  if len(node.parents) &gt; 0:\n    node.depends_on_input =  any(\n      node_list[i].depends_on_input for i in node.parents\n    )\n\nnode_list\n\n{0: Node(number=0, eqn=None, parents=[], depends_on_input=False),\n 1: Node(number=1, eqn=None, parents=[], depends_on_input=False),\n 2: Node(number=2, eqn=None, parents=[], depends_on_input=True),\n 3: Node(number=3, eqn=0, parents=[2], depends_on_input=True),\n 4: Node(number=4, eqn=1, parents=[3], depends_on_input=True),\n 5: Node(number=5, eqn=2, parents=[2], depends_on_input=True),\n 6: Node(number=6, eqn=3, parents=[0, 5], depends_on_input=True),\n 7: Node(number=7, eqn=4, parents=[4, 6], depends_on_input=True),\n 8: Node(number=8, eqn=5, parents=[7], depends_on_input=True),\n 9: Node(number=9, eqn=6, parents=[8], depends_on_input=True),\n 10: Node(number=10, eqn=7, parents=[1], depends_on_input=False),\n 11: Node(number=11, eqn=8, parents=[10, 9], depends_on_input=True),\n 12: Node(number=12, eqn=9, parents=[1], depends_on_input=False),\n 13: Node(number=13, eqn=10, parents=[12], depends_on_input=False),\n 14: Node(number=14, eqn=11, parents=[8], depends_on_input=True),\n 15: Node(number=15, eqn=12, parents=[14], depends_on_input=True),\n 16: Node(number=16, eqn=13, parents=[13], depends_on_input=False),\n 17: Node(number=17, eqn=14, parents=[16, 15], depends_on_input=True),\n 18: Node(number=18, eqn=15, parents=[11, 17], depends_on_input=True),\n 19: Node(number=19, eqn=16, parents=[18], depends_on_input=True),\n 20: Node(number=20, eqn=17, parents=[2, 2], depends_on_input=True),\n 21: Node(number=21, eqn=18, parents=[20], depends_on_input=True),\n 22: Node(number=22, eqn=19, parents=[19, 21], depends_on_input=True)}\n\n\nNow let’s identify which equations are linear and which aren’t.\n\nlinear_eqn =[False] * len(jpr.jaxpr.eqns)\n\nfor node in node_list.values():\n  if node.eqn is None:\n    continue\n\n  prim = jpr.jaxpr.eqns[node.eqn].primitive.name\n  \n  if prim in jax_linear:\n    linear_eqn[node.eqn] = True\n  elif prim in jax_sometimes_linear:\n    # this is a check for being called once\n    linear_eqn[node.eqn] = (\n      sum(\n        node_list[i].depends_on_input for i in node.parents\n      ) == 1\n    )\n  elif prim in jax_first_linear:\n    linear_eqn[node.eqn] = (\n      node_list[node.parents[0]].depends_on_input \n      and not any(node_list[pa].depends_on_input for pa in node.parents[1:])\n    )\n  elif prim in jax_last_linear:\n    linear_eqn[node.eqn] = (\n      node_list[node.parents[-1]].depends_on_input \n      and not any(node_list[pa].depends_on_input for pa in node.parents[:-1])\n    )\n  elif all(not node_list[i].depends_on_input for i in node.parents):\n    linear_eqn[node.eqn] = True # Constants are linear\n\nThe only messy thing21 in here is dealing with the sometimes linear primitives. If I was sure that every JAX primitive was guaranteed to have only two inputs, this could be simplified, but sadly I don’t know that.\n\n\nPartitioning the graph\nNow it’s time for the fun: partitioning the problem into sub-graphs. To do this, we need to think about what rules we want to encode.\nThe first rule is that every input for an equation or sub-graph needs to be either a constant, the function input, or the output of some other sub-graph that has already been computed. This means that if we find an equation with an input that doesn’t satisfy these conditions, we need to split the sub-graph that it’s in into two sub-graphs.\nThe second rule is the only exception to the first rule. A sub-graph can have inputs from non-linear sub-graphs if an only if it contains a sequence of sum or sub terms and it finishes with the terminal node. This covers the common case where the function we are taking the Hessian of is a linear combination of independent functions. For instance, log_posterior(beta) = log_likelihood(beta) + log_prior(beta). In this case we can compute the Hessians for the non-linear sub-expressions and then combine them.\nThe third rule is that every independent use of the function input is the opportunity to start a new tree. (It may merge with a known tree.)\nAnd that’s it. Should be simple enough to implement.\nI’m feeling like running this bad boy backwards, so let’s do that. One of the assumption we have made is that the function we are tracing has a single output and that is always in the last node and defined in the last equation. So first off, lets get our terminal combination expressions.\n\n## Find the terminal combination expressions\nterminal_expressions = {\"sum\", \"sub\"}\ncomb_eqns = []\nfor eqn in jpr.jaxpr.eqns[::-1]:\n  if any(\n    node_list[a.count].depends_on_input \n    for a in eqn.invars \n    if not isinstance(a, jcore.Literal)\n  )  and (\n    eqn.primitive.name in terminal_expressions\n  ):\n    comb_eqns.append(eqn)\n  else:\n    break\n\nprint(comb_eqns)\n\n[a:f32[] = sub b c]\n\n\nNow for each of the terminal combination expressions, we will trace their parent back until we run out of tree. While we are doing this, we can also keep track of runs of linear operations. We also have to visit each equation once, so we need to keep track of our visited equations. This is, whether we like it or not, a depth-first search. It’s always a bloody depth-first search, isn’t it.\nSo what we are going to do is go through each of the combiner nodes and trace the graph down from it and note the path and it’s parent. If we run into a portion of the graph we have already traced, we will note that for later. These paths will either be merged or, if the ancestral path from that point is all linear, will be used as a linear sub-graph.\n\ndef dfs(visited, graph, subgraph, to_check, node):\n  if node in visited:\n    to_check.add(node)\n  else:\n    visited.add(node)\n    subgraph.add(graph[node].eqn)\n    for neighbour in graph[node].parents:\n      dfs(visited, graph, subgraph, to_check, neighbour)\n  \n\nvisited = consts_and_inputs\nto_check = set()\nsubgraphs = []\nfor ce in comb_eqns:\n  for v in (a for a in ce.invars if not isinstance(a, jcore.Literal)):\n    if v.count not in visited:\n      subgraphs.append(set())\n      dfs(visited, node_list, subgraphs[-1], to_check, v.count)\n\nto_check = to_check.difference(consts_and_inputs)\nprint(f\"Subgraphs: {subgraphs}\")\nprint(f\"Danger nodes: {to_check}\")\n\nSubgraphs: [{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {17, 18}]\nDanger nodes: set()\n\n\nThe to_check nodes are only dangerous insofar as we need to make sure that if they are in one of the linear sub-graphs they are terminal nodes of a sub-graph. To that end, let’s make the linear sub-graphs.\n\nlinear_subgraph = []\nnonlin_subgraph = []\nn_eqns = len(jpr.jaxpr.eqns)\nfor subgraph in subgraphs:\n  print(subgraph)\n  split = next(\n    (\n      i for i, lin in enumerate(linear_eqn) \n      if not lin and i in subgraph\n    )\n  )\n  if any(chk in subgraph for chk in to_check):\n    split = min(\n      split, \n      min(chk for chk in to_check if chk in subgraph)\n    )\n\n  linear_subgraph.append(list(subgraph.intersection(set(range(split)))))\n  nonlin_subgraph.append(list(subgraph.intersection(set(range(split, n_eqns)))))\n\nprint(f\"Linear subgraphs: {linear_subgraph}\")\nprint(f\"Nonlinear subgraphs: {nonlin_subgraph}\")\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n{17, 18}\nLinear subgraphs: [[0, 1, 2, 3, 4], []]\nNonlinear subgraphs: [[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [17, 18]]\n\n\nThe only interesting thing here is making sure that if there is a linear node in the graph that was visited twice, it is the terminal node of the linear graph. The better thing would be to actually split the linear graph, but I’m getting a little bit sick of this post and I don’t really want to deal with multiple linear sub-graphs. So I shan’t. But hopefully it’s relatively clear how you would do that.\nIn this case it’s pretty clear that we are ok.\n\nany(linear_eqn[node_list[j].eqn] for j in to_check)\n\nFalse\n\n\n\n\nPutting it together\nWell that’s a nice script that does what I want. Now let’s put it together in a function. I’m going to give it the very unspecific name transform_jaxpr because sometimes you’ve gotta annoy your future self.\n\n\nShow the code\ndef transform_jaxpr(\n  jaxpr: jcore.ClosedJaxpr\n) -&gt; Tuple[List[Set[int]], List[Set[int]], List[jcore.JaxprEqn]]:\n  assert len(jpr.in_avals) == 1\n  assert len(jpr.out_avals) == 1\n\n  from jax import core as jcore\n\n  ## 1. Extract the tree and its relevant behavior\n  node_list = {\n    const.count: Node(\n      number=const.count, \n      depends_on_input=False\n    ) for const in jpr.jaxpr.constvars\n  }\n\n  node_list |= {\n    inval.count: Node(number=inval.count) \n    for inval in jpr.jaxpr.invars\n  }\n\n  ## For later, we need to know the node numbers that correspond\n  ## to the constants and inputs\n\n  consts_and_inputs = {node.number for node in node_list.values()}\n\n  node_list |= {\n    node.count: Node(\n      number=node.count,\n      eqn=j,\n      parents=[\n        invar.count for invar in eqn.invars if not isinstance(invar, jcore.Literal)\n      ],\n    )\n    for j, eqn in enumerate(jpr.jaxpr.eqns)\n    for node in eqn.outvars\n  }\n\n  for node in node_list.values():\n    if len(node.parents) &gt; 0:\n      node.depends_on_input =  any(\n        node_list[i].depends_on_input for i in node.parents\n      )\n\n  ## 2. Identify which equations are linear_eqn\n\n  linear_eqn =[False] * len(jpr.jaxpr.eqns)\n\n  for node in node_list.values():\n    if node.eqn is None:\n      continue\n\n    prim = jpr.jaxpr.eqns[node.eqn].primitive.name\n    \n    if prim in jax_linear:\n      linear_eqn[node.eqn] = True\n    elif prim in jax_sometimes_linear:\n      # this is a check for being called once\n      linear_eqn[node.eqn] = (\n        sum(\n          node_list[i].depends_on_input for i in node.parents\n        ) == 1\n      )\n    elif prim in jax_first_linear:\n      linear_eqn[node.eqn] = (\n        node_list[node.parents[0]].depends_on_input \n        and not any(node_list[pa].depends_on_input for pa in node.parents[1:])\n      )\n    elif prim in jax_last_linear:\n      linear_eqn[node.eqn] = (\n        node_list[node.parents[-1]].depends_on_input \n        and not any(node_list[pa].depends_on_input for pa in node.parents[:-1])\n      )\n    elif all(not node_list[i].depends_on_input for i in node.parents):\n      linear_eqn[node.eqn] = True # Constants are linear\n\n  ##3. Find all the terminal expressions\n  ## Find the terminal combination expressions\n  terminal_expressions = {\"sum\", \"sub\"}\n  comb_eqns = []\n  for eqn in jpr.jaxpr.eqns[::-1]:\n    if any(\n      node_list[a.count].depends_on_input \n      for a in eqn.invars \n      if not isinstance(a, jcore.Literal)\n    )  and (\n      eqn.primitive.name in terminal_expressions\n    ):\n      comb_eqns.append(eqn)\n    else:\n      break\n  \n  ## 4. Identify the sub-graphs \n  def dfs(visited, graph, subgraph, to_check, node):\n    if node in visited:\n      to_check.add(node)\n    else:\n      visited.add(node)\n      subgraph.add(graph[node].eqn)\n      for neighbour in graph[node].parents:\n        dfs(visited, graph, subgraph, to_check, neighbour)\n    \n\n  visited = consts_and_inputs\n  to_check = set()\n  subgraphs = []\n  for ce in comb_eqns:\n    for v in (a for a in ce.invars if not isinstance(a, jcore.Literal)):\n      if v.count not in visited:\n        subgraphs.append(set())\n        dfs(visited, node_list, subgraphs[-1], to_check, v.count)\n\n  to_check = to_check.difference(consts_and_inputs)\n\n  ## 5. Find the linear sub-graphs\n  linear_subgraph = []\n  nonlin_subgraph = []\n  n_eqns = len(jaxpr.eqns)\n  for subgraph in subgraphs:\n    split = next(\n      (\n        i for i, lin in enumerate(linear_eqn) \n        if not lin and i in subgraph\n      )\n    )\n    if any(chk in subgraph for chk in to_check):\n      split = min(\n        split, \n        min(chk for chk in to_check if chk in subgraph)\n      )\n\n    linear_subgraph.append(list(subgraph.intersection(set(range(split)))))\n    nonlin_subgraph.append(list(subgraph.intersection(set(range(split, n_eqns)))))\n  \n  return (linear_subgraph, nonlin_subgraph, comb_eqns)\n\n\nFor one final sense check, let’s compare these outputs to the original jaxpr.\n\nfor j, lin in enumerate(linear_subgraph):\n  print(f\"Linear: {j}\")\n  for i in lin:\n    print(jpr.eqns[i])\n\nfor j, nlin in enumerate(nonlin_subgraph):\n  print(f\"Nonlinear: {j}\")\n  for i in nlin:\n    print(jpr.eqns[i])\n\nprint(\"Combination equations\")\nfor eqn in comb_eqns:\n  print(eqn)\n\nLinear: 0\na:f32[1] = dynamic_slice[slice_sizes=(1,)] b 0\na:f32[] = squeeze[dimensions=(0,)] b\na:f32[5000] = dynamic_slice[slice_sizes=(5000,)] b 1\na:f32[1000] = dot_general[dimension_numbers=(([1], [0]), ([], []))] b c\na:f32[1000] = add b c\nLinear: 1\nNonlinear: 0\na:f32[1000] = logistic b\na:f32[1000] = log b\na:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b\na:f32[1000] = mul b c\na:i32[1000] = convert_element_type[new_dtype=int32 weak_type=True] b\na:i32[1000] = sub 1 b\na:f32[1000] = neg b\na:f32[1000] = log1p b\na:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b\na:f32[1000] = mul b c\na:f32[1000] = add b c\na:f32[] = reduce_sum[axes=(0,)] b\nNonlinear: 1\na:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] b b\na:f32[] = mul 0.5 b\nCombination equations\na:f32[] = sub b c\n\n\nComparing to the original jaxpr, we see it has the same information (the formatting is a bit unfortunate, as the original __repr__ keeps track of the links between things, but what can you do?).\n\nprint(jpr)\n\n{ lambda a:f32[1000,5000] b:bool[1000]; c:f32[5001]. let\n    d:f32[1] = dynamic_slice[slice_sizes=(1,)] c 0\n    e:f32[] = squeeze[dimensions=(0,)] d\n    f:f32[5000] = dynamic_slice[slice_sizes=(5000,)] c 1\n    g:f32[1000] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a f\n    h:f32[1000] = add e g\n    i:f32[1000] = logistic h\n    j:f32[1000] = log i\n    k:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b\n    l:f32[1000] = mul k j\n    m:i32[1000] = convert_element_type[new_dtype=int32 weak_type=True] b\n    n:i32[1000] = sub 1 m\n    o:f32[1000] = neg i\n    p:f32[1000] = log1p o\n    q:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] n\n    r:f32[1000] = mul q p\n    s:f32[1000] = add l r\n    t:f32[] = reduce_sum[axes=(0,)] s\n    u:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] c c\n    v:f32[] = mul 0.5 u\n    w:f32[] = sub t v\n  in (w,) }\n\n\n\n\nMaking sub-functions\nNow that we have the graph partitioned, let’s make our sub-functions. We do this by manipulating the jaxpr and then closing over the literals.\nThere are a few ways we can do this. We could build completely new JaxprEqn objects from the existing Jaxpr. But honestly, that is just annoying, so instead I’m just going to modify the basic, but incomplete, parser22.\nThe only modification from the standard eval_jaxpr is that we are explicitly specifying the invars in order to overwrite the standard ones. This relies on the lexicographic ordering of the jaxpr expression graph.\n\nfrom typing import Callable\n\nfrom jax import core as jcore\nfrom jax import lax\nfrom jax._src.util import safe_map\n\ndef eval_subjaxpr(\n  *args,\n  jaxpr: jcore.Jaxpr, \n  consts: List[jcore.Literal], \n  subgraph: List[int], \n  invars: List[jcore.Var]\n):\n\n\n  assert len(invars) == len(args)\n  \n  # Mapping from variable -&gt; value\n  env = {}\n  \n  def read(var):\n    # Literals are values baked into the Jaxpr\n    if type(var) is jcore.Literal:\n      return var.val\n\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # We need to bind the input to the sub-function\n  # to the environment.\n  # We only need to write the consts that appear\n  # in our sub-graph, but that's more bookkeeping\n  safe_map(write, invars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Loop through equations and evaluate primitives using `bind`\n  outvars = []\n  for j in subgraph:\n    eqn = jaxpr.eqns[j]\n    # Read inputs to equation from environment\n    invals = safe_map(read, eqn.invars)  \n    # `bind` is how a primitive is called\n    outvals = eqn.primitive.bind(*invals, **eqn.params)\n    # Primitives may return multiple outputs or not\n    if not eqn.primitive.multiple_results: \n      outvals = [outvals]\n    outvars += [eqn.outvars]\n    safe_map(write, eqn.outvars, outvals) \n  \n  return safe_map(read, outvars[-1])[0]\n\nThe final thing we should do is combine our transformation with this evaluation module to convert a function into a sequence of callable sub-functions. I am making liberal uses of lambdas to close over variables that the user should never see (like the sub-graph!). Jesus loves closures and so do I.\n\n\ndef decompose(fun: Callable, *args) -&gt; Tuple[List[Callable], List[Callable], List[jcore.Var]]:\n  from functools import partial\n  from jax import make_jaxpr\n\n  jpr = make_jaxpr(fun)(*args)\n  linear_subgraph, nonlin_subgraph, comb_eqns = transform_jaxpr(jpr)\n\n  assert len(linear_subgraph) == len(nonlin_subgraph)\n  assert len(jpr.jaxpr.invars) == 1, \"Functions must only have one input\"\n\n  def get_invars(sub: List[int]) -&gt; List[jcore.Var]:\n    # There is an implicit assumption everywhere in this post \n    # that each sub-function only has one non-constant input\n    \n    min_count = jpr.jaxpr.eqns[sub[0]].outvars[0].count\n    literal_ceil = jpr.jaxpr.invars[0].count\n    for j in sub:\n      for v in jpr.jaxpr.eqns[j].invars:\n        if (\n          not isinstance(v, jcore.Literal) and\n          v.count &gt;= literal_ceil and \n          v.count &lt; min_count\n        ):\n          return [v]\n    raise Exception(\"Somehow you can't find any invars\")\n    \n\n  lin_funs = []\n  nlin_funs = []\n  nlin_inputs = []\n  lin_outputs = []\n  nlin_outputs = []\n\n  for lin in linear_subgraph:\n    if len(lin) == 0:\n      lin_funs += [None]\n      lin_outputs += [jpr.jaxpr.invars[0].count]\n    elif jpr.jaxpr.eqns[lin[-1]].primitive.multiple_results:\n      raise Exception(f\"This code doesn't deal with multiple outputs from subgraph {lin}\")\n    else:\n      # find \n      lin_outputs += [jpr.jaxpr.eqns[lin[-1]].outvars[0].count]\n      lin_funs += [\n        partial(eval_subjaxpr,\n          jaxpr = jpr.jaxpr, \n          consts = jpr.literals, \n          subgraph = lin, \n          invars = get_invars(lin)\n        )\n      ]\n      \n  for nlin in nonlin_subgraph:\n    if len(nlin) == 0:\n      nlin_funs += [None]\n      nlin_inputs += [-1]\n      nlin_outputs += [None]\n    elif jpr.jaxpr.eqns[nlin[-1]].primitive.multiple_results:\n      raise Exception(f\"This code doesn't deal with multiple outputs from subgraph {nlin}\")\n    else:\n      invar = get_invars(nlin)\n      nlin_inputs += [lin_outputs.index(invar.count) if invar.count in lin_outputs else -1]\n      nlin_outputs += [jpr.jaxpr.eqns[nlin[-1]].outvars[0].count]\n      nlin_funs += [\n        partial(eval_subjaxpr,\n          jaxpr = jpr.jaxpr, \n          consts = jpr.literals, \n          subgraph = nlin, \n          invars = get_invars(nlin)\n        )\n      ]\n\n  combine = [0.0] * len(linear_subgraph)\n  # print(combine)\n  for eqn in comb_eqns:\n    combine[nlin_outputs.index(eqn.invars[0].count)] += 1.0\n    if eqn.primitive.name == \"sub\":\n      combine[nlin_outputs.index(eqn.invars[1].count)] += -1.0\n    else:\n      combine[nlin_outputs.index(eqn.invars[1].count)] += 1.0\n\n\n  return lin_funs, nlin_funs, nlin_inputs, combine"
  },
  {
    "objectID": "posts/2024-05-08-laplace/laplace.html#making-the-hessian",
    "href": "posts/2024-05-08-laplace/laplace.html#making-the-hessian",
    "title": "An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won’t you buy me a Laplace approximation",
    "section": "Making the Hessian",
    "text": "Making the Hessian\nAfter all of this work, we can finally make a function that builds a Hessian!\nWe remember that if \\(f(x) = g(h(x))\\), where \\(h(x)\\) is linear and \\(g(x)\\) is nonlinear, then the hessian of \\(f\\) is\n\\[\nH_f(x) = J_h^T H_g J_h,\n\\] where \\(J_h\\) is the Jacobian of \\(h\\).\n\ndef smarter_hessian(fun: Callable) -&gt; Callable:\n  from jax import jacfwd\n  from jax import hessian\n  from jax import numpy as jnp\n  def hess(*args):\n    assert len(args) == 1, \"This only works for functions with one input\"\n    \n    lin_funs, nlin_funs, nlin_inputs, combine = decompose(fun, *args)\n    n_in = args[0].shape[0]\n    part = jnp.zeros((n_in, n_in))\n\n    for lin, nlin, nimp, comb in zip(lin_funs, nlin_funs, nlin_inputs, combine):\n      \n      if lin is not None:\n        lin_val = lin(*args)\n        jac = jacfwd(lin)(*args)\n      \n\n      h_args = (lin_val,) if lin is not None else args\n      hess = hessian(nlin)(*h_args) if nlin is not None else None\n\n      if lin is not None and nlin is not None:\n        part += comb * (jac.T @ (hess @ jac))\n      elif lin is not None:\n        part += comb * jac.T @ jac\n      elif nlin is not None:\n        part += comb * hess\n      \n    return part\n  return hess\n\nAfter all of that, let’s see if this works!\n\nmode_jax, H_jax = laplace(\n  partial(log_posterior, X = X, y = y),\n  x0 =jnp.zeros(X.shape[1] + 1)\n)\n\nH_smarter = -1.0 * smarter_hessian(partial(log_posterior, X = X, y = y))(mode_jax)\n\nprint(f\"The error is {jnp.linalg.norm(H_jax - H_smarter).tolist()}!\")\n\nThe error is 2.3684690404479625e-06!\n\n\nIn single precision, that is good enough for government work.\n\nBut is it faster?\nNow let’s take a look at whether we have actually saved any time.\n\nimport jax\ntimes_hess = timeit.repeat(lambda: jax.hessian(partial(log_posterior, X = X, y = y))(mode_jax), number = 5, repeat = 5)\nprint(f\"Full Hessian: The average time with p = {p} is {np.mean(times_hess): .3f}(+/-{np.std(times_hess): .3f})\")\n\ntimes_smarter = timeit.repeat(lambda: smarter_hessian(partial(log_posterior, X = X, y = y))(mode_jax), number = 5, repeat = 5)\nprint(f\"Smarter Hessian: The average time with p = {p} is {np.mean(times_smarter): .3f}(+/-{np.std(times_smarter): .3f})\")\n\nFull Hessian: The average time with p = 5000 is  3.444(+/- 0.201)\nSmarter Hessian: The average time with p = 5000 is  3.569(+/- 0.024)\n\n\nWell that didn’t make much of a difference. If anything, it’s a little bit slower. This is likely due to compiler operations that can be improved if you just lower the whole thing.\n\n\nBut you forgot the diagonal trick\nThat said, the decomposition into linear and non-linear parts was not the real source of the savings. If we assume the Hessian of the likelihood is diagonal, then we can indeed do a lot better!\nThe problem here is that while smarter_hessian worked for any23 JAX-traceable function, we are now making a structural assumption. In theory, we could go through the JAX primitives and mark all of the ones that would (conditionally) lead to diagonal Hessians, but honestly I kinda want this bit of the post to be done. So I will leave that as an exercise to the interested reader.\n\ndef smart_hessian(fun: Callable) -&gt; Callable:\n  from jax import jacfwd\n  from jax import hessian\n  from jax import numpy as jnp\n  def hess(*args):\n    assert len(args) == 1, \"This only works for functions with one input\"\n    \n    lin_funs, nlin_funs, nlin_inputs, combine = decompose(fun, *args)\n    n_in = args[0].shape[0]\n    part = jnp.zeros((n_in, n_in))\n\n    for lin, nlin, nimp, comb in zip(lin_funs, nlin_funs, nlin_inputs, combine):\n      \n      if lin is not None:\n        lin_val = lin(*args)\n        jac = jacfwd(lin)(*args)\n      \n\n      h_args = (lin_val,) if lin is not None else args\n      D = jvp(grad(nlin), h_args, (jnp.ones_like(h_args[0]),))[1] if nlin is not None else None\n\n      if lin is not None and nlin is not None:\n\n        part += comb * (jac.T @ (jac * D[:,None]))\n      elif lin is not None:\n        part += comb * jac.T @ jac\n      elif nlin is not None:\n        part += comb * jnp.diag(D)\n      \n    return part\n  return hess\n\n\n\nH_smart = -1.0 * smart_hessian(partial(log_posterior, X = X, y = y))(mode_jax)\n\nprint(f\"The error is {jnp.linalg.norm(H_jax - H_smart).tolist()}!\")\n\ntimes_smart = timeit.repeat(lambda: smart_hessian(partial(log_posterior, X = X, y = y))(mode_jax), number = 5, repeat = 5)\nprint(f\"Smart (diagonal-aware) Hessian: The average time with p = {p} is {np.mean(times_smart): .3f}(+/-{np.std(times_smart): .3f})\")\n\nThe error is 2.3684690404479625e-06!\nSmart (diagonal-aware) Hessian: The average time with p = 5000 is  2.269(+/- 0.031)\n\n\nThat is a proper saving!"
  },
  {
    "objectID": "posts/2024-05-08-laplace/laplace.html#some-concluding-thoughts",
    "href": "posts/2024-05-08-laplace/laplace.html#some-concluding-thoughts",
    "title": "An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won’t you buy me a Laplace approximation",
    "section": "Some concluding thoughts",
    "text": "Some concluding thoughts\nWell, this post got out of control. I swear when I sat down I was just going to write a quick post about Laplace approximations. Ooops.\n\nThe power of compiler optimizations\nI think what I’ve shown here is that one of the really powerful things about compiled languages like JAX is that you can perform a pile of code optimizations that can greatly improve their performance.\nIn the ideal world, this type of optimization should be invisible to the end user. Were I to do this seriously24, I would make sure that if the assumptions of the optimized code weren’t met, the behaviour would revert back to the standard jax.hessian.\nRecognizing when to perform an optimization is, in reality, the whole art of this type of process. And it’s very hard. For this post, I was able to do that to automatically recognize the linear operation, but I didn’t try to find conditions that ensured the Hessian would be diagonal.\n\n\nSparsity detection and sparse autodiff\nWould you believe that people have spent a lot of time studying the efficiency gains when you have things like sparse Hessians? There is, in fact, a massive literature on sparse autodiff and it is implemented in several autodiff libraries, including in Julia.\nSparsity exploiting autodiff uses symbolic analysis of the expression tree for a function to identify when certain derivatives are going to be zero. For Hessians, it needs to identify when two variables have at most linear dependencies.\nOnce you have worked out the sparsity pattern, you need to do something with it. In the logistic case, it is diagonal, but in a lot of cases it will depend on more than one element of the latent representation. That is the Hessian will be sparse25, but it won’t be diagonal.\nI guess the question is can we generalist the observation if the Hessian is diagonal we only need to compute a single Hessian-vector product to general sparsity structures.\nIn general, we won’t be able to get away with a single product and will instead need a specially constructed set of \\(k\\) probing vectors, where \\(k\\) is a number to be determined (that is hopefully much smaller than \\(n\\)). This set of vectors \\(s_k\\) will have the special property that \\[\n\\sum_{j=1}^k s_j = 1.\n\\] This means that the non-zero elements of each probing vector corresponds to a disjoint grouping of the variables.\nTo do this, we need to construct our set of probing vectors in a very special way. Each \\(s_k\\) will be a vector containing zeros and ones. The set of indices with \\([s_k]_j = 1\\) have color \\(k\\). The aim is to associate each index with a unique color in such a way that we can recover the algorithm. We can do this with a structurally symmetric orthogonal partition, which is detailed in Section 4 of this great review article.\nImplementing26 sparsity-aware autodiff Hessians does require some graph algorithms, and is frankly beyond the scope of my patience here. But it certainly is possible and you would get quite general performance from it.\nCritically, because it reduces the computation of a \\(p \\times p\\) dense Hessian matrix with \\(k\\) Hessian-vector products, it is extremely well suited to modern GPU acceleration techniques!\n\n\nCould we do more?\nThere are so many many many ways to improve the very simple symbolic reduction of the autodiff beyond the simple “identify $f(Ax)” strategy. For more complex cases, it might be necessary to relax the only one input and only one output assumption.\nIt also might be possible to chain multiple instances of this, although this would require a more complex Hessian chain rule. Nevertheless, the extra complexity might be balanced by savings form the applicable instances of sparse autodiff.\nBut probably the thing that actually annoys me in all of this is that we are constantly recomputing the Jacobian for the linear equation, which is fixed. A better implementation would consider implementing symbolic differentiation for linear sub-graphs, which should lead to even more savings.\n\n\nBut is JAX the right framework for this?\nAll of this was a fair bit of work so I’m tempted to throw myself at the sunk-cost fallacy and just declare it to be good. But there is a problem. Because JAX doesn’t do a symbolic transformation of the program (only a trace through paths associated with specific values), there is no guarantee that the sparsity pattern for \\(H\\) remains the same at each step. And there is nothing wrong with that. It’s an expressive, exciting language.\nBut all of the code transformation to make a sparsity-exploiting Hessian doesn’t come for free. And the idea of having to do it again every time a Hessian is needed is … troubling. If we could guarantee that the sparsity pattern was static, then we could factor all of this complex parsing and coloring code away and just run it once for each problem.\nTheoretically, we could do something like hashing on the jaxpr, but I’m not sure how much that would help.\nIdeally, we could do this in a library that performs symbolic manipulations and can compile them into an expression graph. JAX is not quite27 that language. An option for this type of symbolic manipulation would be Aesara. It may even be possible to do it in Stan, but even my wandering mind doesn’t want to work out how to do this in OCaml."
  },
  {
    "objectID": "posts/2024-05-08-laplace/laplace.html#footnotes",
    "href": "posts/2024-05-08-laplace/laplace.html#footnotes",
    "title": "An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won’t you buy me a Laplace approximation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI will never reveal how much. But it was most of it.↩︎\nor on↩︎\nthat probably converges. Think of it like \\(f_n(\\theta) = n^{-1} \\left(\\sum_{i=1}^n p(y_i \\mid \\theta) + p(\\theta)\\right)\\), where \\(p(y_i \\mid \\theta)\\) is the likelihood and \\(p(\\theta)\\) is the prior.↩︎\nThe first-order term disappears because at the mode \\(x^*\\) \\(\\nabla f(x^*)=0\\)↩︎\nor has one dominant mode↩︎\nSomething isn’t always better than nothing but sometimes it is↩︎\nYou could say reproducible code but I won’t because that word means something pretty specific. I mean, this is not the place for a rant, but it is very difficult to write strictly reproducible code and I am frankly not even going to try to take a bite out of that particular onion.↩︎\nThe maths under this is very interesting and surprisingly accessible (in a very advanced sort of way). I guess it depends on what you think of as accessible, but it’s certainly much nicer than entropy and VC-classes. A lovely set of notes that cover everything you’ve ever wanted to know is here↩︎\nUnless someone’s been doing their design of experiments↩︎\nWith 100 observations, we expect our data-driven variation (aka the frequentist version) to be about one decimal place, so the Laplace approximation is accurate within that tolerance. In fact, clever maths types can analyse the error in the Laplace approximation and show that the error is about \\(\\mathcal{O}(n^{-1})\\), which is asymptotically much smaller than the sampling variability of \\(\\mathcal{O}(n^{-1/2})\\), which suggests that the error introduced by the Laplace approximation isn’t catastrophic. At least with enough data.↩︎\nBe still my beating heart.↩︎\nOk. You caught me. They’re not technically the same model. The symbolic code doesn’t include an intercept. I just honestly cannot be arsed to do the very minor matrix algebra to add it in. Nor can I be arsed to add a column of ones to X.↩︎\nSo many tuples↩︎\nThis is in pretty stark contrast to the pytorch docs, which are shit. Be more like JAX.↩︎\nINLA does this. Very explicitly. And a lot of other cool stuff. It doesn’t use autodiff though.↩︎\nFor example, there’s no call to logistic in the code, but a quick look at jax.lax.logistic shows that it’s the same thing as expit.↩︎\nThis basically just works as long as you’ve got graphviz installed on your system. And once you find the right regex to strip out the terrible auto-generated title.↩︎\nYou need to install the dev version, or else it renders a lot of pjits where the sum and subs are supposed to be.↩︎\nIf I wasn’t sure, I deleted them from the linear list. There were also scatter_mul, reduce_window, and reduce_window_shape_tuple, which are all sometimes linear but frankly I didn’t want to work out the logic.↩︎\nThe letters are __repl__ magic↩︎\nLord I hate a big ‘if’/‘elif’ block. Just terrible. I should refactor but this is a weekend blog post not a work thing↩︎\nIt is very little extra work to deal with eg JIT’d primitives and that sort of stuff, but for the purpose of this post, let’s keep things as simple as possible.↩︎\nWith input/output restrictions↩︎\nI am currently dressed like a sexy clown.↩︎\nMost of the entries will be zero↩︎\nThe previous article goes for ease of implementation over speed. A faster and better algorithm, and a very detailed comparison of all of the available options can be found here. And I am not implementing that for a fucking blog.↩︎\nAnd it’s not trying to. Their bread and butter is autodiff and what they’re doing is absolutely natural for that.↩︎"
  },
  {
    "objectID": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html",
    "href": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html",
    "title": "Sparse Matrices 2: An invitation to a sparse Cholesky factorisation",
    "section": "",
    "text": "This is part two of an ongoing exercise in hubris. Part one is here."
  },
  {
    "objectID": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#so-how-do-we-store-a-sparse-matrix",
    "href": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#so-how-do-we-store-a-sparse-matrix",
    "title": "Sparse Matrices 2: An invitation to a sparse Cholesky factorisation",
    "section": "So how do we store a sparse matrix?",
    "text": "So how do we store a sparse matrix?\nIf we look at the Cholesky algorithm, we notice that we are scanning through the matrix column-by-column. When a computer stores a matrix, it stores it as a long 1D array with some side information. How this array is constructed from the matrix depends on the language.\nThere are (roughly) two options: column-major or row-major storage. Column major storage (used by Fortran8, R, Matlab, Julia, Eigen, etc) stacks a matrix column by column. A small example: \\[\n\\begin{pmatrix}1&3&5\\\\2&4&6 \\end{pmatrix} \\Rightarrow [1,2,3,4,5,6].\n\\] Row-mjor ordering (C/C++ arrays, SAS, Pascal, numpy9) stores things row-by-row.\nWhich one do we use? Well. If you look at the Cholesky algorithm, it scans through the matrix column-by-column. It is much much much more memory efficient in this case to have the whole column available in one contiguous chunk of memory. So we are going to use column-major storage.\nBut there’s an extra wrinkle: Most of the entries in our matrix are zero. It would be very inefficient to store all of those zeros. You may be sceptical about this, but it’s true. It helps to realize that even in the examples at the bottom of this post that are not trying very hard to minimise the fill in, only 3-4% of the potential elements in \\(L\\) are non-zero.\nIt is far more efficient to just store the locations10 of the non-zeros and their values. If only 4% of your matrix is non-zero, you are saving11 a lot of memory!\nThe storage scheme we are inching towards is called compressed sparse column (CSC) storage. This stores the matrix in three arrays. The first array indices (which has as many entries as there are non-zeros) stores the row numbers for each non-zero element. So if \\[\nB = \\begin{pmatrix}\n1 &&5 \\\\\n2&3& \\\\\n&4&6\n\\end{pmatrix}\n\\] then (using zero-based indices because I’ve to to make this work in Python)\n\nB_indices = [0,1,1,2,0,3]\n\nThe second array indptr is an \\(n+1\\)-dimensional array that indexes the first element of each row. The final element of indptr is nnz(B)12. This leads to\n\nB_indptr = [0,2,4,6]\n\nThis means that the entries in column13 j are have row numbers\n\nB_indices[B_indptr[j]:B_indptr[j+1]]\n\nThe third and final array is x, which stores the values of the non-negative entries of \\(A\\) column-by-column. This gives\n\nB_x = [1,2,3,4,5,6]\n\nUsing these three arrays we can get access to the jth row of \\(B\\) by accessing\n\nB_x[B_indptr[j]:B_indptr[j+1]]\n\nThis storage scheme is very efficient for what we are about to do. But it is fundamentally a static scheme: it is extremely expensive to add a new non-zero element. There are other sparse matrix storage schemes that make this work better."
  },
  {
    "objectID": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#how-sparse-is-a-cholesky-factor-of-a-sparse-matrix",
    "href": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#how-sparse-is-a-cholesky-factor-of-a-sparse-matrix",
    "title": "Sparse Matrices 2: An invitation to a sparse Cholesky factorisation",
    "section": "How sparse is a Cholesky factor of a sparse matrix?",
    "text": "How sparse is a Cholesky factor of a sparse matrix?\nOk. So now we’ve got that out of the way, we need to work out the sparsity structure of a Choleksy factorisation. At this point we need to close our eyes, pray, and start thinking about graphs.\nWhy graphs? I promise, it is not because I love discrete14 maths. It is because symmetric sparse matrices are strongly related to graphs.\nTo remind people, a graph15 (in a mathematical sense) \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) consists of two lists:\n\nA list of vertices \\(\\mathcal{V}\\) numbered from \\(1\\) to \\(n\\)16.\nA list of edges \\(\\mathcal{E}\\) in the graph (aka all the pairs \\((i,j)\\) such that \\(i&lt;j\\) and there is an edge between \\(i\\) and \\(j\\)).\n\nEvery symmetric sparse matrix \\(A\\) has a graph naturally associated with it. The relationship is that \\((i,j)\\) (for \\(i\\neq j\\)) is an edge in \\(\\mathcal{G}\\) if and only if \\(A_{ij} \\neq 0\\).\nSo, for instance, if \\[\nA = \\begin{pmatrix}\n1&2&&8 \\\\\n2&3&& 5\\\\\n&&4&6 \\\\\n8&5&6&7\n\\end{pmatrix},\n\\]\nthen we can plot the associated graph, \\(\\mathcal{G}\\).\n\n\n\n\n\n\n\n\n\nBut why do we care about graphs?\nWe care because they let us answer our question for this section: which elements of the Cholesky factor \\(L\\) are non-zero?\nIt is useful to write the algorithm out for a second time17, but this time closer to how we will implement it.\n\nL = np.tril(A)\nfor j in range(n):\n  for k in range(j-1):\n    L[j:n, j] -= L[j, k] * L[j:n, k]\n  L[j,j]= np.sqrt(L[j,j])\n  L[j+1:n, j] = L[j+1:n] / L[j, j]\n\nIf we stare at this long enough we can work out when \\(L_{ij}\\) is going to be potentially non-zero.\nAnd here is where we have to take a quick zoom out. We are not interested if the numerical entry \\(L_{ij}\\) is actually non-zero. We are interested if it could be non-zero. Why? Because this will allow us to set up our storage scheme for the sparse Cholesky factor. And it will tell us exactly which bits of the above loops we actually need to do!\nSo with that motivation in mind, can we spot the non-zeros? Well. I’ll be honest with you. I struggle at this game. This is part of why I do not like thinking about graphs18. But with a piece of paper and a bit of time, I can convince myslef that \\(L_ij\\) is potentially non-zero (or a structural non-zero) if:\n\n\\(A_{ij}\\) is non-zero (because tmp[i-j] is non-zero!), or\n\\(L_{ik} \\neq 0\\) and \\(L_{jk} \\neq 0\\) for some \\(k &lt; \\min\\{i, j\\}\\) (because that is the only time an element of tmp is updated through tmp[i] = tmp[i] - L[i, k] * L[j, k])\n\nIf we dig into the second condition a bit more,19 we notice that the second case can happen if and only if there is a path in \\(\\mathcal{G}\\)20 from node \\(i\\) to node \\(j\\) \\[\ni \\rightarrow v_1 \\rightarrow v_2 \\rightarrow \\ldots \\rightarrow v_{\\ell-1} \\rightarrow j\n\\] with \\(v_1, \\ldots v_{\\ell-1} &lt; \\min\\{i,j\\}\\). The proof is an induction on \\(\\min\\{i,j\\}\\) that I can’t be arsed typing out.\n(As an aside, Theorem 2.8 in Rue and Held’s book gives a very clearn nice statistical proof of this result.)\nThis is enough to see that fill in patterns are going to be a complex thing.\n\nA toy example\nConsider the following graph\n\n\n\n\n\n\n\n\n\nIt’s pretty clear that there is a path between \\((i,j)\\) for every pair \\((i,j)\\) (the path goes through the fully connected vertex, which is labelled 1).\nAnd indeed, we can check this numerically21\n\nlibrary(Matrix)\nn &lt;- 6\nA &lt;- sparseMatrix(i = c(1:n, rep(1,n)), \n                  j = c(rep(1,n),1:n), \n                  x = -0.2, \n                  dims = c(n,n)) + \n      Diagonal(n)\nA != 0 #print the non-zero structrure\n\n6 x 6 sparse Matrix of class \"lgCMatrix\"\n                \n[1,] | | | | | |\n[2,] | | . . . .\n[3,] | . | . . .\n[4,] | . . | . .\n[5,] | . . . | .\n[6,] | . . . . |\n\nL = t(chol(as.matrix(A))) # transpose is for R reasons\nround(L, digits = 1) # Fully dense!\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  0.8  0.0  0.0  0.0  0.0    0\n[2,] -0.3  1.0  0.0  0.0  0.0    0\n[3,] -0.3 -0.1  1.0  0.0  0.0    0\n[4,] -0.3 -0.1 -0.1  1.0  0.0    0\n[5,] -0.3 -0.1 -0.1 -0.1  1.0    0\n[6,] -0.3 -0.1 -0.1 -0.1 -0.1    1\n\n\nBut what if we changed the labels of our vertices? What is the fill in pattern implied by a labelling where the fully collected vertex is labelled last instead of first?\n\n\n\n\n\n\n\n\n\nThere are now no paths from \\(i\\) to \\(j\\) that only go through lower-numbered vertices. So there is no fill in! We can check this numerically!22\n\nA2 &lt;- A[n:1,n:1]\nL2 &lt;- t(chol(A2))\nL2!=0\n\n6 x 6 sparse Matrix of class \"ltCMatrix\"\n                \n[1,] | . . . . .\n[2,] . | . . . .\n[3,] . . | . . .\n[4,] . . . | . .\n[5,] . . . . | .\n[6,] | | | | | |\n\n\n\n\nSo what is the lesson here?\nThe lesson is that the sparse Cholesky algorithm cares deeply about what order the rows and columns of the matrix are in. This is why, in the previous post, we put the dense rows and columns of \\(Q_{u \\mid y, \\theta}\\) at the end of the matrix!\nLuckily, a lot of clever graph theorists got on the job a while back and found a number of good algorithms for finding decent23 ways to reorder the vertices of a graph to minimise fill in. There are two particularly well-known reorderings: the approximate minimum degree (AMD) reordering and the nested-dissection reordering. Neither of these are easily available in Python24.\nAMD is a bog-standard black box that is a greedy reordering that tries to label the next vertex so that graph you get after removing that vertex and adding edges between all of the nodes that connect to that vertex isn’t too fucked.\nNested dissection tries to generalise the toy example above by finding nodes that separate the graph into two minimally connected components. The separator node is then labelled last. The process is repeated until you run out of nodes. This algorithm can be very efficient in some cases (eg if the graph is planar25, the sparse Cholesky algorithm using this reordering provably costs at most \\(\\mathcal{O}(n^{3/2})\\)).\nTypically, you compute multiple reorderings26 and pick the one that results in the least fill in."
  },
  {
    "objectID": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#which-elements-of-the-cholesky-factor-are-non-zero-aka-symbolic-factorisation",
    "href": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#which-elements-of-the-cholesky-factor-are-non-zero-aka-symbolic-factorisation",
    "title": "Sparse Matrices 2: An invitation to a sparse Cholesky factorisation",
    "section": "Which elements of the Cholesky factor are non-zero (aka symbolic factorisation)",
    "text": "Which elements of the Cholesky factor are non-zero (aka symbolic factorisation)\nOk. So I guess we’ve got to work out an algorithm for computing the non-zero structure of a sparse Cholesky factor. Naively, this seems easy: just use the Cholesky algorithm and mark which elements are non-zero.\nBut this is slow and inefficient. You’re not thinking like a programmer! Or a graph theorist. So let’s talk about how to do this efficiently.\n\nThe elimination tree\nLet’s consider the graph \\(\\mathcal{G}_L\\) that contains the sparsity pattern of \\(L\\). We know that the non-zero structure consists of all \\((i,j)\\) such that \\(i &lt; j\\) and there is a path \\(in \\mathcal{G}\\) from \\(i\\) to \\(j\\). This means we could just compute that and make \\(\\mathcal{G}_L\\).\nThe thing that you should notice immediately is that there is a lot of redundancy in this structure. Remember that if \\(L_{ik}\\) is non-zero and \\(L_{jk}\\) is also non-zero, then \\(L_{ij}\\) is also non-zero.\nThis suggests that if we have \\((i,k)\\) and \\((j,k)\\) in the graph, we can remove the edge \\((i,j)\\) from \\(\\mathcal{G}_L\\) and still be able to work out that \\(L_{ij}\\) is non-zero. This new graph is no longer the graph associated with \\(L\\) but, for our purposes, it contains the same information.\nIf we continue pruning the graph this way, we are going to end up with a27 rooted tree! From this tree, which is called the elimination tree of \\(A\\)28 we can easily work out the non-zero structure of \\(L\\).\nThe elimination tree is the fundamental structure needed to build an efficient sparse Cholesky algorithm. We are not going to use it to its full potential, but it is very cheap to compute (roughly29 \\(\\mathcal{O}(\\operatorname{nnz}(A))\\) operations).\nOnce we have the elimination tree, it’s cheap to compute properties of \\(L\\) like the number of non-zeros in a column, the exact sparsity pattern of every column, which columns can be grouped together to form supernodes30, and the approximate minimum degree reordering.\nAll of those things would be necessary for a modern, industrial-strength sparse Cholesky factorisation. But, and I cannot stress this enough, fuck that shit.\n\n\nThe symbolic factorisation\nWe are doing the easy version. Which is to say I refuse to do anything here that couldn’t be easily done in the early 90s. Specifically, we are going to use the version of this thatGeorge, Liu, and Ng wrote about31 in the 90s. Understanding this is, I think, enough to see how things like supernodal factorisations work, but it’s so much less to keep track of.\nThe nice thing about this method is that we compute the elimination tree implicitly as we go along.\nLet \\(\\mathcal{L}_j\\) be the non-zero entries in the \\(j\\)th column of \\(L\\). Then our discussion in the previous section tells us that we need to determine the reach of the node i \\[\n\\text{Reach}(j, S_j) = \\left\\{i: \\text{there is a path from } i\\text{ to }j\\text{ through }S_j\\right\\},\n\\] where \\(S_j = \\{1,\\ldots, j-1\\}\\).\nIf we can compute the reach, then \\(\\mathcal{L}_j  = \\text{Reach}(j, S_j) \\cup\\{j\\}\\)!\nThis is where the elimination tree comes in: it is an efficient representation of these sets. Indeed, \\(i \\in \\text{Reach}(j, S_j)\\) if and only if there is a directed32 path from \\(j\\) to \\(i\\) in the elimination tree! Now this tree is ordered33 so that if \\(i\\) is a child of \\(j\\) (aka directly below it in the tree), then \\(i &lt; j\\). This means that its column in the Cholesky factorisation has already been computed. So all of the nodes that can be reached from \\(j\\) by going through \\(i\\) are in \\(\\mathcal{L}_{i} \\cap \\{j+1, \\ldots, n\\}\\).\nThis means that we can compute the non-zeros of the \\(j\\)th column of \\(L\\) efficiently from the non-zeros of all of the (very few, hopefully) columns associated with the child nodes of \\(j\\).\nSo all that’s left is to ask “how can we find the child?” (as phones around the city start buzzing). Well, a little bit of thinking time should convince you that if \\[\np = \\min\\{i : i \\in \\text{Reach}(j, S_j) \\},\n\\] then \\(p\\) is the parent of \\(i\\). Or, the parent of column \\(j\\) is the index of its first34 non-zero below the diagonal.\nWe can put all of these observations together into the following algorithm. We assume that we are given the non-zero structure of tril(A) (aka the lower-triangle of \\(A\\)).\n\nimport numpy as np\n\ndef _symbolic_factor_csc(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] &gt; j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) &gt; 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n  \n\nThis was the first piece of Python I’ve written in about 13 years35, so it’s a bit shit. Nevertheless, it works. It is possible to replace the children structure by a linked list implemented in an n-dimensional integer array36, but why bother. This function is run once.\nIt’s also worth noting that the children array expresses the elimination tree. If we were going to do something with it explicitly, we could just spit it out and reshape it into a more useful data structure.\nThere’s one more piece of tedium before we can get to the main event: we need to do a deep copy of \\(A\\) into the data structure of \\(L\\). There is no37 avoiding this.\nHere is the code.\n\ndef _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x"
  },
  {
    "objectID": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#computing-the-cholesky-factorisation",
    "href": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#computing-the-cholesky-factorisation",
    "title": "Sparse Matrices 2: An invitation to a sparse Cholesky factorisation",
    "section": "Computing the Cholesky factorisation",
    "text": "Computing the Cholesky factorisation\nIt feels like we’ve been going for a really long time and we still don’t have a Cholesky factorisation. Mate. I feel your pain. Believe me.\nBut we are here now: everything is in place. We can now write down the Cholesky algorithm!\nThe algorithm is as it was before, with the main difference being that we now know two things:\n\nWe only need to update tmp with descendent of j in the elimination tree.\nThat’s it. That is the only thing we know.\n\nOf course, we could use the elimination tree to do this very efficiently, but, as per my last email, I do not care. So we will simply build up a copy of all of the descendants. This will obviously be less efficient, but it’s fine for our purposes. Let’s face it, we’re all going to die eventually.\nSo here it goes.\n\ndef _sparse_cholesky_csc_impl(L_indices, L_indptr, L_x):\n    n = len(L_indptr) - 1\n    descendant = [[] for j in range(0, n)]\n    for j in range(0, n):\n        tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n        for bebe in descendant[j]:\n            k = bebe[0]\n            Ljk= L_x[bebe[1]]\n            pad = np.nonzero(                                                \\\n              L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n            update_idx = np.nonzero(np.in1d(                                 \\\n              L_indices[L_indptr[j]:L_indptr[j+1]],                          \\\n              L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n            tmp[update_idx] = tmp[update_idx] -                              \\\n              Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n        diag = np.sqrt(tmp[0])\n        L_x[L_indptr[j]] = diag\n        L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n        for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n            descendant[L_indices[idx]].append((j, idx))\n    return L_x\n\nThe one thing that you’ll note in this code38 is that we are implicitly using things that we know about the sparsity structure of the \\(j\\)th column. In particular, we know that the sparsity structure of the \\(j\\)th column is the union of the relevant parts of the sparsity structure of their dependent columns. This allows a lot of our faster indexing to work.\nFinally, we can put it all together.\n\ndef sparse_cholesky_csc(A_indices, A_indptr, A_x):\n    L_indices, L_indptr= _symbolic_factor_csc(A_indices, A_indptr)\n    L_x = _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr)\n    L_x = _sparse_cholesky_csc_impl(L_indices, L_indptr, L_x)\n    return L_indices, L_indptr, L_x\n\nRight. Let’s test it. We’re going to work on a particular39 sparse matrix.\n\nfrom scipy import sparse\n\nn = 50\none_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\nA = sparse.kronsum(one_d, one_d) + sparse.eye(n*n)\nA_lower = sparse.tril(A, format = \"csc\")\nA_indices = A_lower.indices\nA_indptr = A_lower.indptr\nA_x = A_lower.data\n\nL_indices, L_indptr, L_x = sparse_cholesky_csc(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr), shape = (n**2, n**2))\n\nerr = np.sum(np.abs((A - L @ L.transpose()).todense()))\nprint(f\"Error in Cholesky is {err}\")\n\nError in Cholesky is 3.871041263071504e-12\n\nnnz = len(L_x)\nprint(f\"Number of non-zeros is {nnz} (fill in of {len(L_x) - len(A_x)})\")\n\nNumber of non-zeros is 125049 (fill in of 117649)\n\n\nFinally, let’s demonstrate that we can reduce the amount of fill-in with a reordering. Obviously, the built in permutation in scipy is crappy, so we will not see much of a difference. But nevertheless. It’s there.\n\nperm = sparse.csgraph.reverse_cuthill_mckee(A, symmetric_mode=True)\nprint(perm)\n\n[2499 2498 2449 ...   50    1    0]\n\nA_perm = A[perm[:,None], perm]\nA_perm_lower = sparse.tril(A_perm, format = \"csc\")\nA_indices = A_perm_lower.indices\nA_indptr = A_perm_lower.indptr\nA_x = A_perm_lower.data\n\nL_indices, L_indptr, L_x = sparse_cholesky_csc(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr), shape = (n**2, n**2))\nerr = np.sum(np.abs((A_perm - L @ L.transpose()).todense()))\nprint(f\"Error in Cholesky is {err}\")\n\nError in Cholesky is 3.0580421951974465e-12\n\nnnz_rcm = len(L_x)\nprint(f\"Number of non-zeros is {nnz_rcm} (fill in of {len(L_x) - len(A_x)}),\\nwhich is less than the unpermuted matrix, which had {nnz} non-zeros.\")\n\nNumber of non-zeros is 87025 (fill in of 79625),\nwhich is less than the unpermuted matrix, which had 125049 non-zeros.\n\n\nAnd finally, let’s check that we’ve not made some fake non-zeros. To do this we need to wander back into R because scipy doesn’t have a sparse Cholesky40 factorisation.\n\nind &lt;- py$A_indices\nindptr &lt;- py$A_indptr\nx &lt;- as.numeric(py$A_x)\nA = sparseMatrix(i = ind + 1, p = indptr, x=x, symmetric = TRUE)\n\nL = t(chol(A))\nsum(L@i - py$L_indices)\n\n[1] 0\n\nsum(L@p - py$L_indptr)\n\n[1] 0\n\n\nPerfect."
  },
  {
    "objectID": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#ok-we-are-done-for-today.",
    "href": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#ok-we-are-done-for-today.",
    "title": "Sparse Matrices 2: An invitation to a sparse Cholesky factorisation",
    "section": "Ok we are done for today.",
    "text": "Ok we are done for today.\nI was hoping that we were going to make it to the JAX implementation, but this is long enough now. And I suspect that there will be some issues that are going to come up.\nIf you want some references, I recommend:\n\nGeorge, Liu, and Ng’s notes (warning: FORTRAN).\nTimothy Davis’ book (warning: pure C).\nLiu’s survey paper about elimination trees (warning: trees).\nRue and Held’s book (Statistically motivated).\n\nObviously this is a massive area and I obviously did not do it justice in a single blog post. It’s well worth looking further into. It is very cool. And obviously, I go through all this41 to get a prototype that I can play with all of the bits of. For the love of god, use Cholmod or Eigen or MUMPS or literally anything else. The only reason to write these yourself is to learn how to understand it."
  },
  {
    "objectID": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#footnotes",
    "href": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html#footnotes",
    "title": "Sparse Matrices 2: An invitation to a sparse Cholesky factorisation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe old numerical linear algebra naming conventions: Symmetric letters are symmetric matrices, upper case is a matrix, lower case is a vector, etc etc etc. Obviously, all conventions in statistics go against this so who really cares. Burn it all down.↩︎\nGo girl. Give us nothing.↩︎\nor scalars↩︎\nThis is actually how you check if a matrix is SPD. Such a useful agorithm!↩︎\nThis variant is called the left-looking Cholesky. There are 6 distinct ways to rearrange these computations that lead to algorithms that are well-adapted to different structures. The left-looking algorithm is well adapted to matrices stored column-by-column. But it is not the only one! The variant of the sparse Cholesky in Matlab and Eigen is the upward-looking Cholesky. CHOLMOD uses the left-looking Cholesky (because that’s how you get supernodes). MUMPS uses the right-looking variant. Honestly this is a fucking fascinating wormhole you can fall down. A solid review of some of the possibilities is in Chapter 4 of Tim Davis’ book.↩︎\nHere A is a \\(n\\times n\\) matrix and u' is the transpose of the vector u.↩︎\nYou can also see that if \\(A\\) is stored in memory by stacking the columns, this algorithm is set up to be fairly memory efficient. Of course, if you find yourself caring about what your cache is doing, you’ve gone astray somewhere. That is why professionals have coded this up (only a fool competes with LAPACK).↩︎\nThe ultimate language of scientific computing. Do not slide into my DMs and suggest Julia is.↩︎\nYou may be thinking well surely we have to use a row-major ordering. But honey let me tell you. We are building our own damn storage method, so we can order it however we bloody want. Also, somewhere down the line I’m going to do this in Eigen, which is column major by default.↩︎\nIf you look at the algorithm, you’ll see that we only need to store the diagonal and the entries below. This is enough (in general) because we know the matrix is symmetric!↩︎\nCPU operations are a lot less memory-limited than they used to be, but nevertheless it piles up. GPU operations still very much are, but sparse matrix operations mostly don’t have the arithmetic intensity to be worth putting on a GPU.↩︎\n(NB: zero-based indexing!) This is a superfluous entry (the information is available elsewhere), but having it in makes life just a million times easier because you don’t have to treat the final column separately!.↩︎\nZERO BASED, PYTHON SLICES↩︎\nI am not a headless torso that can’t host. I differentiate.↩︎\nWe only care about undirected graphs↩︎\nOr from \\(0\\) to \\(n-1\\) if you have hate in your heart and darkness in your soul.↩︎\nTo get from the previous version of the algorithm to this, we unwound all of those beautiful vectorised matrix-vector products. This would be a terrible idea if we were doing a dense Cholesky, but as general rule if you are implementing your own dense Cholesky factorisation you have already committed to a terrible idea. (The same, to be honest, is true for sparse Choleskys. But nevertheless, she persisted.)↩︎\nor trees or really any discrete structure.↩︎\nDon’t kid yourself, we look this shit up.↩︎\nThis means that all of the pairs \\((i, v_1)\\), \\((v_i, v_{i+1})\\) and \\((v_{\\ell-1}, v_j)\\) are all in the edge set \\(\\mathcal{E}\\)↩︎\nThe specific choices building this matrix are to make sure it’s positive definite. The transpose is there because in R, R &lt;- chol(A) returns an upper triangular matrix that satisfies \\(A = R^TR\\). I assume this is because C has row-major storage, but I honestly don’t care enough to look it up.↩︎\nHere the pivot = FALSE option is needed because the default for a sparse Cholesky decomposition in R is to re-order the vertices to try to minimise the fill-in. But that goes against the example!↩︎\nFinding the minimum fill reordering is NP-hard, so everything is heuristic.↩︎\nscipy has the reverse Cuthill-McKee reordering—which is shit—easily available. As far as I can tell, the easiest way to get AMD out is to factorise a sparse matrix in scipy and pull the reordering out. If I were less lazy, I’d probably just bind SuiteSparse’s AMD algorithm, which is permissively licensed. But nah. The standard nested-dissection implementation is in the METIS package, which used to have a shit license but is now Apache2.0. Good on you METIS!↩︎\nand some other cases↩︎\nThey are cheap to compute↩︎\nActually, you get a forest in general. You get a tree if \\(\\mathcal{G}\\) has a single connected component, otherwise you get a bunch of disjoint trees. But we still call it a tree because maths is wild.↩︎\nFun fact: it is the spanning tree of the graph of \\(L + L^T\\). Was that fun? I don’t think that was fun.↩︎\nThis is morally but not actually true. There is a variant (slower in practice, faster asymptotically), that costs \\(\\mathcal{O}\\left(\\operatorname{nnz}(A)\\alpha(\\operatorname{nnz}(A), n)\\right)\\), where \\(\\alpha(m,n)\\) is the inverse Ackerman function, which is a very slowly growing function that is always equal to 4 for our purposes. The actual version that people use is technically \\(\\mathcal{O}(\\operatorname{nnz}(A) \\log n)\\), but is faster and the \\(\\log n\\) is never seen in practice.↩︎\nThis is beyond the scope, but basically it’s trying to find groups of nodes that can be eliminated as a block using dense matrix operations. This leads to a much more efficient algorithm.↩︎\nThere is, of course, a typo in the algorithm we’re about to implement. We’re using the correct version from here.↩︎\nfrom parent to child (aka in descending node order)↩︎\nby construction↩︎\nIf there are no non-zeros below the diagonal, then we have a root of one of the trees in the forest!↩︎\nI did not make it prettier because a) I think it’s useful to show bad code sometimes, and b) I can’t be arsed. The real file has some comments in it because I am not a monster, but in some sense this whole damn blog is a code comment.↩︎\nThe George, Liu, Ng book does that in FORTRAN. Enjoy decoding it.↩︎\nWell, there is some avoiding this. If the amount of fill in is small, it may be more efficient to do insertions instead. But again, I am not going to bother. And anyway. If A_x is a JAX array, it’s going to be immutable and we are not going to be able to avoid the deep copy.↩︎\nand in the deep copy code↩︎\nThis is the discretisation of a 2D laplacian on a square with some specific boundary conditions↩︎\nCholmod, which is the natural choice, is GPL’d, which basically means it can’t be used in something like Scipy. R does not have this problem.↩︎\nBjörk voice↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Un garçon pas comme les autres (Bayes)",
    "section": "",
    "text": "An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won’t you buy me a Laplace approximation\n\n\n\n\n\n\nJAX\n\n\nLaplace approximation\n\n\nSparse matrices\n\n\nAutodiff\n\n\n\nExploiting linearity and sparisty to speed up JAX Hessians and slowly ruin my life. \n\n\n\n\n\nMay 8, 2024\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nDiffusion models; or Yet another way to sample from an arbitrary distribution\n\n\n\n\n\n\nDiffusion model\n\n\nIntroductions\n\n\n\nIf you gaze for long into an abyss, the abyss creates a cutting-edge AI method. So saddle up your Ganzfeld effect, girls. We ride at dawn. \n\n\n\n\n\nFeb 9, 2023\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nMarkovian Gaussian processes: A lot of theory and some practical stuff\n\n\n\n\n\n\nGaussian processes\n\n\nFundamentals\n\n\nTheory\n\n\nDeep Dives\n\n\n\nWell this is gonna be technical. And yes, I’m going to define it three ways. Because that’s how comedy works. \n\n\n\n\n\nJan 21, 2023\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nSparse matrices part 7a: Another shot at JAX-ing the Cholesky decomposition\n\n\n\n\n\n\nJAX\n\n\nSparse matrices\n\n\nAutodiff\n\n\n\nI work in R a lot so I should be used to weird syntax. This part looks at the non-zero pattern. \n\n\n\n\n\nDec 2, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC with the wrong acceptance probability\n\n\n\n\n\n\nFundamentals\n\n\nMCMC\n\n\nBayes\n\n\n\nSometimes I chat work with people. Sometimes an interesting factlet comes up. Sometimes I blog about it. This is one of those times. \n\n\n\n\n\nNov 23, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nOn that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for\n\n\n\n\n\n\nFundamentals\n\n\nSurvey sampling\n\n\nMRP\n\n\nBayes\n\n\n\nLook, it’s a dull example of Bayes being bad. But it comes up often enough to be worth talking about. I’m going to, unsurprisingly, argue that Bayes isn’t bad. Neither are Robings/Ritov/Wasserman wrong. They’re just looking at the problem through a different lens. \n\n\n\n\n\nNov 15, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nPriors for the parameters in a Gaussian process\n\n\n\n\n\n\nPrior distributions\n\n\nGaussian Processes\n\n\nPC priors\n\n\n\nIf you’re not a machine learner (and sometimes if you are), Gaussian processes need priors on their parameters. Like everything else to do with Gaussian processes, this can be delicate. This post works through some options. \n\n\n\n\n\nSep 27, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nA first look at multilevel regression; or Everybody’s got something to hide except me and my macaques\n\n\n\n\n\n\nMultilevel models\n\n\nVisual diagnostics\n\n\nPrior distributions\n\n\nfundamentals\n\n\n\nA small introduction to multilevel models. Why? Because I said so, that’s why. And you will simply not believe what happens to residual plots. \n\n\n\n\n\nSep 6, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nPriors part 4: Specifying priors that appropriately penalise complexity\n\n\n\n\n\n\nPrior distributions\n\n\nfundamentals\n\n\nPC priors\n\n\n\nLet us not lie. Specifying priors are hard. This post steps through a technique for setting priors that I think gives a good basis for realistically complex problems. \n\n\n\n\n\nSep 3, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nTail stabilization of importance sampling etimators: A bit of theory\n\n\n\n\n\n\nImportance sampling\n\n\nComputation\n\n\nTruncated importance sampling\n\n\nWindsorized importance sampling\n\n\nPareto smoothed importance sampling\n\n\nPSIS\n\n\n\nLook. I had to do it so I wrote it out in detail. This is some of the convergence theory for truncated and winzorised importance sampling estimators \n\n\n\n\n\nJun 15, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nSparse matrices 6: To catch a derivative, first you’ve got to think like a derivative\n\n\n\n\n\n\nJAX\n\n\nSparse matrices\n\n\nAutodiff\n\n\n\nOpen up the kennels, Kenneth. Mamma’s coming home tonight. \n\n\n\n\n\nMay 30, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Matrices 5: I bind you Nancy\n\n\n\n\n\n\nSparse matrices\n\n\nSparse Cholesky factorisation\n\n\nPython\n\n\nJAX\n\n\n\nA new JAX primitive? In this economy? \n\n\n\n\n\nMay 20, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Matrices 4: Design is my passion\n\n\n\n\n\n\nSparse matrices\n\n\nSparse Cholesky factorisation\n\n\nPython\n\n\nJAX\n\n\n\nJust some harmeless notes. Like the ones Judy Dench took in that movie. \n\n\n\n\n\nMay 16, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Matrices 3: Failing at JAX\n\n\n\n\n\n\nSparse matrices\n\n\nSparse Cholesky factorisation\n\n\nPython\n\n\nJAX\n\n\n\nTakes a long drag on cigarette. JAX? Where was he when I had my cancer? \n\n\n\n\n\nMay 14, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Matrices 2: An invitation to a sparse Cholesky factorisation\n\n\n\n\n\n\nSparse matrices\n\n\nSparse Cholesky factorisation\n\n\nPython\n\n\n\nCome for the details, stay for the shitty Python, leave with disappointment. Not unlike the experience of dating me. \n\n\n\n\n\nMar 31, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Matrices 1: The linear algebra of linear mixed effects models and their generalisations\n\n\n\n\n\n\nSparse matrices\n\n\nLinear mixed models\n\n\n\nHubris. Just hubris. But before the fall comes the statement of purpose. This is that statement. \n\n\n\n\n\nMar 22, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nBarry Gibb came fourth in a Barry Gibb look alike contest (Repost)\n\n\n\n\n\n\nComputation\n\n\nAssessing algorithms\n\n\n\nA repost from Andrew’s blog about comparing computational methods for performing a task. (Lightly edited.) Original posted 20 October, 2017. \n\n\n\n\n\nJan 26, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nWhy won’t you cheat with me? (Repost)\n\n\n\n\n\n\nPrior distributions\n\n\nFundamentals\n\n\nDesign dependence\n\n\n\nA repost from Andrew’s blog about how design information infects multivariate priors. (Lightly edited. Well, a bit more than lightly because the last version didn’t fully make sense. But whatever. Blogs, eh.) Original posted 5 November, 2017. \n\n\n\n\n\nDec 9, 2021\n\n\nDan Simpson, Dan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe king must die (repost)\n\n\n \n\n\nBayesian Lasso\n\nVariable Selection\n\nFundamentals\n\nTheory\n\nThings that don't work\n\n\n \n\nA repost (with edits, revisions, and footnotes) from Andrew's blog about how much I hate the Bayesian Lasso. Originally published 2nd November, 2017.\n\n\n\n\n`Dec 8, 2021`{=html}\nDan Simpson, Dan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nGetting into the subspace; or what happens when you approximate a Gaussian process\n\n\n\n\n\n\nGaussian processes\n\n\nFundamentals\n\n\nTheory\n\n\n\nFuck man, I don’t know about this one. A lot of stuff happens. At some point there’s a lot of PDEs. There are proofs. Back away. \n\n\n\n\n\nNov 24, 2021\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nYes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness\n\n\n\n\n\n\nGaussian processes\n\n\nFundamentals\n\n\nTheory\n\n\nDeep dive\n\n\n\nGaussian processes. As narrated by an increasingly deranged man during a day of torrential rain. \n\n\n\n\n\nNov 3, 2021\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nPriors: Fire With Fire (Track 3)\n\n\n\n\n\n\nPrior distributions\n\n\nFundamentals\n\n\n\nObjective priors? In finite dimensions? A confidence trick? Yes. \n\n\n\n\n\nOct 17, 2021\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nPriors: Whole New Way (Track 2)\n\n\n\n\n\n\nPrior distributions\n\n\nFundamentals\n\n\n\nConjugate Priors? The crystal deoderant of Bayesian statistics \n\n\n\n\n\nOct 16, 2021\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\nPriors: Night work (Track 1)\n\n\n\n\n\n\nPrior distributions\n\n\nFundamentals\n\n\n\nPriors? Defined. Questions? Outlined. Purpose? Declared. \n\n\n\n\n\nOct 15, 2021\n\n\nDan Simpson\n\n\n\n\n\n\n\n\n\n\n\n\n\\((n-1)\\)-sane in the membrane\n\n\n\n\n\n\nTeaching\n\n\nFundamentals\n\n\nOpinionated\n\n\n\nWindmills? Tilted. Topic? Boring. (n-1)? No. \n\n\n\n\n\nOct 14, 2021\n\n\nDan Simpson\n\n\n\n\n\n\nNo matching items"
  }
]