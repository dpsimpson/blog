<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.15">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dan Simpson">
<meta name="dcterms.date" content="2023-01-21">
<meta name="description" content="Well this is gonna be technical. And yes, I’m going to define it three ways. Because that’s how comedy works.">

<title>Un garçon pas comme les autres (Bayes) - Markovian Gaussian processes: A lot of theory and some practical stuff</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Un garçon pas comme les autres (Bayes) - Markovian Gaussian processes: A lot of theory and some practical stuff">
<meta property="og:description" content="Well this is gonna be technical. And yes, I’m going to define it three ways. Because that’s how comedy works.">
<meta property="og:image" content="https://dansblog.netlify.app/posts/2023-01-21-markov/gays.png">
<meta property="og:site-name" content="Un garçon pas comme les autres (Bayes)">
<meta property="og:image:height" content="640">
<meta property="og:image:width" content="560">
<meta name="twitter:title" content="Markovian Gaussian processes: A lot of theory and some practical stuff">
<meta name="twitter:description" content="Well this is gonna be technical. And yes, I’m going to define it three ways. Because that’s how comedy works.">
<meta name="twitter:image" content="https://dansblog.netlify.app/posts/2023-01-21-markov/gays.png">
<meta name="twitter:creator" content="@dan_p_simpson">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="640">
<meta name="twitter:image-width" content="560">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Un garçon pas comme les autres (Bayes)</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About this blog</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/dan_p_simpson"><i class="bi bi-twitter" role="img" aria-label="twitter">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dpsimpson"><i class="bi bi-github" role="img" aria-label="github">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://dansblog.netlify.app"><i class="bi bi-person-circle" role="img" aria-label="website">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Markovian Gaussian processes: A lot of theory and some practical stuff</h1>
                  <div>
        <div class="description">
          <p>Well this is gonna be technical. And yes, I’m going to define it three ways. Because that’s how comedy works.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Gaussian processes</div>
                <div class="quarto-category">Fundamentals</div>
                <div class="quarto-category">Theory</div>
                <div class="quarto-category">Deep Dives</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dan Simpson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 21, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#gaussian-processes-via-the-covariance-operator" id="toc-gaussian-processes-via-the-covariance-operator" class="nav-link active" data-scroll-target="#gaussian-processes-via-the-covariance-operator">Gaussian processes via the covariance operator</a>
  <ul class="collapse">
  <li><a href="#white-noise-and-its-associated-things" id="toc-white-noise-and-its-associated-things" class="nav-link" data-scroll-target="#white-noise-and-its-associated-things">White noise and its associated things</a></li>
  <li><a href="#the-generalised-gaussian-process-eta-mathcalc12w" id="toc-the-generalised-gaussian-process-eta-mathcalc12w" class="nav-link" data-scroll-target="#the-generalised-gaussian-process-eta-mathcalc12w">The generalised Gaussian process <span class="math inline">\(\eta = \mathcal{C}^{1/2}W\)</span></a></li>
  <li><a href="#approximating-gps-when-mathcalc-12-is-a-differential-operator" id="toc-approximating-gps-when-mathcalc-12-is-a-differential-operator" class="nav-link" data-scroll-target="#approximating-gps-when-mathcalc-12-is-a-differential-operator">Approximating GPs when <span class="math inline">\(\mathcal{C}^{-1/2}\)</span> is a differential operator</a></li>
  </ul></li>
  <li><a href="#the-markov-property-for-on-abstract-spaces" id="toc-the-markov-property-for-on-abstract-spaces" class="nav-link" data-scroll-target="#the-markov-property-for-on-abstract-spaces">The Markov property for on abstract spaces</a>
  <ul class="collapse">
  <li><a href="#rewriting-the-markov-property-i-splitting-spaces" id="toc-rewriting-the-markov-property-i-splitting-spaces" class="nav-link" data-scroll-target="#rewriting-the-markov-property-i-splitting-spaces">Rewriting the Markov property I: Splitting spaces</a></li>
  <li><a href="#rewriting-the-markov-property-ii-the-dual-random-field-ha" id="toc-rewriting-the-markov-property-ii-the-dual-random-field-ha" class="nav-link" data-scroll-target="#rewriting-the-markov-property-ii-the-dual-random-field-ha">Rewriting the Markov property II: The dual random field <span class="math inline">\(H^*(A)\)</span></a></li>
  <li><a href="#building-out-our-toolset-with-the-conjugate-gp" id="toc-building-out-our-toolset-with-the-conjugate-gp" class="nav-link" data-scroll-target="#building-out-our-toolset-with-the-conjugate-gp">Building out our toolset with the conjugate GP</a></li>
  <li><a href="#when-does-xi-exits-or-a-surprising-time-with-the-reproducing-kernel-hilbert-space" id="toc-when-does-xi-exits-or-a-surprising-time-with-the-reproducing-kernel-hilbert-space" class="nav-link" data-scroll-target="#when-does-xi-exits-or-a-surprising-time-with-the-reproducing-kernel-hilbert-space">When does <span class="math inline">\(\xi^*\)</span> exits? or, A surprising time with the reproducing kernel Hilbert space</a></li>
  <li><a href="#but-when-does-hs-h_scperp" id="toc-but-when-does-hs-h_scperp" class="nav-link" data-scroll-target="#but-when-does-hs-h_scperp">But when does <span class="math inline">\(H^*(S) = H_+(S^c)^\perp\)</span>?</a></li>
  <li><a href="#at-long-last-an-rkhs-characterisation-of-the-markov-property" id="toc-at-long-last-an-rkhs-characterisation-of-the-markov-property" class="nav-link" data-scroll-target="#at-long-last-an-rkhs-characterisation-of-the-markov-property">At long last, an RKHS characterisation of the Markov property</a></li>
  <li><a href="#putting-this-all-in-terms-of-eta" id="toc-putting-this-all-in-terms-of-eta" class="nav-link" data-scroll-target="#putting-this-all-in-terms-of-eta">Putting this all in terms of <span class="math inline">\(\eta\)</span></a></li>
  </ul></li>
  <li><a href="#using-the-rkhs-to-build-computationally-efficient-approximations-to-markovian-gps" id="toc-using-the-rkhs-to-build-computationally-efficient-approximations-to-markovian-gps" class="nav-link" data-scroll-target="#using-the-rkhs-to-build-computationally-efficient-approximations-to-markovian-gps">Using the RKHS to build computationally efficient approximations to Markovian GPs</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Gaussian processes are lovely things. I’m a big fan. They are, however, thirsty. They will take your memory, your time, and anything else they can. Basically, the art of fitting Gaussian process models is the fine art of reducing the GP model until it’s simple enough to fit while still being flexible enough to be useful.</p>
<p>There’s a long literature on effective approximation to Gaussian Processes that don’t turn out to be computational nightmares. I’m definitely not going to summarise them here, but I’ll point to an <a href="https://dansblog.netlify.app/posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html">earlier (quite technical) post</a> that mentioned some of them. The particular computational approximation that I am most fond of makes use of the Markov property and efficient sparse matrix computations to reduce memory use and make the linear algebra operations significantly faster.</p>
<p>One of the odder challenges with Markov models is that information about how Markov structures work in more than one dimension can be quite difficult to find. So in this post I am going to lay out some of the theory.</p>
<p>A much more practical (and readable) introduction to this topic can be found in this <a href="https://arxiv.org/abs/2111.01084">lovely paper by Finn, David, and Håvard</a>. So don’t feel the burning urge to read this post if you don’t want to. I’m approaching the material from a different viewpoint and, to be very frank with you, I was writing something else and this section just became extremely long so I decided to pull it out into a blog post.</p>
<p>So please enjoy today’s entry in <em>Dan writes about the weird corners of Gaussian processes</em>. I promise that even though this post doesn’t make it seem like this stuff is useful, it really is. If you want to know anything else about this topic, essentially all of the Markov property parts of this post come from Rozanov’s excellent book <a href="https://link.springer.com/book/10.1007/978-1-4613-8190-7">Markov Random Fields</a>.</p>
<section id="gaussian-processes-via-the-covariance-operator" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-processes-via-the-covariance-operator">Gaussian processes via the covariance operator</h2>
<p>The problem with basing our computations off a RKHS is that it is not immediately obvious how we will do that. This is in contrast to a covariance function approach, where it is quite easy<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> to work out how to convert the model specification to something you can attack with a computer.</p>
<p>The extra complexity of the RKHS pays off in modelling flexibility, both in terms of the types of model that can be build and the spaces<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> you can build them on. I am telling you this now because things are about to get a little mathematical.</p>
<p>To motivate the technique, let’s consider the covariance operator <span class="math display">\[
[\mathcal{C}f](s) = \int_T c(s, s') f(s') \, ds',
\]</span> where <span class="math inline">\(T\)</span> is the domain over which the GP is defined (usually <span class="math inline">\(\mathbb{R}^d\)</span> but maybe you’re feeling frisky).</p>
<p>To see how this could be useful, we are going to need to think a little bit about how we can simulate a multivariate Gaussian random variable <span class="math inline">\(N(0, \Sigma)\)</span>. To do this, we first compute the square root<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math inline">\(L = \Sigma^{1/2}\)</span> and sample a vector of iid standard normal variables <span class="math inline">\(z \sim N(0,I)\)</span>. Then <span class="math inline">\(u = Lz \sim N(0, \Sigma)\)</span>. You can check it by checking the covariance. (it’s ok. I’ll wait.)</p>
<p>While the square root of the covariance operator <span class="math inline">\(\mathcal{C}^{1/2}\)</span> is a fairly straightforward mathematical object^{Albeit a bit advanced. It’s straightforward in the sense that for an infinite-dimensional operaotr it happens to work a whole like a symmetric positive semi-definite matrix. It is not straightforward in the sense that your three year old could do it. Your three year old can’t do it. But it will keep them quiet in the back seat of the car while you pop into the store for some fags. It’s ok. The window’s down.}, the analogue of the iid vector of standard normal random variables is a bit more complex.</p>
<section id="white-noise-and-its-associated-things" class="level3">
<h3 class="anchored" data-anchor-id="white-noise-and-its-associated-things">White noise and its associated things</h3>
<p>Thankfully I’ve covered this <a href="file:///Users/danielsimpson/Documents/blog/_site/posts/2022-09-07-priors5/priors5.html#spectral-representations-and-the-simplest-of-the-many-many-versions-of-a-stochastic-integral">in a previous blog</a>. The engineering definition of white noise as a GP <span class="math inline">\(w(\cdot)\)</span> such that for every <span class="math inline">\(s\)</span>, <span class="math inline">\(w(s)\)</span> is an iid <span class="math inline">\(N(0,1)\)</span> random variable is not good enough for our purposes. Such a process is hauntingly irregular<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and it’s fairly difficult to actually do anything with it. Instead, we consider white noise as a random function defined on the subsets of our domain. This feels like it’s just needless technicality, but it turns out to actually be very very useful.</p>
<div id="def-white-noise" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (White noise) </strong></span>A (complex) Gaussian white noise is a random measure<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="math inline">\(W(\cdot)\)</span> such that, for every<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> disjoint<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> pair of sets <span class="math inline">\(A, B\)</span> satisfies the following properties</p>
<ol type="1">
<li><span class="math inline">\(W(A) \sim N(0, |A|)\)</span></li>
<li>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint then <span class="math inline">\(W(A\cup B) = W(A) + W(B)\)</span></li>
<li>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint then <span class="math inline">\(W(A)\)</span> and <span class="math inline">\(W(B)\)</span> are uncorrelated<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, ie <span class="math inline">\(\mathbb{E}(W(A) \overline{W(B)}) = 0\)</span>.</li>
</ol>
</div>
<p>This doesn’t feel like we are helping very much because how on <em>earth</em> am I going to define the product <span class="math inline">\(\mathcal{C}^{1/2} W\)</span>? Well the answer, you may be shocked to discover, requires a little bit more maths. We need to define an integral, which turns out to not be <em>shockingly</em> difficult to do. The trick is to realise that if I have an indicator function <span class="math display">\[
1_A(s) = \begin{cases} 1, \qquad &amp;s \in A \\ 0, &amp; s \not \in A \end{cases}
\]</span> then<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> <span class="math display">\[
\int_T 1_A(s)\, dW(s) = \int_A dW(s) = W(A) \sim N(0, |A|).
\]</span> In that calculation, I just treated <span class="math inline">\(W(s)\)</span> like I would any other measure. (If you’re more of a probability type of girl, it’s the same thing as noticing <span class="math inline">\(\mathbb{E}(1_A(X) = \Pr(X \in A)\)</span>.)</p>
<p>We can extend the above by taking the sum of two indicator function <span class="math display">\[
f(s) = f_11_{A_1}(s) + f_2 1_{A_2}(s),
\]</span> where <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are disjoint and <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are any real numbers. By the same reasoning above, and using the linearity of the integral, we get that <span class="math display">\[\begin{align*}
\int_T f(s) \, dW(s) &amp;= f_1 \int_{A_1} \,d W(s) + f_2 \int_{A_2} \,d W(s) \\
&amp;= N(0, f_1^2 |A_1| + f_2^2 |A_2|) \\
&amp;= N\left(0, \int_T f(s)^2 \,ds\right),
\end{align*}\]</span> where the last line follows by doing the ordinary<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> integral of <span class="math inline">\(f(s)\)</span>.</p>
<p>It turns out that every interesting function can be written as the limit of piecewise constant functions<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> and we can therefore <em>define</em> for any function<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> <span class="math inline">\(f\in L^2(T)\)</span> <span class="math display">\[
\int f(s) \, dW(s) \sim N\left(0, \int_T f(s)^2 \,ds\right).
\]</span></p>
<p>With this notion in hand, we can finally define the action of an operator on white noise.</p>
<div id="def-operator-on-noise" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (The action of an operator on white noise) </strong></span>Let <span class="math inline">\(\mathcal{A}\)</span> be an operator on some Hilbert space of functions <span class="math inline">\(H\)</span> with adjoint <span class="math inline">\(\mathcal{A}^*\)</span>, then we define <span class="math inline">\(\mathcal{A}W\)</span> to be the random measure that satisfies, for every <span class="math inline">\(f \in \operatorname{Dom}(\mathcal{A^*})\)</span>, <span class="math display">\[
\int_T f(s) \, d (\mathcal{A}W)(s) = \int_T \mathcal{A}^*f(s) \, dW(s).
\]</span></p>
</div>
</section>
<section id="the-generalised-gaussian-process-eta-mathcalc12w" class="level3">
<h3 class="anchored" data-anchor-id="the-generalised-gaussian-process-eta-mathcalc12w">The generalised Gaussian process <span class="math inline">\(\eta = \mathcal{C}^{1/2}W\)</span></h3>
<p>One of those inconvenient things that you may have noticed from above is that <span class="math inline">\(\mathcal{C}^{1/2}W\)</span> is <em>not</em> going to be a function. It is going to be a measure or, as it is more commonly known, a <em>generalised Gaussian process</em>. This is the GP analogue of a generalised function and, as such, only gives an actual value when you integrate it against some sufficiently smooth function.</p>
<div id="def-generalised-gp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Generalised Gauusian Process) </strong></span>A generalised Gaussian process <span class="math inline">\(\xi\)</span> is a random signed measure (or a random generalised function) that, for any <span class="math inline">\(f \in C^\infty_0(T)\)</span>, <span class="math inline">\(\int_T f(s)\,d\xi(s)\)</span> is Gaussian. We will often write <span class="math display">\[
\xi(f) = \int_T f(s)\,d\xi(s),
\]</span> which helps us understand that a generalised GP is indexed by functions.</p>
</div>
<p>In order to separate this out from the ordinary GP <span class="math inline">\(u(s)\)</span>, we will write it as <span class="math display">\[
\eta = \mathcal{C}^{1/2}W.
\]</span> These two ideas coincide in the special case where <span class="math display">\[
\eta = u(s)\,ds,
\]</span> which will occur when <span class="math inline">\(\mathcal{C}^{1/2}\)</span> smooths the white noise sufficiently. In all of the cases we really care about today, this happens. But there are plenty of Gaussian processes that can only be considered as generalised GPs<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
</section>
<section id="approximating-gps-when-mathcalc-12-is-a-differential-operator" class="level3">
<h3 class="anchored" data-anchor-id="approximating-gps-when-mathcalc-12-is-a-differential-operator">Approximating GPs when <span class="math inline">\(\mathcal{C}^{-1/2}\)</span> is a differential operator</h3>
<p>This type of construction for <span class="math inline">\(\eta\)</span> is used in two different situations: kernel convolution methods directly use the representation, and the SPDE methods of <a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2011.00777.x">Lindgren, Lindström and Rue</a> use it indirectly.</p>
<p>I’m interested in the SPDE method, as it ties into today’s topic. Also because it works really well. This method uses a slightly modified version of the above equation <span class="math display">\[
\mathcal{C}^{-1/2}\eta = W,
\]</span> where <span class="math inline">\(\mathcal{C}^{-1/2}\)</span> is the (left) inverse of <span class="math inline">\(\mathcal{C}^{1/2}\)</span>. I have covered this method <a href="https://dansblog.netlify.app/posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html#example-3-the-spde-method">in a previous post</a>, but to remind you the SDPE method in its simplest form involves three steps:</p>
<ol type="1">
<li><p>Approximate <span class="math inline">\(\eta = \sum_{j=1}^n u_j \psi_j(s)\,ds\)</span> for some set of weights <span class="math inline">\(u \sim N(0, Q^{-1})\)</span> and a set of deterministic functions <span class="math inline">\(\psi_j\)</span> that we are going to use to approximate the GP</p></li>
<li><p>Approximate<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> the <em>test function</em> <span class="math inline">\(f = \sum_{k=1}^n f_k \psi_k(s)\)</span> for some set of deterministic weights <span class="math inline">\(f_j\)</span></p></li>
<li><p>Plug these approximations into the equation <span class="math inline">\(\mathcal{C}^{-1/2} \eta = W\)</span> to get the equation <span class="math display">\[
\sum_{k,j=1}^n u_j f_k \int_T \psi_k(s) \mathcal{C}^{-1/2} \psi_j(s)\,ds \sim N\left(0, \sum_{j,k=1}^n \psi_j(s)\psi_k(s)\,ds\right)
\]</span></p></li>
</ol>
<p>As this has to be true for <em>every</em> vector <span class="math inline">\(f\)</span>, this is equivalent to the linear system <span class="math display">\[
K u \sim N(0, C),
\]</span> where <span class="math inline">\(K_{kj} = \int_T \psi_k(s) \mathcal{C}^{-1/2} \psi_j(s)\,ds\)</span> and <span class="math inline">\(C_{kj} = \sum_{j,k=1}^n \psi_j(s)\psi_k(s)\)</span>.</p>
<p>Obviously this method is only going to be useful if it’s possible to compute the elements of <span class="math inline">\(K\)</span> and <span class="math inline">\(C\)</span> efficiently. In the special case where <span class="math inline">\(\mathcal{C}^{-1/2}\)</span> is a differential operator<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> and the basis functions are chosen to have compact support<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, these calculations form the basis of the finite element method for solving partial differential equations.</p>
<p>The most important thing, however, is that if <span class="math inline">\(\mathcal{C}^{-1/2}\)</span> is a differential operator <em>and</em> the basis functions have compact support, the matrix <span class="math inline">\(K\)</span> is sparse and the matrix <span class="math inline">\(C\)</span> can be made<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> diagonal, which means that <span class="math display">\[
u \sim N(0, K^{-1} C K^{-T})
\]</span> has a sparse precision matrix. This can be used to make inference with these GPs very efficient and is the basis for GPs in the <a href="http://r-inla.org">INLA software</a>.</p>
<p>A natural question to ask is <em>when will we end up with a sparse precision matrix</em>? The answer is not quite when <span class="math inline">\(\mathcal{C}^{-1/2}\)</span> is a differential operator. Although that will lead to a sparse precision matrix (and a Markov process), it is not required. So the purpose of the rest of this post is to quantify all of the cases where a GP has the Markov property and we can make use of the resulting computational savings.</p>
</section>
</section>
<section id="the-markov-property-for-on-abstract-spaces" class="level2">
<h2 class="anchored" data-anchor-id="the-markov-property-for-on-abstract-spaces">The Markov property for on abstract spaces</h2>
<p>Part of the reason why I introduced the notion of a generalised Gaussian process is that it is useful in the definition of the Markov process. Intuitively, we know what this definition is going to be: if I split my space into three disjoint sets <span class="math inline">\(A\)</span>, <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(B\)</span> in such a way that you can’t get from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> without passing through <span class="math inline">\(\Gamma\)</span>, then the Markov property should say, roughly, that every random variable <span class="math inline">\(\{x(s): s\in A\}\)</span> is conditionally independent of every random variable <span class="math inline">\(\{x(s): s \in B\}\)</span> <em>given</em> (or conditional on) knowing the values of the entire set <span class="math inline">\(\{x(s): s \in \Gamma\}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="markov.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A graphical illustration of the three sets used above Markov property.</figcaption><p></p>
</figure>
</div>
<p>That definition is all well and good for a hand-wavey approach, but unfortunately it doesn’t quite hold up to mathematics. In particular, if we try to make <span class="math inline">\(\Gamma\)</span> a line<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>, we will hit a few problems. So instead let’s do this properly.</p>
<p>All of the material here is covered in Rozanov’s excellent but unimaginatively named book <em>Markov Random Fields</em>.</p>
<p>To set us up. we should consider the types of sets we have. There are three main sets that we are going to be using: the open<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> set <span class="math inline">\(S_1 \subset T\)</span>, its boundary<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> <span class="math inline">\(\Gamma \supseteq \partial S\)</span>. For example, if <span class="math inline">\(T = \mathbb{R}^2\)</span> and <span class="math inline">\(S\)</span> is the interior of the unit circle, and its open complement <span class="math inline">\(S_2 = S_1^C \backslash \partial S_1\)</span>. For a 2D example, if <span class="math inline">\(S_1\)</span> is the <em>interior</em> of the unit circle, then <span class="math inline">\(\Gamma\)</span> could be the unit circle, and <span class="math inline">\(S_2\)</span> would be the _exterior of the unit circle.</p>
<p>One problem with these sets, is that while <span class="math inline">\(S_1\)</span> will be a 2D set, <span class="math inline">\(\Gamma\)</span> is only one dimensional (it’s a circle, so it’s a line!). This causes some troubles mathematically, which we need to get around by using the <span class="math inline">\(\epsilon\)</span> fattening of <span class="math inline">\(\Gamma\)</span>, which is the set <span class="math display">\[
\Gamma^\epsilon = \{s \in T : d(s, \Gamma) &lt; \epsilon\},
\]</span> where <span class="math inline">\(d(s, \Gamma)\)</span> is the distance from <span class="math inline">\(s\)</span> to the nearest point in <span class="math inline">\(\Gamma\)</span>.</p>
<p>With all of this in hand we can now give a general definition of the Markov property.</p>
<div id="def-markov" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (The Markov property for a generalised Gaussian process) </strong></span>Consider a zero mean generalised GP<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> <span class="math inline">\(\xi\)</span>. For any<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> subset <span class="math inline">\(A \subset T\)</span>, we define the collection of random variables<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> <span class="math display">\[
H(A) = \operatorname{span}\{\xi(f): \operatorname{supp}(f) \subseteq A\}.
\]</span> We will call <span class="math inline">\(\{H(A); A \subseteq T\}\)</span> the <em>random field</em><a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> associated with <span class="math inline">\(\xi\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{G}\)</span> be a system of domains<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> in <span class="math inline">\(T\)</span>. We say that <span class="math inline">\(\xi\)</span> has the Markov<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> property (with respect to <span class="math inline">\(\mathcal{G}\)</span>) if, for all <span class="math inline">\(S_1 \in \mathcal{G}\)</span> and for any sufficiently small <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math display">\[
\mathbb{E}(xy \mid H(\Gamma^\epsilon)) = 0, \qquad x \in H(S_1), y \in H(S_2),
\]</span> where <span class="math inline">\(\Gamma = \partial S_1\)</span> and <span class="math inline">\(S_2 = S_1^C \backslash \Gamma\)</span>.</p>
</div>
<section id="rewriting-the-markov-property-i-splitting-spaces" class="level3">
<h3 class="anchored" data-anchor-id="rewriting-the-markov-property-i-splitting-spaces">Rewriting the Markov property I: Splitting spaces</h3>
<p>The Markov property defined above is great and everything, but in order to manipulate it, we need to think carefully about the how the domains <span class="math inline">\(S_1\)</span>, <span class="math inline">\(\Gamma^\epsilon\)</span> and <span class="math inline">\(S_2\)</span> can be used to divide up the space <span class="math inline">\(H(T)\)</span>. To do this, we need to basically localise the Markov property to one set of <span class="math inline">\(S_1\)</span>, <span class="math inline">\(\Gamma\)</span>, <span class="math inline">\(S_2\)</span>. This concept is called a <em>splitting</em><a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> of <span class="math inline">\(H(S_1)\)</span> and <span class="math inline">\(H(S_2)\)</span> by <span class="math inline">\(H(\Gamma^\epsilon)\)</span></p>
<div id="def-splitting" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 </strong></span>For some domain <span class="math inline">\(S_1\)</span> and <span class="math inline">\(\Gamma \supseteq \partial S_1\)</span>, set <span class="math inline">\(S_2 = (S_1 \cup \Gamma)^c\)</span>. The space <span class="math inline">\(H(\Gamma^\epsilon)\)</span> splits <span class="math inline">\(H(S_1)\)</span> and <span class="math inline">\(H(S_2)\)</span> if <span class="math display">\[
H(T) = H(S_1 \ominus \Gamma^\epsilon) \oplus H(\Gamma^\epsilon) \oplus H(S_2 \ominus \Gamma^\epsilon),
\]</span> where <span class="math inline">\(\oplus\)</span> is the sum of orthogonal components<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> and <span class="math inline">\(x\in H(S \ominus \Gamma^\epsilon)\)</span> if and only if there is some <span class="math inline">\(y \in H(S)\)</span> such that<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> <span class="math display">\[
x = y - \mathbb{E}(y \mid H(\Gamma^\epsilon)).
\]</span></p>
</div>
<p>This emphasizes that we can split our space into three separate components: inside <span class="math inline">\(S_1\)</span>, outside <span class="math inline">\(S_1\)</span> and on the boundary of <span class="math inline">\(S_1\)</span> and the ability to do that for any<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> domain is the key part of the Markov<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> property.</p>
<p>A slightly more convenient way to deal with spiting spaces is the case where the we have overlapping sets <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> that cover the domain (ie <span class="math inline">\(A \cup B = T\)</span>) and the splitting set is their intersection <span class="math inline">\(S = A \cap B\)</span>. In this case, the splitting equation becomes <span class="math display">\[
H(A)^\perp \perp H(B)^\perp.
\]</span> I shan’t lie: that looks wild. But it makes sense when you take <span class="math inline">\(A = S_1 \cup \Gamma^\epsilon\)</span> and <span class="math inline">\(B = S_2 \cup \Gamma^\epsilon\)</span>, in which case <span class="math inline">\(H(A)^\perp = H(S_2)\)</span> and <span class="math inline">\(H(B)^\perp = H(S_1)\)</span>.</p>
<p>The final thing to add before we can get to business is a way to get rid of all of the annoying <span class="math inline">\(\epsilon\)</span>s. The idea is to take the intersection of all of the <span class="math inline">\(H(\Gamma^\epsilon)\)</span> as the splitting space. If we define <span class="math display">\[
H_+(\Gamma) = \bigcap_{\epsilon&gt;0} H(\Gamma^\epsilon)
\]</span> we can re-write<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> the splitting equation as <span class="math display">\[\begin{align*}
&amp;H_+(\Gamma) = H_+(S_1 \cup \Gamma) \cap H_+(S_1 \cup \Gamma) \\
&amp; H_+(S_1 \cup \Gamma)^\perp \perp H_+(S_2 \cup \Gamma)^\perp.
\end{align*}\]</span></p>
<p>This gives the following statement of the Markov property.</p>
<div id="def-markov2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 </strong></span>Let <span class="math inline">\(\mathcal{G}\)</span> be a system of domains<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> in <span class="math inline">\(T\)</span>. We say that <span class="math inline">\(\xi\)</span> has the Markov property (with respect to <span class="math inline">\(\mathcal{G}\)</span>) if, for all <span class="math inline">\(S_1 \in \mathcal{G}\)</span>, <span class="math inline">\(\Gamma\supseteq \partial S_1\)</span> ,<span class="math inline">\(S_2 = S_1^C \backslash \Gamma\)</span>, we have, for some <span class="math inline">\(\epsilon &gt; 0\)</span> <span class="math display">\[
H_+(\Gamma^\epsilon) = H_+(S_1 \cup \Gamma^\epsilon) \cap H_+(S_1 \cup \Gamma^\epsilon)
\]</span> and <span class="math display">\[
H_+(S_1 \cup \Gamma)^\perp \perp H_+(S_2 \cup \Gamma)^\perp.
\]</span></p>
</div>
</section>
<section id="rewriting-the-markov-property-ii-the-dual-random-field-ha" class="level3">
<h3 class="anchored" data-anchor-id="rewriting-the-markov-property-ii-the-dual-random-field-ha">Rewriting the Markov property II: The dual random field <span class="math inline">\(H^*(A)\)</span></h3>
<p>We are going to fall further down the abstraction rabbit hole in the hope of ending up somewhere useful. In this case, we are going to invent an object that has no reason to exist and we will show that it can be used to compactly restate the Markov property. It will turn out in the next section that it is actually a useful characterization that will lead (finally) to an operational characterisation of a Markovian Gaussian process.</p>
<div id="def-dual-field" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (Dual random field) </strong></span>Let <span class="math inline">\(\xi\)</span> be a generalised Gaussian process with an associated random field <span class="math inline">\(H(A)\)</span>, <span class="math inline">\(A \subseteq T\)</span> and let <span class="math inline">\(\mathcal{G}\)</span> be a complete system of open domains in <span class="math inline">\(T\)</span>. The <em>dual</em> to the random field <span class="math inline">\(H(A)\)</span>, <span class="math inline">\(A \subseteq T\)</span> on the system <span class="math inline">\(\mathcal{G}\)</span> is the random field <span class="math inline">\(H^*(A)\)</span>, <span class="math inline">\(A \subseteq T\)</span> that satisfies <span class="math display">\[
H^*(T) = H(T)
\]</span> and <span class="math display">\[
H^*(A) = H_+(A^c)^\perp, \qquad A \in \mathcal{G}.
\]</span></p>
</div>
<p>This definition looks frankly a bit wild, but I promise you, we will use it.</p>
<p>The reason for its structure is that it directly relates to the Markov property. In particular, the existence of a dual field implies that, if we have any <span class="math inline">\(S_1 \in \mathcal{G}\)</span>, then <span class="math display">\[\begin{align*}
H_+(S_1 \cup \bar{\Gamma^\epsilon}) \cap H_+(S_1 \cup \bar{\Gamma^\epsilon}) &amp;= H^*((S_1 \cup \bar{\Gamma^\epsilon})^c)^\perp \cap H^*((S_2 \cup \bar{\Gamma^\epsilon})^c)^\perp \\
H^*((S_1 \cup \bar{\Gamma^\epsilon})^c \cup (S_2 \cup \bar{\Gamma^\epsilon})^c) \\
&amp;= H_+((S_1 \cup \bar{\Gamma^\epsilon}) \cap (S_2 \cup \bar{\Gamma^\epsilon})) \\
&amp;= H_+(\Gamma^\epsilon).
\end{align*}\]</span> That’s the first thing we need to show to demonstrate the Markov property.</p>
<p>The second part is much easier. If we note that <span class="math inline">\((S_2 \cup \Gamma)^c = S_1 \backslash \Gamma\)</span>, it follows that <span class="math display">\[
H_+(S_1 \cup \Gamma)^\perp = H^*(S_2 \backslash \Gamma).
\]</span></p>
<p>This gives us our third (and final) characterisation of the (second-order) Markov property.</p>
<div id="def-markov3" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8 </strong></span>Let <span class="math inline">\(\mathcal{G}\)</span> be a system of domains<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> in <span class="math inline">\(T\)</span>. Assume that the random field <span class="math inline">\(H(\cdot)\)</span> has an associated dual random field <span class="math inline">\(H^*(\cdot)\)</span>.</p>
<p>We say that <span class="math inline">\(H(A)\)</span>, <span class="math inline">\(A \in \mathcal{G}\)</span> has the Markov property (with respect to<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> <span class="math inline">\(\mathcal{G}\)</span>) if and only if for all <span class="math inline">\(S_1 \in \mathcal{G}\)</span>, <span class="math display">\[
H^*(S_1 \backslash \Gamma) \perp H^*(S_2 \backslash \Gamma).
\]</span> When this holds, we say that the dual field is <em>orthogonal</em> with respect to <span class="math inline">\(\mathcal{G}\)</span>.</p>
</div>
<p>There is probably more to say about dual fields. For instance, the dual of the dual field is the original field. Neat, huh. But really, all we need to do is know that an orthogonal dual field implies a the Markov property. Because next we are going to construct a dual field, which will give us an actually useful characterisation of Markovian GPs.</p>
</section>
<section id="building-out-our-toolset-with-the-conjugate-gp" class="level3">
<h3 class="anchored" data-anchor-id="building-out-our-toolset-with-the-conjugate-gp">Building out our toolset with the conjugate GP</h3>
<p>In this section, our job is to construct a dual random field. To do this, we are going to exploit the notion of a <em>conjugate<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> Gaussian process</em>, which is a generalised<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> GP <span class="math inline">\(\xi^*\)</span> such that<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a> <span class="math display">\[
\mathbb{E}(\xi(f)\xi^*(g)) = \int_T f(s)g(s)\,ds.
\]</span> It is going to turn out that <span class="math inline">\(H^*(\cdot)\)</span> is the random field generated by <span class="math inline">\(\xi^*\)</span>. The condition that <span class="math inline">\(H(T) = H^*(T)\)</span> can be assumed <em>a fortiori</em>. What we need to show is that the existence of a conjugate Gaussian process implies that, for all <span class="math inline">\(S \subset \mathcal{G}\)</span>, <span class="math inline">\(H^*(S) \perp H^*( S^C)\)</span>.</p>
<p>We will return to the issue of whether or not <span class="math inline">\(\xi^*\)</span> actually exists later, but assuming it does let’s see how it’s associated random field <span class="math inline">\(H*(S)\)</span> relates to <span class="math inline">\(H_+(S^c)^\perp\)</span> for <span class="math inline">\(S\in \mathcal{G}\)</span>. While it is not always true that these things are equal, it <em>is</em> always true that <span class="math display">\[
H^*(S) \subseteq H_+(S^c)^\perp.
\]</span> We will consider when equality holds in the next section. But first let’s show the inclusion.</p>
<p>The space <span class="math inline">\(H^*(S)\)</span> contains all random variables of the form <span class="math inline">\(\xi^*(u)\)</span>, where the support of <span class="math inline">\(u\)</span> is compact in <span class="math inline">\(S\)</span>, which means that it is a positive distance from <span class="math inline">\(S^C\)</span>. That means that, for some <span class="math inline">\(\epsilon &gt; 0\)</span>, the support of <span class="math inline">\(u\)</span> is outside<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a> of <span class="math inline">\((S^c)^\epsilon\)</span>. So if we fix that <span class="math inline">\(u\)</span> and consider any smooth <span class="math inline">\(v\)</span> with support in<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a> <span class="math inline">\((S^c)^\epsilon\)</span>, then, from the definition of the conjugate GP, we have<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a> <span class="math display">\[
\mathbb{E}(\xi(v)\xi^*(u)) = \int_T u(s) v(s)\, ds = 0.
\]</span> This means that <span class="math inline">\(\xi^*(u)\)</span> is perpendicularity to <span class="math inline">\(\xi(v)\)</span> and, therefore, <span class="math inline">\(\xi^*(u) \in H((S^c)^\epsilon)^\perp\)</span>. Now, <span class="math inline">\(H_+(S^c)\)</span> is defined as the intersection of these spaces, but it turns out that<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a> for any spaces <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, <span class="math display">\[
(A \cap B)^\perp = A^\perp \cup B^\perp.
\]</span> This is because <span class="math inline">\(A\cap B \subset A\)</span> and so every function that’s orthogonal to functions in <span class="math inline">\(A\)</span> is also orthogonal to functions in <span class="math inline">\(A\cap B\)</span>. The same goes for <span class="math inline">\(B\)</span>. We have shown that <span class="math display">\[
H_+(S^c) = \bigcup_{\epsilon &gt; 0} H((S^c)^\epsilon)^\perp
\]</span> and every <span class="math inline">\(\eta^* \in H^*(S)\)</span> is in <span class="math inline">\(H((S^c)^\epsilon)^\perp\)</span> for some <span class="math inline">\(\epsilon &gt;0\)</span>. This gives the inclusion <span class="math display">\[
H^*(S) \subseteq H_+(S^c)^\perp.
\]</span></p>
<p>To give conditions for when it’s an actual equality is a bit more difficult. It, maybe surprisingly, involves thinking carefully about the reproducing kernel Hilbert space of <span class="math inline">\(\xi\)</span>. We are going to take this journey together in two steps. First we will give a condition on the RKHS that guarantees that <span class="math inline">\(\xi^*\)</span> exists. Then we will look at when <span class="math inline">\(H^*(S) = H_+(S^c)^\perp\)</span>.</p>
</section>
<section id="when-does-xi-exits-or-a-surprising-time-with-the-reproducing-kernel-hilbert-space" class="level3">
<h3 class="anchored" data-anchor-id="when-does-xi-exits-or-a-surprising-time-with-the-reproducing-kernel-hilbert-space">When does <span class="math inline">\(\xi^*\)</span> exits? or, A surprising time with the reproducing kernel Hilbert space</h3>
<p>First off, though, we need to make sure that <span class="math inline">\(\xi^*\)</span> exists. Obviously<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> if it exists then it is unique and <span class="math inline">\(\xi^{**} = \xi\)</span>.</p>
<p>But does it exist? The answer turns out to be <em>sometimes</em>. But also <em>usually</em>. To show this, we need to do something that is, frankly, just a little bit fancy. We need to deal with the reproducing kernel Hilbert space<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a>. This feels somewhat surprising, but it turns out that it is a fundamental object<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a> and intrinsically tied to the space <span class="math inline">\(H(T)\)</span>.</p>
<p>The reproducing kernel space, which we will now<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a> call <span class="math inline">\(V(T)\)</span> because we are using <span class="math inline">\(H\)</span> for something else in this section, is a set of deterministic generalised functions <span class="math inline">\(\psi\)</span>, that can be evaluated at <span class="math inline">\(C_0^\infty(T)\)</span> functions<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a> as <span class="math display">\[
\psi(u) = \int_T u(s)\,d\psi(s), \qquad u \in C_0^\infty(T).
\]</span> A generalised function <span class="math inline">\(\psi \in V(T)\)</span> if there is a corresponding random variable in <span class="math inline">\(\eta \in H(T)\)</span> that satisfies <span class="math display">\[
\psi(u) = \mathbb{E}\left[\xi(u) \eta\right], \qquad u \in C_0^\infty(T).
\]</span> It can be shown<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a> that there is a one-to-one correspondence between <span class="math inline">\(H(T)\)</span> and <span class="math inline">\(V(T)\)</span>, in the sense that for every <span class="math inline">\(\psi\)</span> there is a unique <span class="math inline">\(\eta = \eta(\psi) \in H(T)\)</span>.</p>
<p>We can use this correspondence to endow <span class="math inline">\(V(T)\)</span> with an inner product <span class="math display">\[
\langle \psi_1, \psi_2\rangle_{V(T)} = \mathbb{E}(\eta(\psi_1), \eta(\psi_2)).
\]</span></p>
<p>So far, so abstract. The point of the conjugate GP is that it gives us an explicit construction of the<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a> mapping <span class="math inline">\(\eta\)</span>. And, importantly for the discussion of existence, if there is a conjugate GP then the RKHS has a particular relationship with <span class="math inline">\(C_0^\infty(T)\)</span>.</p>
<p>To see this, let’s assume <span class="math inline">\(\xi^*\)</span> exists. Then, for each <span class="math inline">\(v \in C_0^\infty(T)\)</span>, the generalised function <span class="math display">\[
\psi_v(u) = \int_T u(s) v(s)\,ds
\]</span> is in <span class="math inline">\(V(T)\)</span> because, by the definition of <span class="math inline">\(\xi^*\)</span> we have that <span class="math display">\[
\phi_v(u) = \mathbb{E}(\xi(u)\xi^*(v)) = \int_T u(s) v(s)\,ds.
\]</span> Hence, the embedding is given by <span class="math inline">\(\eta(v) = \xi^*(v)\)</span>.</p>
<p>Now, if we do a bit of mathematical trickery and equate things that are isomorphic, <span class="math inline">\(C_0^\infty(T) \subseteq V(T)\)</span>. On its face, that doesn’t make much sense because on the left we have a space of actual functions and on the right we have a space of generalised functions. To make it work, we associate each smooth function <span class="math inline">\(v\)</span> with the generalised function <span class="math inline">\(\psi_v\)</span> defined above.</p>
<p>This make <span class="math inline">\(V(T)\)</span> the closure<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a> of <span class="math inline">\(C_0^\infty(T)\)</span> under the norm <span class="math display">\[
\|v\|^2_{V(T)} = \mathbb{E}\left(\xi^*(v)^2\right).
\]</span> and hence we have showed that if there is a conjugate GP, then <span class="math display">\[
C_0^\infty(T) \subseteq V(T), \qquad \overline{C_0^\infty(T)} = V(T).
\]</span> It turns out that if <span class="math inline">\(C_0^\infty(T)\)</span> is dense in <span class="math inline">\(V(T)\)</span> then that implies that there exists a conjugate function defined through the isomorphism <span class="math inline">\(\eta(\cdot)\)</span>. This is because <span class="math inline">\(H(T) = \eta(V(T))\)</span> and <span class="math inline">\(\eta\)</span> is continuous. Hence if we choose <span class="math inline">\(\xi^*(v) = \eta(v)\)</span> then <span class="math inline">\(H^*(T) = H(T)\)</span>.</p>
<p>We have shown the following.</p>
<div id="thm-conjugate-exist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 </strong></span>A conjugate GP exists if and only if <span class="math inline">\(C_0^\infty(T)\)</span> is dense in <span class="math inline">\(V(T)\)</span>.</p>
</div>
<p>This is our first step towards making statements about the stochastic process <span class="math inline">\(\xi\)</span> into statements about the RKHS. We shall continue along this road.</p>
<p>You might, at this point, be wondering if that condition ever actually holds. The answer is yes. It does fairly often. For instance, if <span class="math inline">\(\xi\)</span> is a <a href="https://dansblog.netlify.app/posts/2022-09-07-priors5/priors5.html#part-2-an-invitation-to-the-theory-of-stationary-gaussian-processes">stationary GP</a> with spectral density <span class="math inline">\(f(\omega)\)</span>, the biorothogonal function exists if and only if there is some <span class="math inline">\(k&gt;0\)</span> such that <span class="math display">\[
\int (1 + |\omega|^2)^{-k}f(\omega)^{-1}\,d\omega &lt; \infty.
\]</span> This basically says that the theory we are developing doesn’t work for GPs with extremely smooth sample paths (like a GP with the square-exponential covariance function). This is not a restriction that bothers me at all.</p>
<p>For non-stationary GPs that aren’t too smooth, this will also hold as long as nothing too bizarre is happening at infinity.</p>
</section>
<section id="but-when-does-hs-h_scperp" class="level3">
<h3 class="anchored" data-anchor-id="but-when-does-hs-h_scperp">But when does <span class="math inline">\(H^*(S) = H_+(S^c)^\perp\)</span>?</h3>
<p>We have shown already<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a> that <span class="math display">\[
H((S^c)^\epsilon)^\perp = \left\{\xi^*(u): u \in V(T),\, \operatorname{supp}(u) \subseteq [(S^c)^\epsilon]^c\right\}
\]</span> (that last bit with all the complements can be read as “the support of <span class="math inline">\(u\)</span> is inside <span class="math inline">\(S\)</span> and always more than <span class="math inline">\(\epsilon\)</span> from the boundary.”). It follows then that <span class="math display">\[
H_+(S^c)^\perp = \bigcup_{\epsilon&gt;0}\left\{\xi^*(u):  u \in V(T),\, \operatorname{supp}(u) \subseteq [(S^c)^\epsilon]^c\right\}.
\]</span> This is nice because it shows that <span class="math inline">\(H_+(S^c)^\perp\)</span> is related to the space <span class="math display">\[
V(S) = \bigcup_{\epsilon&gt;0}\left\{  u \in V(T),\, \operatorname{supp}(u) \subseteq [(S^c)^\epsilon]^c\right\},
\]</span> that is if <span class="math inline">\(v\in V(T)\)</span> is a function that is the limit of a sequence of functions <span class="math inline">\(v_n \in V(T)\)</span> with <span class="math inline">\(\operatorname{supp}(v_n) = [(S^c)^\epsilon]^c\)</span> for some <span class="math inline">\(\epsilon&gt;0\)</span>, then <span class="math inline">\(\xi^*(v) \in H_+(S^c)^\perp\)</span> and <em>every</em> such random variable has an associated <span class="math inline">\(v\)</span>.</p>
<p>So, in the sense<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a> of isomorphisms these are equivalent, that is <span class="math display">\[
H_+(S^c)^\perp \cong V(S).
\]</span></p>
<p>This means that if we can show that <span class="math inline">\(H^*(S) \cong V(S)\)</span>, then we have two spaces that are isomorphic to the same space <em>and</em> use the same isomorphism <span class="math inline">\(\xi^*\)</span>. This would mean that the spaces are equivalent.</p>
<p>This can also be placed in the language of function spaces. Recall that <span class="math display">\[
H^*(S) = \overline\{\xi(u): u \in C_0^\infty(S)\}.
\]</span> Hence <span class="math inline">\(H^*(S)\)</span> will be isomorphic to <span class="math inline">\(V(S)\)</span> if and only if <span class="math display">\[
V(S) = \overline{C_0^\infty(S)},
\]</span> that is, if and only if every <span class="math inline">\(v \in V(S)\)</span> is the limit of a sequence of smooth functions compactly supported within <span class="math inline">\(S\)</span>.</p>
<p>This turns out to not <em>always</em> be true, but it’s true in the situations that we most care about. In particular, we get the following theorem, which I am certainly not going to prove.</p>
<div class="{thm-conjugate-dual}">
<p>Assume that the conjugate GP <span class="math inline">\(\xi^*\)</span> exists. Assume that <em>either</em> of the following holds:</p>
<ol type="1">
<li><p>Multiplication by a function <span class="math inline">\(w \in C_0^\infty\)</span> is bounded in <span class="math inline">\(V(T)\)</span>, ie <span class="math display">\[
\|wu \|_{V(T)} \leq C(w) \|u\|_{V(T)}, \qquad u \in C_0^\infty (T).
\]</span></p></li>
<li><p>The shift operator is bounded under both the RKHS norm and the covariance<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a> norm for small <span class="math inline">\(s_0\)</span>, ie <span class="math display">\[
\|u(\cdot - s_0)\| \leq C \|u\|, \qquad u \in C_0^\infty(T)
\]</span> holds in both norms for all <span class="math inline">\(s_0 \leq s_\max\)</span>, <span class="math inline">\(\s_\max &gt;0\)</span> sufficiently small.</p></li>
</ol>
<p>Then <span class="math inline">\(H^*(\cdot)\)</span> is the dual of <span class="math inline">\(H(\cdot)\)</span> over the system of sets that are bounded or have bounded complements in <span class="math inline">\(T\)</span>.</p>
</div>
<p>The second condition is particularly important because it <em>always</em> holds for stationary GPs with <span class="math inline">\(C=1\)</span> as their covariance structure is shift invariant. It’s not impossible to come up with examples of generalised GPs that don’t satisfy this condition, but they’re all a bit weird (eg the “derivative” of white noise). So as long as your GP is not too weird, you should be fine.</p>
</section>
<section id="at-long-last-an-rkhs-characterisation-of-the-markov-property" class="level3">
<h3 class="anchored" data-anchor-id="at-long-last-an-rkhs-characterisation-of-the-markov-property">At long last, an RKHS characterisation of the Markov property</h3>
<p>And with that, we are finally here! We have that <span class="math inline">\(H^*(S)\)</span> is the dual random field to <span class="math inline">\(H(S)\)</span>, <span class="math inline">\(S\in G\)</span> <em>and</em> we have a lovely characterisation of <span class="math inline">\(H^*(S)\)</span> in terms of the RKHS <span class="math inline">\(V(S)\)</span>. We can combine this with our definition of a Markov property for GPs with a dual random field and get that a GP <span class="math inline">\(\xi\)</span> is Markovian if and only if <span class="math display">\[
H^*(S_1 \backslash \Gamma) \perp H^*(S_2 \backslash \Gamma).
\]</span> We can use the isomorphism to say that if <span class="math inline">\(\eta_j \in H^*(S_j \backslash \Gamma)\)</span>, <span class="math inline">\(j=1,2\)</span>, then there is a <span class="math inline">\(v_j \in V(S_j \backslash \Gamma)\)</span> such that <span class="math display">\[
\eta_j = \xi^*(v_j).
\]</span> Moreover, this isomorphism is unitary (aka it preserves the inner product) and so <span class="math display">\[
\mathbb{E}(\eta_1 \eta_2) = \langle v_1, v_2\rangle_{V(T)}.
\]</span> Hence, <span class="math inline">\(\xi\)</span> has the Markov property if and only if <span class="math display">\[
\langle v_1, v_2\rangle_{V(T)} = 0, \qquad v_j \in V(S_j \backslash \Gamma),\,S_1 \in \mathcal{G},\, S_2 = S_1^c,\, j=1,2.
\]</span></p>
<p>Let’s memorialise this as a theorem.</p>
<div id="thm-markov-rkhs" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 </strong></span>A GP <span class="math inline">\(\xi\)</span> with a conjugate GP <span class="math inline">\(\xi^*\)</span> is Markov if and only if its RKHS is local, ie if <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> have disjoint supports, then <span class="math display">\[
\langle v_1, v_2\rangle_{V(T)} = 0.
\]</span></p>
</div>
<p>This result is <em>particularly</em> nice because it entirely characterises the RHKS inner product of a Markovian GP. The reason for this is a deep result from functional analysis called Peetre’s Theorem, which states, in our context, that locality implies that the inner product has the form <span class="math display">\[
\langle v_1, v_2\rangle_{V(T)} = \sum_{\mathbf{k}, \mathbf{j}} \int_T a_{\mathbf{k}\mathbf{j}}(s)\frac{\partial^{|\mathbf{k}|}u}{\partial s_\mathbf{k}} \frac{\partial^{|\mathbf{j}|}u}{\partial s_\mathbf{j}}\,ds,
\]</span> where<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a> <span class="math inline">\(a_{\mathbf{k}\mathbf{j}}(s)\)</span> are integrable functions and only a finite number of them are non-zero at any point <span class="math inline">\(s\)</span>.</p>
<p>This connection between the RKHS and the dual space also gives the following result for stationary GPs.</p>
<div id="thm-stationary-gp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 </strong></span>Let <span class="math inline">\(\xi\)</span> be a stationary Gaussian process. Then <span class="math inline">\(\xi\)</span> has the Markov property if and only if its spectral density is the inverse of a non-negative, symmetric polynomial.</p>
</div>
<p>This follows from the characterisation of the RKHS as having the inner product as <span class="math display">\[
\langle v_1, v_2\rangle_{V(T)} = \int_T \hat{v_1}(\omega) \hat{v_2}(\omega) f(\omega)^{-1}\,d\omega,
\]</span> where <span class="math inline">\(\hat{v_1}\)</span> is the Fourier transform of <span class="math inline">\(v_1\)</span> and the fact that a differential operator can is transformed to a polynomial in Fourier space.</p>
</section>
<section id="putting-this-all-in-terms-of-eta" class="level3">
<h3 class="anchored" data-anchor-id="putting-this-all-in-terms-of-eta">Putting this all in terms of <span class="math inline">\(\eta\)</span></h3>
<p><em>Waaaay</em> back near the top of the post I described a way to write a (generalised) GP in terms of its covariance operator and the white noise process <span class="math display">\[
\eta = \mathcal{C}^{1/2}W.
\]</span> From the discussions above, it follows that the corresponding conjugate GP is given by <span class="math display">\[
\eta^* = C^{-1/2}W.
\]</span> This means that the RKHS inner product is given by <span class="math display">\[\begin{align*}
\langle v_1, v_2 \rangle_{V(T)} = \mathbb{E}(\eta^*(v_1)\eta^*(v_2))\\
&amp;= \mathbb{E}\left[\int_T \mathcal{C}^{-1/2}v_1(s)\,dW(s)\int_T \mathcal{C}^{-1/2}v_2(s)\,dW(s)\right] \\
&amp;= \int_T v_1(s)\mathcal{C}^{-1}v_2(s)\,ds
\end{align*}\]</span> From the discussion above, if <span class="math inline">\(\eta\)</span> is Markovian, then <span class="math inline">\(\mathcal{C}^{-1}\)</span> is<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a> a differential<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a> operator.</p>
</section>
</section>
<section id="using-the-rkhs-to-build-computationally-efficient-approximations-to-markovian-gps" class="level2">
<h2 class="anchored" data-anchor-id="using-the-rkhs-to-build-computationally-efficient-approximations-to-markovian-gps">Using the RKHS to build computationally efficient approximations to Markovian GPs</h2>
<p>To close out this post, let’s look at how we can use the RKHS to build an approximation to a Markovian GP. This is equivalent<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a> to the SPDE method that was very briefly sketched above, but it only requires knowledge of the RKHS inner product.</p>
<p>In particular, if we have a set of basis functions <span class="math inline">\(\psi_j\)</span>, <span class="math inline">\(j=1,\ldots,n\)</span>, we can define the approximate RKHS <span class="math inline">\(V_n(T)\)</span> as the space of all functions <span class="math display">\[
f(s) = \sum_{j=1}^n f_j \psi_j(s)
\]</span> equipped with the inner product <span class="math display">\[
\langle f, g \rangle_{V_n(T)} = f^T Q g,
\]</span> where the LHS <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are functions and on the right they are the vectors of weights, and <span class="math display">\[
Q_{ij} = \langle \psi_i, \psi_j\rangle_{V(T)}.
\]</span></p>
<p>For a finite dimensional GP, the matrix that defines the RKHS inner product is<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a> the inverse of the covariance matrix. Hence the finite dimensional GP <span class="math inline">\(u^{(n)}(\cdot)\)</span> associated with the RKHS <span class="math inline">\(V_n(T)\)</span> is the random function <span class="math display">\[
u^{(n)}(s) = \sum_{j = 1}^n u_j \psi_j(s),
\]</span> where the weights <span class="math inline">\(u \sim N(0, Q^{-1})\)</span>.</p>
<p>If the GP is Markovian <em>and</em> the basis functions have compact support, then <span class="math inline">\(Q\)</span> is a sparse matrix and maybe he’ll love me again.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Step 1: Open Rasmussen and Williams.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>For example, the process I’m about to describe is not meaningfully different for a process on a sphere. Whereas if you want to use a covariance function on a sphere you are stuck trying to find a whole new class of positive definite functions. It’s frankly very annoying. Although if you want to build a career out of characterising positive definite functions on increasingly exotic spaces, you probably don’t find it annoying.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Or the Cholesky factor if you add a bunch of transposes in the right places, but let’s not kid ourselves this is not a practical discussion of how to do it<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>For any subset <span class="math inline">\(B\)</span>, <span class="math inline">\(\sup_{s\in B} w(s) = \infty\)</span> <em>and</em> <span class="math inline">\(\inf_{s \in B} w(s) = -\infty\)</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Countably additive set-valued function taking any value in <span class="math inline">\(\mathbb{C}\)</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>measurable<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="math inline">\(A \cap B = \emptyset\)</span><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>If <span class="math inline">\(W(A)\)</span> is also Gaussian then this is the same as them being independent<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Recall that <span class="math inline">\(T\)</span> is our whole space. Usually <span class="math inline">\(\mathbb{R}^d\)</span>, but it doesn’t matter here.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>A bit of a let down really.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>like <span class="math inline">\(f(s)\)</span> but with more subsets<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><span class="math inline">\(L^2(T)\)</span> is the space of functions with the property that <span class="math inline">\(\int_T f(s)^2\,ds &lt; \infty\)</span>.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>eg the Gaussian free field in physics, or the de Wijs process.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>You can use a separate set of basis functions here, but I’m focusing on simplicity<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>The standard example is <span class="math display">\[
\mathcal{C}^{-1/2} = \kappa^2 - \sum_{j=1}^d \frac{\partial^2}{\partial s_j^2}.
\]</span><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>In particular piecewise linear tent functions build on a triangulation<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>Read the paper, it’s a further approximation but the error is negligible<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>(<span class="math inline">\(d-1\)</span>)-dimensional sub-manifold<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>This set does not include its boundary<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>This is defined as the set <span class="math inline">\(\partial S_1 = \bar{S_1} \backslash S_1\)</span>, where <span class="math inline">\(\bar{S_1}\)</span> is the closure of <span class="math inline">\(S_1\)</span>. But let’s face it. It’s the fucking boundary. It means what you think it means.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>I’m using <span class="math inline">\(\xi\)</span> here as a <em>generic</em> generalised GP, rather than <span class="math inline">\(\eta\)</span>, which is built using an ordinary GP. This doesn’t really make much of a difference (the Markov property for one is the same as the other), but it makes me feel better.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>measurable<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>Here <span class="math inline">\(\operatorname{supp}(f)\)</span> is the support of <span class="math inline">\(f\)</span>, that is the values of <span class="math inline">\(s\)</span> such that <span class="math inline">\(f(s) \neq 0\)</span>.<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>This is the terminology of Rozanov. Random Field is also another term for stochastic process. Why only let words mean one thing?<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>non-empty connected open sets<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>Strictly, this is the <em>weak</em> or <em>second-order</em> Markov property<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>If you’re curious, this is basically the same thing as a splitting <span class="math inline">\(\sigma\)</span>-algebra. But, you know, sans the <span class="math inline">\(\sigma\)</span>-algebra bullshit.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>That is, any <span class="math inline">\(x \in H(T)\)</span> can be written as the sum <span class="math inline">\(x = x_1 + x_2 + x_3\)</span>, where <span class="math inline">\(x_1 \in H(S_1 \ominus \Gamma^\epsilon)\)</span>, <span class="math inline">\(x_2 \in H(\Gamma^\epsilon)\)</span>, and <span class="math inline">\(x_3 \in H(S_2 \ominus \Gamma^\epsilon)\)</span> are <em>mutually orthogonal</em> (ie <span class="math inline">\(\mathbb{E}(x_1x_2) = \mathbb{E}(x_1x_3) = \mathbb{E}(x_2x_3) =0\)</span>!).<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>This is using the idea that the conditional expectation is a projection.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>Typically any open set, or any open connected set, or any open, bounded set. A subtlety that I don’t really want to dwell on is that it is possible to have a GP that is Markov with respect to one system of domains but not another.<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>The Markov property can be restated in this language as for every system of complementary domains and boundary <span class="math inline">\(S_1\)</span>, <span class="math inline">\(\Gamma\)</span>, <span class="math inline">\(S_2\)</span>, there exists a small enough <span class="math inline">\(\epsilon &gt; 0\)</span> such that <span class="math inline">\(\Gamma^\epsilon\)</span> splits <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span><a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p>Technically we are assuming that for small enough <span class="math inline">\(\epsilon\)</span> <span class="math inline">\(H(\Gamma^\epsilon) = \operatorname{span}\left(H(\Gamma^\epsilon \cap S_1) \cup H_+(\Gamma) \cup H(\Gamma^\epsilon \cap S_2)\right)\)</span>. This is not a particularly onerous assumption.<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p>non-empty connected open sets<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p>non-empty connected open sets<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p>The result works with some subsystem <span class="math inline">\(\mathcal{G_0}\)</span>. To prove it for <span class="math inline">\(\mathcal{G}\)</span> it’s enough to prove it for some subset <span class="math inline">\(\mathcal{G}_0\)</span> that separates points of <span class="math inline">\(T\)</span>. This is a wildly technical aside and if it makes no sense to you, that’s very much ok. Frankly I’m impressed you’ve hung in this long.<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p>Rozanov also calls this the <em>biorthogonal</em> GP. I like conjugate more.<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37"><p>Up to this point, it hasn’t been technically necessary for the GP to be generalised. However, here is very much is. It turns out that if realisations of <span class="math inline">\(\xi\)</span> are almost surely continuous, then realisations of <span class="math inline">\(\xi^*\)</span> are almost surely generalised functions.<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38"><p>I’m writing this as if all of these GPs are real valued, but for full generality, we should be dealing with complex GPs. Just imagine I put complex conjugates in all the correct places. I can’t stop you.<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39"><p>That is, inside <span class="math inline">\(S\)</span> and more than <span class="math inline">\(\epsilon\)</span> from the boundary<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40"><p><span class="math inline">\(v\)</span> can be non-zero inside <span class="math inline">\(S\)</span> but only if it’s less than <span class="math inline">\(\epsilon\)</span> away from the boundary.<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41"><p>It’s zero because the two functions are never non-zero at the same time, so their product is zero.<a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42"><p>Here, and probably in a lot of other places, we are taking the union of spaces to be the span of their sum. Sorry.<a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43"><p>Really Daniel. Really. (It’s an isomorphism so if you do enough analysis courses this is obvious. If that’s not clear to you, you should just trust me. Trust issues aren’t sexy. Unless you have cum gutters. In which case, I’ll just spray my isomorphisms on them and you can keep scrolling TikTok.)<a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44"><p>This example is absolutely why I hate that we’ve settled on RKHS as a name for this object because the thing that we are about to construct does not always have a reproducing kernel property. Cameron-Martin space is less confusing. But hey. Whatever. The RKHS for the rest of this section is not always a Hilbert space with a reproducing kernel. We are just going to have to be ok with that.<a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45"><p>Nothing about this analysis relies on Gaussianity. So this is a general characterisation of a Markov property for <em>any</em> stochastic process with second moments.<a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46"><p>In previous blogs, this was denoted <span class="math inline">\(H_c(T)\)</span> and truly it was too confusing when I tried to do it here. And by that point I wasn’t going back and re-naming <span class="math inline">\(H(T)\)</span>.<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47"><p><span class="math inline">\(C_0^\infty(T)\)</span> is the space of all infinitely differentiable compactly supported functions on <span class="math inline">\(T\)</span><a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48"><p>The trick is to notice that the set of all possible <span class="math inline">\(\xi(u)\)</span> is dense in <span class="math inline">\(H(T)\)</span>.<a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49"><p>unitary<a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn50"><p>the space containing the limits (in the <span class="math inline">\(V(T)\)</span>-norm) of all sequences in <span class="math inline">\(v_n \in C_0^\infty(T)\)</span><a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn51"><p>If you take some limits<a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn52"><p>I mean, really. Basically we say that <span class="math inline">\(A \cong B\)</span> if there is an isomorphism between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Could I be more explicit? Yes. Would that make this unreadable? Also yes.<a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn53"><p><span class="math inline">\(\|u\|^2 = \mathbb{E}(\xi(u)^2)\)</span>.<a href="#fnref53" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn54"><p><span class="math inline">\(\mathbf{j} = (j_1, j_2, \ldots)\)</span> is a multi-index, which can be interpreted as <span class="math inline">\(|\mathbf{j}| = \sum_{\ell\geq 1 }j_\ell\)</span>, and <span class="math display">\[
\frac{\partial^{|\mathbf{j}|}u}{\partial s_\mathbf{j}} = \frac{\partial^{|\mathbf{j}|}u}{\partial^{j_1}s_{1}\partial^{j_2}s_{2}\cdots}.
\]</span><a href="#fnref54" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn55"><p>in every local coordinate system<a href="#fnref55" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn56"><p>Because <span class="math inline">\(\mathcal{C}^{-1}\)</span> defines an inner product, it’s actually a symmetric elliptic differential operator<a href="#fnref56" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn57"><p>Technically, you need to choose different basis functions for <span class="math inline">\(f\)</span>. In particular, you need to choose <span class="math inline">\(f = \sum_{j=1}^n f_j \phi_j\)</span> where <span class="math inline">\(\phi_j = \mathcal{C}^{-1/2} \psi_j\)</span>. This is then called a Petrov-Galerkin approximation and truly we don’t need to think about it at all. Also I am completely eliding issues of smoothness in all of this. It maters, but it doesn’t matter too much. So let’s just assume everything exists.<a href="#fnref57" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn58"><p>If you don’t believe me you are welcome to read <a href="https://dansblog.netlify.app/posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html">the monster blog post</a>, where it’s an example.<a href="#fnref58" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2023,
  author = {Dan Simpson},
  editor = {},
  title = {Markovian {Gaussian} Processes: {A} Lot of Theory and Some
    Practical Stuff},
  date = {2023-01-21},
  url = {https://dansblog.netlify.app/posts/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2023" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Dan Simpson. 2023. <span>“Markovian Gaussian Processes: A Lot of Theory
and Some Practical Stuff.”</span> January 21, 2023. <a href="https://dansblog.netlify.app/posts/">https://dansblog.netlify.app/posts/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>