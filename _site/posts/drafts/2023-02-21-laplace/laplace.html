<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.15">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dan Simpson">
<meta name="dcterms.date" content="2023-02-23">
<meta name="description" content="JAX lets you compute a Laplace approximation in three lines of code. Then it ruins your life.">

<title>Un garçon pas comme les autres (Bayes) - Lord won’t you buy me a Laplace approximation; or An unexpected detour into sparity-expoiting autodiff</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Un garçon pas comme les autres (Bayes) - Lord won’t you buy me a Laplace approximation; or An unexpected detour into sparity-expoiting autodiff">
<meta property="og:description" content="JAX lets you compute a Laplace approximation in three lines of code. Then it ruins your life.">
<meta property="og:image" content="https://dansblog.netlify.app/posts/drafts/2023-02-21-laplace/hat.jpg">
<meta property="og:site-name" content="Un garçon pas comme les autres (Bayes)">
<meta name="twitter:title" content="Lord won’t you buy me a Laplace approximation">
<meta name="twitter:description" content="JAX lets you compute a Laplace approximation in three lines of code. Then it ruins your life.">
<meta name="twitter:image" content="https://dansblog.netlify.app/posts/drafts/2023-02-21-laplace/hat.jpg">
<meta name="twitter:creator" content="@dan_p_simpson">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Un garçon pas comme les autres (Bayes)</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html">About this blog</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/dan_p_simpson"><i class="bi bi-twitter" role="img" aria-label="twitter">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dpsimpson"><i class="bi bi-github" role="img" aria-label="github">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://dansblog.netlify.app"><i class="bi bi-person-circle" role="img" aria-label="website">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Lord won’t you buy me a Laplace approximation; or An unexpected detour into sparity-expoiting autodiff</h1>
                  <div>
        <div class="description">
          <p>JAX lets you compute a Laplace approximation in three lines of code. Then it ruins your life.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">JAX</div>
                <div class="quarto-category">Laplace approximation</div>
                <div class="quarto-category">Sparse matrices</div>
                <div class="quarto-category">Autodiff</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dan Simpson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 23, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-laplace-approximation" id="toc-the-laplace-approximation" class="nav-link active" data-scroll-target="#the-laplace-approximation">The Laplace approximation</a>
  <ul class="collapse">
  <li><a href="#commputing-the-laplace-approximation-in-jax" id="toc-commputing-the-laplace-approximation-in-jax" class="nav-link" data-scroll-target="#commputing-the-laplace-approximation-in-jax">Commputing the Laplace approximation in JAX</a></li>
  <li><a href="#speeding-up-the-computation" id="toc-speeding-up-the-computation" class="nav-link" data-scroll-target="#speeding-up-the-computation">Speeding up the computation</a></li>
  </ul></li>
  <li><a href="#can-we-automate-this-parsing-jax-expressions" id="toc-can-we-automate-this-parsing-jax-expressions" class="nav-link" data-scroll-target="#can-we-automate-this-parsing-jax-expressions">Can we automate this? Parsing JAX expressions</a></li>
  <li><a href="#compressing-a-sparse-hessian" id="toc-compressing-a-sparse-hessian" class="nav-link" data-scroll-target="#compressing-a-sparse-hessian">Compressing a sparse Hessian</a>
  <ul class="collapse">
  <li><a href="#finding-the-sparsity-pattern-by-parsing-jaxs-expression-graph" id="toc-finding-the-sparsity-pattern-by-parsing-jaxs-expression-graph" class="nav-link" data-scroll-target="#finding-the-sparsity-pattern-by-parsing-jaxs-expression-graph">Finding the sparsity pattern by parsing JAX’s expression graph</a></li>
  <li><a href="#putting-it-together" id="toc-putting-it-together" class="nav-link" data-scroll-target="#putting-it-together">Putting it together</a></li>
  </ul></li>
  <li><a href="#but-is-it-really-faster" id="toc-but-is-it-really-faster" class="nav-link" data-scroll-target="#but-is-it-really-faster">But is it really faster?</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>I am, once again, in a bit of a mood. And the only thing that will fix my mood is a good martini and a Laplace approximation. And I’m all out of martinis.</p>
<section id="the-laplace-approximation" class="level2">
<h2 class="anchored" data-anchor-id="the-laplace-approximation">The Laplace approximation</h2>
<p>One of the simplest approximations to a distribution is the Laplace approximation. It be definied as the Gaussian distribution that matches that the mode and the curvature and the curvature at the mode of the target distriution. It lives its best life when the density is of the form <span class="math display">\[
p(x) \propto \exp(-nf_n(x)),
\]</span> where <span class="math inline">\(f_n\)</span> is a sequence of functions<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Let’s imagine that we want to approximate the normalised density <span class="math inline">\(p(x)\)</span> near the mode <span class="math inline">\(x^*\)</span>. We can do this by taking the second order Taylor expansion of <span class="math inline">\(f_n\)</span> around <span class="math inline">\(x=x_0\)</span>, which is <span class="math display">\[
f_n = f_n(x^*) + (x-x^*)^TH(x^*)(x-x^*)  + \mathcal{O}((x-x^*)^3),
\]</span> where<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <span class="math display">\[
[H(x^*)]_{ij} = \frac{\partial^2 f_n}{\partial x_i \partial x_j}
\]</span> is the Hessian matrix.</p>
<p>If we replace <span class="math inline">\(f_n\)</span> by it’s quadratic approximation we get<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math display">\[
p(x) \approx N(x^*, n^{-1}H(x^*)^{-1}).
\]</span></p>
<p>The Laplace approximation can be justified rigorously and has a well-studied error and it’s known to work quite well when <span class="math inline">\(p(x)\)</span> is a) unimodal<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and b) isn’t tooooo non-Gaussian.</p>
<p>In practice, people have found that Laplace approximations do a reasonable<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> job quantifying uncertainty even in complex neural network models and it is at the heart of any number of classical estimators in statistics.</p>
<p>Also, like, it’s probably possible to make this all seem a lot more complicated, but it really isn’t. It’s just a two step process:</p>
<div class="algorithm">
<ol type="1">
<li><p>Find the mode <span class="math inline">\(x^* = \arg \max_x f_n(x)\)</span></p></li>
<li><p>Compute the Hessian <span class="math inline">\(H(x^*)\)</span>.</p></li>
</ol>
</div>
<p>In a Bayesian context, we typically take <span class="math display">\[
f_n(x) = \frac{1}{n} \sum_{i=1}^n \log p(y_i \mid x) + \frac{1}{n} \log p(x),
\]</span> which will lead to a Gaussian approximation to the posterior distribution.</p>
<section id="commputing-the-laplace-approximation-in-jax" class="level3">
<h3 class="anchored" data-anchor-id="commputing-the-laplace-approximation-in-jax">Commputing the Laplace approximation in JAX</h3>
<p>This is a two step process and, to be honest, all of the steps are pretty standard. So (hopefully) this will not be too tricky to implement. For simplicity, I’m not going to bother with the dividing and multiplying by <span class="math inline">\(n\)</span>, although for very large data it could be quite</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> jax.scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> jax.scipy.special <span class="im">import</span> expit</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">from</span> jax <span class="im">import</span> jacfwd, grad</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">from</span> jax <span class="im">import</span> Array</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="im">from</span> typing <span class="im">import</span> Callable, Tuple, List</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="kw">def</span> laplace(f: Callable, x0: Array) <span class="op">-&gt;</span> Array:</span>
<span id="cb1-9"><a href="#cb1-9"></a>    nx <span class="op">=</span> x0.shape[<span class="dv">0</span>]</span>
<span id="cb1-10"><a href="#cb1-10"></a>    mode, <span class="op">*</span>details <span class="op">=</span> minimize(<span class="kw">lambda</span> x: <span class="op">-</span>f(x), x0, method <span class="op">=</span> <span class="st">"BFGS"</span>)</span>
<span id="cb1-11"><a href="#cb1-11"></a>    H <span class="op">=</span>  <span class="op">-</span><span class="fl">1.0</span> <span class="op">*</span> jacfwd(grad(f))(mode)</span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="cf">return</span> mode, H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are a few things worth noting here. There’s not really much in this code, except to note that <code>jax.scipy.optimize.minimize</code> finds the minimum of <span class="math inline">\(f\)</span>, so I had to pass through the negative of the function. This change also propogagtes to the compuation of the Hessian, which is computed as the Jacobian of the gradient of f.&nbsp;</p>
<p>Depending on what needs to be done with the Laplace approximation, it might be more appropriate to output the log-density rather than just the mode and the Hessian, but for the moment we will keep this signature.</p>
<p>Let’s try it out. First of all, I’m going to generate some random data from a logistic regression model. This is going to use <a href="https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html">Jax’s slightly odd random number system where you need to manually update the state of the pseudo-random number generator</a>. This is beautifully repeatable<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> unlike, say, R or standard numpy, where you’ve got to pay <em>a lot</em> of attention to the state of the random number generator to avoid oddities.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="kw">def</span> make_data(key, n: <span class="bu">int</span>, p: <span class="bu">int</span>) <span class="op">-&gt;</span> Tuple[Array, Array]:</span>
<span id="cb2-4"><a href="#cb2-4"></a>  key, sub <span class="op">=</span> random.split(key)</span>
<span id="cb2-5"><a href="#cb2-5"></a>  X <span class="op">=</span> random.normal(sub, shape <span class="op">=</span> (n,p)) <span class="op">/</span>jnp.sqrt(p)</span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a>  key, sub <span class="op">=</span> random.split(key)</span>
<span id="cb2-8"><a href="#cb2-8"></a>  beta <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> random.normal(sub, shape <span class="op">=</span> (p,))</span>
<span id="cb2-9"><a href="#cb2-9"></a>  key, sub <span class="op">=</span> random.split(key)</span>
<span id="cb2-10"><a href="#cb2-10"></a>  beta0 <span class="op">=</span> random.normal(sub)</span>
<span id="cb2-11"><a href="#cb2-11"></a></span>
<span id="cb2-12"><a href="#cb2-12"></a></span>
<span id="cb2-13"><a href="#cb2-13"></a>  key, sub <span class="op">=</span> random.split(key)</span>
<span id="cb2-14"><a href="#cb2-14"></a>  y <span class="op">=</span> random.bernoulli(sub, expit(beta0 <span class="op">+</span> X <span class="op">@</span> beta))</span>
<span id="cb2-15"><a href="#cb2-15"></a></span>
<span id="cb2-16"><a href="#cb2-16"></a>  <span class="cf">return</span> (y, X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>An interesting sidenote here is that I’ve generated the design matrix <span class="math inline">\(X\)</span> to have standard Gaussian columns. This is <em>not</em> a benign choice as <span class="math inline">\(n\)</span> gets big. With <em>very</em> high probability, the columns of <span class="math inline">\(X\)</span> will be almost<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> orthonormal, which means that this is the best possible case for logistic regression. Generally speaking, design matrices from real<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> data have a great deal of colinearity in them and so algorithms that perform well on random design matrices may perform less well on real data.</p>
<p>Ok, so let’s fit the model! I’m just going to use <span class="math inline">\(N(0,1)\)</span> priors on all of the <span class="math inline">\(\beta\)</span>s.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb3-2"><a href="#cb3-2"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>p <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>key <span class="op">=</span> random.PRNGKey(<span class="dv">30127</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a>y, X <span class="op">=</span> make_data(key, n, p)</span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="kw">def</span> log_posterior(beta: Array, X: Array, y: Array) <span class="op">-&gt;</span> Array:</span>
<span id="cb3-9"><a href="#cb3-9"></a>    <span class="cf">assert</span> beta.shape[<span class="dv">0</span>] <span class="op">==</span> X.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a>    prob <span class="op">=</span> expit(beta[<span class="dv">0</span>] <span class="op">+</span> X <span class="op">@</span> beta[<span class="dv">1</span>:])</span>
<span id="cb3-12"><a href="#cb3-12"></a>    </span>
<span id="cb3-13"><a href="#cb3-13"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(y <span class="op">*</span> jnp.log(prob) <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y) <span class="op">*</span> jnp.log1p(<span class="op">-</span>prob)) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> jnp.dot(beta, beta) </span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a></span>
<span id="cb3-16"><a href="#cb3-16"></a>post_mean, H <span class="op">=</span> laplace(partial(log_posterior, X <span class="op">=</span> X, y <span class="op">=</span> y),</span>
<span id="cb3-17"><a href="#cb3-17"></a>                        x0 <span class="op">=</span>jnp.zeros(X.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb3-18"><a href="#cb3-18"></a></span>
<span id="cb3-19"><a href="#cb3-19"></a>post_cov <span class="op">=</span> jnp.linalg.inv(H)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see how this performs relative to MCMC. To do that, I’m going to build and equivalent PyMC model.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> logistic_reg:</span>
<span id="cb4-6"><a href="#cb4-6"></a>  beta <span class="op">=</span> pm.Normal(<span class="st">'beta'</span>, <span class="dv">0</span>, <span class="dv">1</span>, shape <span class="op">=</span> (p<span class="op">+</span><span class="dv">1</span>,))</span>
<span id="cb4-7"><a href="#cb4-7"></a>  linpred <span class="op">=</span> beta[<span class="dv">0</span>] <span class="op">+</span> pm.math.dot(np.array(X), beta[<span class="dv">1</span>:])</span>
<span id="cb4-8"><a href="#cb4-8"></a>  pm.Bernoulli(<span class="st">"y"</span>, p <span class="op">=</span> pm.math.invlogit(linpred), observed <span class="op">=</span> np.array(y))</span>
<span id="cb4-9"><a href="#cb4-9"></a>  posterior <span class="op">=</span> pm.sample(tune<span class="op">=</span><span class="dv">1000</span>, draws<span class="op">=</span><span class="dv">1000</span>, chains<span class="op">=</span><span class="dv">4</span>, cores <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># I would like to apologise for the following pandas code.</span></span>
<span id="cb4-12"><a href="#cb4-12"></a>tmp <span class="op">=</span> pm.summary(posterior)</span>
<span id="cb4-13"><a href="#cb4-13"></a>tmp <span class="op">=</span> tmp.assign(laplace_mean <span class="op">=</span> post_mean, laplace_sd <span class="op">=</span> np.sqrt(np.diag(post_cov)), Variable <span class="op">=</span> tmp.index)[[<span class="st">"Variable"</span>,<span class="st">"mean"</span>, <span class="st">"laplace_mean"</span>, <span class="st">"sd"</span>, <span class="st">"laplace_sd"</span>]]</span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="cf">with</span> pd.option_context(<span class="st">'display.precision'</span>, <span class="dv">3</span>):</span>
<span id="cb4-15"><a href="#cb4-15"></a>    <span class="bu">print</span>(tmp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="2000" class="" max="2000" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 0, 0 divergences]
    </div>
    
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="2000" class="" max="2000" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 1, 0 divergences]
    </div>
    
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="2000" class="" max="2000" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 2, 0 divergences]
    </div>
    
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="2000" class="" max="2000" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 3, 0 divergences]
    </div>
    
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        Variable   mean  laplace_mean     sd  laplace_sd
beta[0]  beta[0]  0.248         0.234  0.236       0.229
beta[1]  beta[1] -0.968        -0.914  0.445       0.428
beta[2]  beta[2] -1.719        -1.616  0.470       0.470
beta[3]  beta[3] -0.976        -0.926  0.432       0.416
beta[4]  beta[4] -0.750        -0.716  0.472       0.457
beta[5]  beta[5]  0.638         0.609  0.486       0.475</code></pre>
</div>
</div>
<p>Well that’s just dandy! Everything is pretty<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> close. With 1000 observations it’s identical to within 3 decimal places.</p>
</section>
<section id="speeding-up-the-computation" class="level3">
<h3 class="anchored" data-anchor-id="speeding-up-the-computation">Speeding up the computation</h3>
<p>So that is all well and dandy. Let’s see how long it takes. I am interested in big models, so for this demonstration, I’m going to take <span class="math inline">\(p = 5000\)</span>. That said, I’m not enormously interested in seeing how this scales in <span class="math inline">\(n\)</span> (linearly), so I’m going to keep that at the fairly unrealistic value of <span class="math inline">\(n=1000\)</span>.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> timeit</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="kw">def</span> hess_test(key, n, p):</span>
<span id="cb6-3"><a href="#cb6-3"></a>  y, X <span class="op">=</span> make_data(key, n , p)</span>
<span id="cb6-4"><a href="#cb6-4"></a>  inpu <span class="op">=</span> jnp.ones(p<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>  <span class="kw">def</span> hess():</span>
<span id="cb6-6"><a href="#cb6-6"></a>    f <span class="op">=</span> partial(log_posterior, X <span class="op">=</span> X, y <span class="op">=</span> y)</span>
<span id="cb6-7"><a href="#cb6-7"></a>    <span class="cf">return</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">*</span> jacfwd(grad(f))(inpu)</span>
<span id="cb6-8"><a href="#cb6-8"></a>  <span class="cf">return</span> hess</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>p <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb6-12"><a href="#cb6-12"></a>key, sub <span class="op">=</span> random.split(key)</span>
<span id="cb6-13"><a href="#cb6-13"></a>hess <span class="op">=</span> hess_test(sub, n , p)</span>
<span id="cb6-14"><a href="#cb6-14"></a>times <span class="op">=</span> timeit.repeat(hess, number <span class="op">=</span> <span class="dv">5</span>, repeat <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="bu">print</span>(<span class="ss">f"Autodiff: The average time with p = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss"> is </span><span class="sc">{</span>np<span class="sc">.</span>mean(times)<span class="sc">: .3f}</span><span class="ss">(+/-</span><span class="sc">{</span>np<span class="sc">.</span>std(times)<span class="sc">: .3f}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Autodiff: The average time with p = 5000 is  4.106(+/- 0.456)</code></pre>
</div>
</div>
<p>That doesn’t seem too bad, but the thing is that I know quite a lot about logistic regression. It is, after all, logistic regression. In particular, I know that the Hessian has the form <span class="math display">\[
H = X^T D(\beta) X,
\]</span> where <span class="math inline">\(D(\beta)\)</span> is a <em>diagonal</em> <span class="math inline">\(n \times n\)</span> matrix that has a known form.</p>
<p>This means that the appropriate comparision is between the speed of the autodiff Hessian and how long it takes to comput <span class="math inline">\(X^TDX\)</span> for some diagonal matrix X.</p>
<p>Now you might be worried here that I didn’t explictly save <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>, so the comparision might not be fair. But my friends, I have good news! All ofthat awkard <code>key, sub = random.split(key)</code> malarky has the singular advantage that if I pass the same key into <code>make_data</code> that I used for <code>hess_test</code>, I will get <em>the exact same generated data</em>! So let’s do that. For <span class="math inline">\(D\)</span> I’m just going to pick a random matrix.</p>
<p>If you look at that code and say <em>but Daniel you used the wrong multiplication operator</em>, you can convince yourself that <code>X * d[:, None]</code> gives the same result as <code>jnp.diag(d) @ X</code>. But it will be faster. And it uses such beautiful<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> broadcating rules.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>y, X <span class="op">=</span> make_data(key, n , p)</span>
<span id="cb8-2"><a href="#cb8-2"></a>key, sub <span class="op">=</span> random.split(key)</span>
<span id="cb8-3"><a href="#cb8-3"></a>d <span class="op">=</span> random.normal(sub, shape <span class="op">=</span> (n,))</span>
<span id="cb8-4"><a href="#cb8-4"></a>mm <span class="op">=</span> <span class="kw">lambda</span> : X.T <span class="op">@</span> (X <span class="op">*</span> d[:, <span class="va">None</span>])</span>
<span id="cb8-5"><a href="#cb8-5"></a>times <span class="op">=</span> timeit.repeat(mm, number <span class="op">=</span> <span class="dv">5</span>, repeat <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="bu">print</span>(<span class="ss">f"Symbolic: The average time with p = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss"> is </span><span class="sc">{</span>np<span class="sc">.</span>mean(times)<span class="sc">: .3f}</span><span class="ss">(+/-</span><span class="sc">{</span>np<span class="sc">.</span>std(times)<span class="sc">: .3f}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Symbolic: The average time with p = 5000 is  0.951(+/- 0.058)</code></pre>
</div>
</div>
<p>Oh dear. The symbolic derivative<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> is <em>a lot</em> faster.</p>
<p>Speeding this up is going to take a little work. The first thing we can try is to explictly factor out the linear transformation. Instead of passing in the function <span class="math inline">\(f\)</span>, we could pass in <span class="math inline">\(g\)</span> such that <span class="math display">\[
f(x) = g(Ax),
\]</span> for some matrix <span class="math inline">\(A\)</span>. In our case <span class="math inline">\(g\)</span> would have a diagonal Hessian. Let’s convince ourselves of that with a small example. As well as dropping the intercept, I’ve also dropped the prior term.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>g <span class="op">=</span> <span class="kw">lambda</span> prob: jnp.<span class="bu">sum</span>(y <span class="op">*</span> jnp.log(prob) <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y) <span class="op">*</span> jnp.log1p(<span class="op">-</span>prob))</span>
<span id="cb10-2"><a href="#cb10-2"></a>key, sub2 <span class="op">=</span> random.split(key)</span>
<span id="cb10-3"><a href="#cb10-3"></a>y, X <span class="op">=</span> make_data(sub2, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb10-4"><a href="#cb10-4"></a>b <span class="op">=</span> X <span class="op">@</span> jnp.ones(<span class="dv">3</span>)</span>
<span id="cb10-5"><a href="#cb10-5"></a>D <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">*</span> jacfwd(grad(g))(b)</span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="bu">print</span>(np.<span class="bu">round</span>(D, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.7 0.  0.  0.  0. ]
 [0.  3.7 0.  0.  0. ]
 [0.  0.  7.8 0.  0. ]
 [0.  0.  0.  4.9 0. ]
 [0.  0.  0.  0.  0.3]]</code></pre>
</div>
</div>
<p>Wonderfully diagonal!</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">def</span> hess2(g, A, x):</span>
<span id="cb12-2"><a href="#cb12-2"></a>  <span class="co"># </span></span>
<span id="cb12-3"><a href="#cb12-3"></a>  b <span class="op">=</span> A <span class="op">@</span> x</span>
<span id="cb12-4"><a href="#cb12-4"></a>  D <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">*</span> jacfwd(grad(g))(b)</span>
<span id="cb12-5"><a href="#cb12-5"></a>  H <span class="op">=</span> A.T <span class="op">@</span> (A <span class="op">*</span> jnp.diag(D)[:, <span class="va">None</span>])</span>
<span id="cb12-6"><a href="#cb12-6"></a>  <span class="cf">return</span> H</span>
<span id="cb12-7"><a href="#cb12-7"></a></span>
<span id="cb12-8"><a href="#cb12-8"></a>y, X <span class="op">=</span> make_data(sub, n, p)</span>
<span id="cb12-9"><a href="#cb12-9"></a>g <span class="op">=</span> <span class="kw">lambda</span> prob: jnp.<span class="bu">sum</span>(y <span class="op">*</span> jnp.log(prob) <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y) <span class="op">*</span> jnp.log1p(<span class="op">-</span>prob))</span>
<span id="cb12-10"><a href="#cb12-10"></a>x0 <span class="op">=</span> jnp.ones(p)</span>
<span id="cb12-11"><a href="#cb12-11"></a>h2 <span class="op">=</span> <span class="kw">lambda</span>: hess2(g, X, x0)</span>
<span id="cb12-12"><a href="#cb12-12"></a>times <span class="op">=</span> timeit.repeat(h2, number <span class="op">=</span> <span class="dv">5</span>, repeat <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="bu">print</span>(<span class="ss">f"Separated Hessian: The average time with p = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss"> is </span><span class="sc">{</span>np<span class="sc">.</span>mean(times)<span class="sc">: .3f}</span><span class="ss">(+/-</span><span class="sc">{</span>np<span class="sc">.</span>std(times)<span class="sc">: .3f}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Separated Hessian: The average time with p = 5000 is  1.133(+/- 0.195)</code></pre>
</div>
</div>
<p>Well that’s definitely better.</p>
<p>Now, we might be able to do even better than that if we notice that if we <em>know</em> that <span class="math inline">\(D\)</span> is diagonal, then we don’t need to compute the entire Hessian, we can simply compute the Hessian-vector product <span class="math display">\[
diag(H) = H 1 \qquad \text{iff }H\text{ is diagonal},
\]</span> where <span class="math inline">\(1\)</span> is the vector of ones. Just as we computed the Hessian by computing the Jacobian of the gradient, it turns out that we can compute a Hessian-vector product by computing a Jacobian-vector product <code>jvp</code> of the gradient. The syntax in JAX is, honestly, a little bit gross here<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, but if you want to read up about how it works <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode">the docs are really nice</a>.</p>
<p>This observation is going to be useful because <code>jacfwd</code> computes the Jacobian by computing <span class="math inline">\(n\)</span> Jacobian-vector products. So this observation is saving us <em>a lot</em> of work.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">from</span> jax <span class="im">import</span> jvp</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="kw">def</span> hess3(g, A, x):</span>
<span id="cb14-3"><a href="#cb14-3"></a>  <span class="co"># </span></span>
<span id="cb14-4"><a href="#cb14-4"></a>  b <span class="op">=</span> A <span class="op">@</span> x</span>
<span id="cb14-5"><a href="#cb14-5"></a>  D <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">*</span> jvp(grad(g), (b,), (jnp.ones(n),))[<span class="dv">1</span>]</span>
<span id="cb14-6"><a href="#cb14-6"></a>  H <span class="op">=</span> A.T <span class="op">@</span> (A <span class="op">*</span> D[:, <span class="va">None</span>])</span>
<span id="cb14-7"><a href="#cb14-7"></a>  <span class="cf">return</span> H</span>
<span id="cb14-8"><a href="#cb14-8"></a></span>
<span id="cb14-9"><a href="#cb14-9"></a>h3 <span class="op">=</span> <span class="kw">lambda</span>: hess3(g, X, x0)</span>
<span id="cb14-10"><a href="#cb14-10"></a>times <span class="op">=</span> timeit.repeat(h3, number <span class="op">=</span> <span class="dv">5</span>, repeat <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb14-11"><a href="#cb14-11"></a><span class="bu">print</span>(<span class="ss">f"Compressed Hessian: The average time with p = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss"> is </span><span class="sc">{</span>np<span class="sc">.</span>mean(times)<span class="sc">: .3f}</span><span class="ss">(+/-</span><span class="sc">{</span>np<span class="sc">.</span>std(times)<span class="sc">: .3f}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Compressed Hessian: The average time with p = 5000 is  1.047(+/- 0.101)</code></pre>
</div>
</div>
<p>This is very nearly as fast as the symbolic version. There must be a way to use this.</p>
</section>
</section>
<section id="can-we-automate-this-parsing-jax-expressions" class="level2">
<h2 class="anchored" data-anchor-id="can-we-automate-this-parsing-jax-expressions">Can we automate this? Parsing JAX expressions</h2>
<p>So that was all lovely and shiny. But the problem is that it was very labour intensive. I had to recognise both that you could write <span class="math inline">\(f(x) = g(Ax)\)</span> <em>and</em> that <span class="math inline">\(g\)</span> would have a diagonal Hessian. That is, frankly, hard to do in general.</p>
<p>If I was building a system like Bambi or brms or INLA, where the model classes are relatively constrained, it’s possible to automate both of these steps by analysing the formula. But all I get is a function. So I need to work out how I can automatically parse the code for <span class="math inline">\(f\)</span> to find <span class="math inline">\(g\)</span> and <span class="math inline">\(A\)</span> (if they exist) and to determine if <span class="math inline">\(g\)</span> would have a sparse Hessian.</p>
<p>This is, I guess, a fairly standard task. But fuck me I hate<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> this type of programming. So be prepared for some fairly janky code.</p>
<p>So what are our tasks. First of all we need to trace our way through the JAX code. We can do this by using the intermediate representation that JAX uses when transforming functions: the <code>jaxpr</code>s.</p>
<p>A <code>jaxpr</code> is a transformation of the python code for evaluating a JAX function into a human-readable language that maps types primitives through the code. We can view it using the <code>jax.make_jaxpr</code> function.</p>
<p>Let’s look at the log-posterior function after partial evaluation to make it a single-input function.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">from</span> jax <span class="im">import</span> make_jaxpr</span>
<span id="cb16-2"><a href="#cb16-2"></a></span>
<span id="cb16-3"><a href="#cb16-3"></a>lp <span class="op">=</span> partial(log_posterior, X<span class="op">=</span>X, y<span class="op">=</span>y)</span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="bu">print</span>(make_jaxpr(lp)(jnp.ones(p<span class="op">+</span><span class="dv">1</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{ lambda a:f32[1000,5000] b:bool[1000]; c:f32[5001]. let
    d:f32[1] = dynamic_slice[slice_sizes=(1,)] c 0
    e:f32[] = squeeze[dimensions=(0,)] d
    f:f32[5000] = dynamic_slice[slice_sizes=(5000,)] c 1
    g:f32[1000] = dot_general[
      dimension_numbers=(((1,), (0,)), ((), ()))
      precision=None
      preferred_element_type=None
    ] a f
    h:f32[1000] = add e g
    i:f32[1000] = logistic h
    j:f32[1000] = log i
    k:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b
    l:f32[1000] = mul k j
    m:i32[1000] = convert_element_type[new_dtype=int32 weak_type=True] b
    n:i32[1000] = sub 1 m
    o:f32[1000] = neg i
    p:f32[1000] = log1p o
    q:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] n
    r:f32[1000] = mul q p
    s:f32[1000] = add l r
    t:f32[] = reduce_sum[axes=(0,)] s
    u:f32[] = dot_general[
      dimension_numbers=(((0,), (0,)), ((), ()))
      precision=None
      preferred_element_type=None
    ] c c
    v:f32[] = mul 0.5 u
    w:f32[] = sub t v
  in (w,) }</code></pre>
</div>
</div>
<p>This can be a bit tricky to read the first time you see it, but it’s waaaay easier that X86-Assembly or the LLVM-IR. Basicaly it says that to compute <code>lp(jnp.ones(p+1))</code> you need to run through this program. The first line gives the inputs (with types and shapes). Then after the <code>let</code> statemnt, there are a the commands that need to be executed in order. A single execution looks like</p>
<pre><code>d:f32[1] = dynamic_slice[slice_sizes=(1,)] c 0</code></pre>
<p>This can be read as _take a slice of vector <code>c</code> starting at <code>0</code> of shape <code>(1,)</code> and store it in <code>d</code>, which is a 1-dimensional 32bit float array. (The line after turns it into a scalar.)</p>
<p>All of the other lines can be read similarly. A good trick, if you don’t recognise the primitive<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>, is to <a href="https://jax.readthedocs.io/en/latest/jax.lax.html">look it up</a> in the <code>jax.lax</code> submodule.</p>
<p>Even a cursory read of this suggests that we could probably save a couple of tedious operations by passing in an integer <code>y</code>, rather than a boolean <code>y</code>, but hey. That really shouldn’t cost much.</p>
<p>While the <code>jaxpr</code> is lovely, it’s a whole lot easier to reason about if you see it graphically. We can plot the <em>expression graph</em> using<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> the <code>haiku</code> package from DeepMind.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="im">from</span> haiku.experimental <span class="im">import</span> to_dot</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="im">import</span> graphviz</span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="im">import</span> re</span>
<span id="cb19-4"><a href="#cb19-4"></a>f <span class="op">=</span> partial(log_posterior, X <span class="op">=</span> X, y <span class="op">=</span> y)</span>
<span id="cb19-5"><a href="#cb19-5"></a>dot <span class="op">=</span> to_dot(f)(jnp.ones(p<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb19-6"><a href="#cb19-6"></a><span class="co">#Strip out an obnoxious autogen title</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>dot <span class="op">=</span> re.sub(<span class="st">"&lt;&lt;.*&gt;&gt;;"</span>,<span class="st">"</span><span class="ch">\"</span><span class="st"> </span><span class="ch">\"</span><span class="st">"</span>, dot, count <span class="op">=</span> <span class="dv">1</span>, flags<span class="op">=</span>re.DOTALL)</span>
<span id="cb19-8"><a href="#cb19-8"></a>graphviz.Source(dot)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<p><img src="laplace_files/figure-html/cell-12-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>To understand this graph, the orange-y boxes represent the input for <code>lp</code>. In this case it’s an array of floating point digets with <span class="math inline">\(p+1 = 5001\)</span>. The purple boxes are constants that are used in the function. Some of these are signed integers (s32), there’s a matrix (f32[1000, 5000]), and there is even a literal (0.5). The blue box is the output. That leaves the yellow boxes, which have all of the operations, with inward arrows indicating the inputs and outward arrows indicating the outputs.</p>
<p>Looking at the graph, we can split it into three sub-graphs. The first sub-graph can be found by tracing an input value through the graph until it hits either a non-linear operation or the end of the graph. The sub-graph is created by making the penultimate node in that sequence an output node. This subgraph represents a linear transformations.</p>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display" data-execution_count="12">
<p><img src="laplace_files/figure-html/cell-13-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Once we have reached the end of the linear portion, we can link the output from this operation to the input of the non-linear subgraph.</p>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display" data-execution_count="13">
<p><img src="laplace_files/figure-html/cell-14-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Finally, we have one more trace of <span class="math inline">\(\beta\)</span> through the graph that is non-linear. We could couple this into the non-linear graph at the cost of having to reason about a bivariate Hessian (which will become complex).</p>
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-display" data-execution_count="14">
<p><img src="laplace_files/figure-html/cell-15-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>The two non-linear portions of the graph are merged through a trivial linear combination.</p>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-display" data-execution_count="15">
<p><img src="laplace_files/figure-html/cell-16-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>So we need to trace through these jaxprs and keep a record of which of the sub-graphs they are in (and we do not know how many subgraphs there will be!). We also need to note if an operation is linear or not. This is not something that is automatically provided. We need to store this information ourselves.</p>
<p>The only way I can think to do this is to make a set of all of the jax operations that I know to be linear. Many of them are just index or type stuff. Unfortunately, there is a more complex class of operation, which are only <em>sometimes</em> linear.</p>
<p>The first example we see of this is</p>
<pre><code>g:f32[1000] = dot_general[
      dimension_numbers=(((1,), (0,)), ((), ()))
      precision=None
      preferred_element_type=None
    ] a f</code></pre>
<p>This line represents the general tensor dot product between <code>a</code> and <code>f</code>. In this case, <code>a</code> is constant input (the matrix <span class="math inline">\(X\)</span>) while <code>f</code> is a linear transformation of the input (<code>beta[1:]</code>), so the resulting step is linear. However, there is a second <code>dot_general</code> in the code, which occurs at</p>
<pre><code>u:f32[] = dot_general[
      dimension_numbers=(((0,), (0,)), ((), ()))
      precision=None
      preferred_element_type=None
    ] c c</code></pre>
<p>In this case, <code>c</code> is a linear transformation of the input (it’s just <code>beta</code>), but <code>dot(c,c)</code> is a quadratic function. Hence in this case, <code>dot_general</code> is not linear.</p>
<p>We are going to need to work out how to handle this case. In the folded code is a partial<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> list of the <code>jax.lax</code> primitives that are linear or occasionally linear. All in all there are 69 linear or no-op primatives and 7 sometimes linear primitives.</p>
<div class="cell" data-execution_count="16">
<details>
<summary>jax.lax linear and sometimes linear primitives</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>jax_linear <span class="op">=</span> {</span>
<span id="cb22-2"><a href="#cb22-2"></a>  <span class="st">'add'</span>,</span>
<span id="cb22-3"><a href="#cb22-3"></a>  <span class="st">'bitcast_convert_type'</span>,</span>
<span id="cb22-4"><a href="#cb22-4"></a>  <span class="st">'broadcast'</span>,</span>
<span id="cb22-5"><a href="#cb22-5"></a>  <span class="st">'broadcast_in_dim'</span>,</span>
<span id="cb22-6"><a href="#cb22-6"></a>  <span class="st">'broadcast_shapes'</span>,</span>
<span id="cb22-7"><a href="#cb22-7"></a>  <span class="st">'broadcast_to_rank'</span>,</span>
<span id="cb22-8"><a href="#cb22-8"></a>  <span class="st">'clz'</span>,</span>
<span id="cb22-9"><a href="#cb22-9"></a>  <span class="st">'collapse'</span>,</span>
<span id="cb22-10"><a href="#cb22-10"></a>  <span class="st">'complex'</span>,</span>
<span id="cb22-11"><a href="#cb22-11"></a>  <span class="st">'concatenate'</span>,</span>
<span id="cb22-12"><a href="#cb22-12"></a>  <span class="st">'conj'</span>,</span>
<span id="cb22-13"><a href="#cb22-13"></a>  <span class="st">'convert_element_type'</span>,</span>
<span id="cb22-14"><a href="#cb22-14"></a>  <span class="st">'dtype'</span>,</span>
<span id="cb22-15"><a href="#cb22-15"></a>  <span class="st">'dtypes'</span>,</span>
<span id="cb22-16"><a href="#cb22-16"></a>  <span class="st">'dynamic_slice'</span>,</span>
<span id="cb22-17"><a href="#cb22-17"></a>  <span class="st">'expand_dims'</span>,</span>
<span id="cb22-18"><a href="#cb22-18"></a>  <span class="st">'full'</span>,</span>
<span id="cb22-19"><a href="#cb22-19"></a>  <span class="st">'full_like'</span>,</span>
<span id="cb22-20"><a href="#cb22-20"></a>  <span class="st">'imag'</span>,</span>
<span id="cb22-21"><a href="#cb22-21"></a>  <span class="st">'neg'</span>,</span>
<span id="cb22-22"><a href="#cb22-22"></a>  <span class="st">'pad'</span>,</span>
<span id="cb22-23"><a href="#cb22-23"></a>  <span class="st">'padtype_to_pads'</span>,</span>
<span id="cb22-24"><a href="#cb22-24"></a>  <span class="st">'real'</span>,</span>
<span id="cb22-25"><a href="#cb22-25"></a>  <span class="st">'reduce'</span>,</span>
<span id="cb22-26"><a href="#cb22-26"></a>  <span class="st">'reshape'</span>,</span>
<span id="cb22-27"><a href="#cb22-27"></a>  <span class="st">'rev'</span>,</span>
<span id="cb22-28"><a href="#cb22-28"></a>  <span class="st">'rng_bit_generator'</span>,</span>
<span id="cb22-29"><a href="#cb22-29"></a>  <span class="st">'rng_uniform'</span>,</span>
<span id="cb22-30"><a href="#cb22-30"></a>  <span class="st">'select'</span>,</span>
<span id="cb22-31"><a href="#cb22-31"></a>  <span class="st">'select_n'</span>,</span>
<span id="cb22-32"><a href="#cb22-32"></a>  <span class="st">'squeeze'</span>,</span>
<span id="cb22-33"><a href="#cb22-33"></a>  <span class="st">'sub'</span>,</span>
<span id="cb22-34"><a href="#cb22-34"></a>  <span class="st">'transpose'</span>,</span>
<span id="cb22-35"><a href="#cb22-35"></a>  <span class="st">'zeros_like_array'</span>,</span>
<span id="cb22-36"><a href="#cb22-36"></a>  <span class="st">'GatherDimensionNumbers'</span>,</span>
<span id="cb22-37"><a href="#cb22-37"></a>  <span class="st">'GatherScatterMode'</span>,</span>
<span id="cb22-38"><a href="#cb22-38"></a>  <span class="st">'ScatterDimensionNumbers'</span>,</span>
<span id="cb22-39"><a href="#cb22-39"></a>  <span class="st">'dynamic_index_in_dim'</span>,</span>
<span id="cb22-40"><a href="#cb22-40"></a>  <span class="st">'dynamic_slice'</span>,</span>
<span id="cb22-41"><a href="#cb22-41"></a>  <span class="st">'dynamic_slice_in_dim'</span>,</span>
<span id="cb22-42"><a href="#cb22-42"></a>  <span class="st">'dynamic_update_index_in_dim'</span>,</span>
<span id="cb22-43"><a href="#cb22-43"></a>  <span class="st">'dynamic_update_slice'</span>,</span>
<span id="cb22-44"><a href="#cb22-44"></a>  <span class="st">'dynamic_update_slice_in_dim'</span>,</span>
<span id="cb22-45"><a href="#cb22-45"></a>  <span class="st">'gather'</span>,</span>
<span id="cb22-46"><a href="#cb22-46"></a>  <span class="st">'index_in_dim'</span>,</span>
<span id="cb22-47"><a href="#cb22-47"></a>  <span class="st">'index_take'</span>,</span>
<span id="cb22-48"><a href="#cb22-48"></a>  <span class="st">'reduce_sum'</span>,</span>
<span id="cb22-49"><a href="#cb22-49"></a>  <span class="st">'scatter'</span>,</span>
<span id="cb22-50"><a href="#cb22-50"></a>  <span class="st">'scatter_add'</span>,</span>
<span id="cb22-51"><a href="#cb22-51"></a>  <span class="st">'slice'</span>,</span>
<span id="cb22-52"><a href="#cb22-52"></a>  <span class="st">'slice_in_dim'</span>,</span>
<span id="cb22-53"><a href="#cb22-53"></a>  <span class="st">'conv'</span>,</span>
<span id="cb22-54"><a href="#cb22-54"></a>  <span class="st">'conv_dimension_numbers'</span>,</span>
<span id="cb22-55"><a href="#cb22-55"></a>  <span class="st">'conv_general_dilated'</span>,</span>
<span id="cb22-56"><a href="#cb22-56"></a>  <span class="st">'conv_general_permutations'</span>,</span>
<span id="cb22-57"><a href="#cb22-57"></a>  <span class="st">'conv_general_shape_tuple'</span>,</span>
<span id="cb22-58"><a href="#cb22-58"></a>  <span class="st">'conv_shape_tuple'</span>,</span>
<span id="cb22-59"><a href="#cb22-59"></a>  <span class="st">'conv_transpose'</span>,</span>
<span id="cb22-60"><a href="#cb22-60"></a>  <span class="st">'conv_transpose_shape_tuple'</span>,</span>
<span id="cb22-61"><a href="#cb22-61"></a>  <span class="st">'conv_with_general_padding'</span>,</span>
<span id="cb22-62"><a href="#cb22-62"></a>  <span class="st">'cumsum'</span>,</span>
<span id="cb22-63"><a href="#cb22-63"></a>  <span class="st">'fft'</span>,</span>
<span id="cb22-64"><a href="#cb22-64"></a>  <span class="st">'all_gather'</span>,</span>
<span id="cb22-65"><a href="#cb22-65"></a>  <span class="st">'all_to_all'</span>,</span>
<span id="cb22-66"><a href="#cb22-66"></a>  <span class="st">'axis_index'</span>,</span>
<span id="cb22-67"><a href="#cb22-67"></a>  <span class="st">'ppermute'</span>,</span>
<span id="cb22-68"><a href="#cb22-68"></a>  <span class="st">'pshuffle'</span>,</span>
<span id="cb22-69"><a href="#cb22-69"></a>  <span class="st">'psum'</span>,</span>
<span id="cb22-70"><a href="#cb22-70"></a>  <span class="st">'psum_scatter'</span>,</span>
<span id="cb22-71"><a href="#cb22-71"></a>  <span class="st">'pswapaxes'</span>,</span>
<span id="cb22-72"><a href="#cb22-72"></a>  <span class="st">'xeinsum'</span></span>
<span id="cb22-73"><a href="#cb22-73"></a>}</span>
<span id="cb22-74"><a href="#cb22-74"></a></span>
<span id="cb22-75"><a href="#cb22-75"></a>jax_sometimes_linear <span class="op">=</span> { </span>
<span id="cb22-76"><a href="#cb22-76"></a>  <span class="st">'batch_matmul'</span>,</span>
<span id="cb22-77"><a href="#cb22-77"></a>  <span class="st">'dot'</span>,</span>
<span id="cb22-78"><a href="#cb22-78"></a>  <span class="st">'dot_general'</span>,</span>
<span id="cb22-79"><a href="#cb22-79"></a>  <span class="st">'mul'</span></span>
<span id="cb22-80"><a href="#cb22-80"></a> }</span>
<span id="cb22-81"><a href="#cb22-81"></a>jax_first_linear <span class="op">=</span> {</span>
<span id="cb22-82"><a href="#cb22-82"></a>  <span class="st">'div'</span></span>
<span id="cb22-83"><a href="#cb22-83"></a> }</span>
<span id="cb22-84"><a href="#cb22-84"></a>jax_last_linear <span class="op">=</span> {</span>
<span id="cb22-85"><a href="#cb22-85"></a>  <span class="st">'custom_linear_solve'</span>,</span>
<span id="cb22-86"><a href="#cb22-86"></a>  <span class="st">'triangular_solve'</span>,</span>
<span id="cb22-87"><a href="#cb22-87"></a>  <span class="st">'tridiagonal_solve'</span></span>
<span id="cb22-88"><a href="#cb22-88"></a> }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>All of the <em>sometimes linear</em> operations are linear as long as only one of their arguments depends on the function inputs. For both <code>div</code> and the various linear solves, the position of the input-depenedent argument is restricted to one of the two positions.</p>
<p>Hence, in order to split our graph into appropriate sub-graphs we need to trace through the <code>jaxpr</code> and keep track of every variable and if it depends on linear or non-linear parts.</p>
<p>Ok then. Let’s see if we can do it.</p>
<p>First things first, I’m going to need a way to represent all of the information I need in the graph. I will do this with a node dataclass<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="im">import</span> dataclasses <span class="im">as</span> dc</span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="at">@dc.dataclass</span></span>
<span id="cb23-3"><a href="#cb23-3"></a><span class="kw">class</span> Node:</span>
<span id="cb23-4"><a href="#cb23-4"></a>  number: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-5"><a href="#cb23-5"></a>  eqn: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-6"><a href="#cb23-6"></a>  parents: List[<span class="bu">int</span>] <span class="op">=</span> dc.field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb23-7"><a href="#cb23-7"></a>  out_vals: List[<span class="bu">int</span>] <span class="op">=</span> dc.field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb23-8"><a href="#cb23-8"></a>  depends_on_input: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next I want to keep track of the inputs, which are all nodes in our graph. First off we need to create the graph and record some information aobut it.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>jpr <span class="op">=</span> make_jaxpr(lp)(jnp.ones(p<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb24-2"><a href="#cb24-2"></a></span>
<span id="cb24-3"><a href="#cb24-3"></a><span class="co">## I really need this to have one output and for now only one input that isn't const</span></span>
<span id="cb24-4"><a href="#cb24-4"></a><span class="cf">assert</span> <span class="bu">len</span>(jpr.in_avals) <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb24-5"><a href="#cb24-5"></a><span class="cf">assert</span> <span class="bu">len</span>(jpr.out_avals) <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb24-6"><a href="#cb24-6"></a></span>
<span id="cb24-7"><a href="#cb24-7"></a>n_const <span class="op">=</span> <span class="bu">len</span>(jpr.literals)</span>
<span id="cb24-8"><a href="#cb24-8"></a>n_invars <span class="op">=</span> <span class="bu">len</span>(jpr.in_avals) <span class="co"># always 1</span></span>
<span id="cb24-9"><a href="#cb24-9"></a>n_eqns <span class="op">=</span> <span class="bu">len</span>(jpr.eqns)</span>
<span id="cb24-10"><a href="#cb24-10"></a>n_nodes <span class="op">=</span> (n_const <span class="op">+</span> n_invars </span>
<span id="cb24-11"><a href="#cb24-11"></a>            <span class="op">+</span> <span class="bu">sum</span>([<span class="bu">len</span>(e.outvars) <span class="cf">for</span> e <span class="kw">in</span> jpr.eqns]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The only strange thing here is that a very small number of primitives have two outputs, in which case we’ve got to make sure the node count is correct. This is annoying but whatever.</p>
<p>Now we can build up our graph with all of the side information we need. The format of a <code>jaxpr</code> places the constant inputs in the first node, followed by the non-constant inputs (which I’m calling the input variables). For simplicity, I am assuming that there is only one input variable.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="im">from</span> jax.core <span class="im">import</span> Literal</span>
<span id="cb25-2"><a href="#cb25-2"></a>linear_eqn <span class="op">=</span>[<span class="va">False</span>] <span class="op">*</span> n_eqns</span>
<span id="cb25-3"><a href="#cb25-3"></a></span>
<span id="cb25-4"><a href="#cb25-4"></a>node_list <span class="op">=</span> [Node(number <span class="op">=</span> j) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_nodes)]</span>
<span id="cb25-5"><a href="#cb25-5"></a><span class="co"># Set constant nodes</span></span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_const):</span>
<span id="cb25-7"><a href="#cb25-7"></a>  node_list[j].depends_on_input <span class="op">=</span> <span class="va">False</span></span>
<span id="cb25-8"><a href="#cb25-8"></a></span>
<span id="cb25-9"><a href="#cb25-9"></a><span class="co"># Set input node</span></span>
<span id="cb25-10"><a href="#cb25-10"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_invars):</span>
<span id="cb25-11"><a href="#cb25-11"></a>  node_list[n_const <span class="op">+</span> j].depends_on_input <span class="op">=</span> <span class="va">True</span> </span>
<span id="cb25-12"><a href="#cb25-12"></a></span>
<span id="cb25-13"><a href="#cb25-13"></a><span class="co"># Do the rest</span></span>
<span id="cb25-14"><a href="#cb25-14"></a><span class="cf">for</span> j, eqn <span class="kw">in</span> <span class="bu">enumerate</span>(jpr.eqns):</span>
<span id="cb25-15"><a href="#cb25-15"></a>  parents <span class="op">=</span> [a.count <span class="cf">for</span> a <span class="kw">in</span> eqn.invars <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(a, Literal) ]</span>
<span id="cb25-16"><a href="#cb25-16"></a></span>
<span id="cb25-17"><a href="#cb25-17"></a>  <span class="cf">for</span> i <span class="kw">in</span> eqn.outvars:</span>
<span id="cb25-18"><a href="#cb25-18"></a>    node_list[i.count].eqn <span class="op">=</span> j</span>
<span id="cb25-19"><a href="#cb25-19"></a>    node_list[i.count].parents <span class="op">=</span> parents</span>
<span id="cb25-20"><a href="#cb25-20"></a>    <span class="cf">if</span> <span class="bu">any</span>(node_list[i].depends_on_input <span class="cf">for</span> i <span class="kw">in</span> parents):</span>
<span id="cb25-21"><a href="#cb25-21"></a>      node_list[i.count].depends_on_input <span class="op">=</span> <span class="va">True</span></span>
<span id="cb25-22"><a href="#cb25-22"></a>    <span class="cf">else</span>:</span>
<span id="cb25-23"><a href="#cb25-23"></a>      node_list[i.count].depends_on_input <span class="op">=</span> <span class="va">False</span></span>
<span id="cb25-24"><a href="#cb25-24"></a>  </span>
<span id="cb25-25"><a href="#cb25-25"></a>  prim <span class="op">=</span> eqn.primitive.name</span>
<span id="cb25-26"><a href="#cb25-26"></a>  <span class="cf">if</span> prim <span class="kw">in</span> jax_linear:</span>
<span id="cb25-27"><a href="#cb25-27"></a>    linear_eqn[j] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb25-28"><a href="#cb25-28"></a>  <span class="cf">elif</span> prim <span class="kw">in</span> jax_sometimes_linear:</span>
<span id="cb25-29"><a href="#cb25-29"></a>    <span class="co"># this is a check for once</span></span>
<span id="cb25-30"><a href="#cb25-30"></a>    linear_eqn[j] <span class="op">=</span> <span class="bu">sum</span>(node_list[i].depends_on_input <span class="cf">for</span> i <span class="kw">in</span> parents) <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb25-31"><a href="#cb25-31"></a>  <span class="cf">elif</span> prim <span class="kw">in</span> jax_first_linear:</span>
<span id="cb25-32"><a href="#cb25-32"></a>    linear_eqn[j] <span class="op">=</span> node_list[parents[<span class="dv">0</span>]].depends_on_input <span class="kw">and</span> <span class="kw">not</span> <span class="bu">any</span>(node_list[pa].depends_on_input <span class="cf">for</span> pa <span class="kw">in</span> parents[<span class="dv">1</span>:])</span>
<span id="cb25-33"><a href="#cb25-33"></a>  <span class="cf">elif</span> prim <span class="kw">in</span> jax_last_linear:</span>
<span id="cb25-34"><a href="#cb25-34"></a>    linear_eqn[j] <span class="op">=</span> node_list[parents[<span class="op">-</span><span class="dv">1</span>]].depends_on_input <span class="kw">and</span> <span class="kw">not</span> <span class="bu">any</span>(node_list[pa].depends_on_input <span class="cf">for</span> pa <span class="kw">in</span> parents[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb25-35"><a href="#cb25-35"></a>  <span class="cf">elif</span> <span class="bu">all</span>(<span class="kw">not</span> node_list[i].depends_on_input <span class="cf">for</span> i <span class="kw">in</span> parents):</span>
<span id="cb25-36"><a href="#cb25-36"></a>    linear_eqn[j] <span class="op">=</span> <span class="va">True</span> <span class="co"># Constants are linear</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The only messy thing in here is dealing wiih the sometimes linear primitives. If I was sure that every JAX primitive was guaranteed to have only two inputs, this could be simplified, but sadly I don’t know that.</p>
<p>Now it’s time for the fun: partitioning the problem into subgraphs. To do this, we need to think about what rules we want to encode.</p>
<p>The <em>first rule</em> is that every input for an equation or sub-graph needs to be either a constant, the function input, or the output of some other sub-graph that has already been computed. This means that if we find an equation with an input that doesn’t satisfy these conditions, we need to split the sub-graph that it’s in into two sub-graphs.</p>
<p>The <em>second rule</em> is the only exception to the first rule. A subgraph can have inputs from non-linear subgraphs if an only if it contains a sequence of <code>sum</code> or <code>sub</code> terms and it finishes with the terminal node. This covers the common case where the function we are taking the Hessian of is a linear combination of independent functions. For instance, <code>log_posterior(beta) = log_likelihood(beta) + log_prior(beta)</code>. In this case we can compute the Hessians for the non-linear subexpressions and then combine them.</p>
<p>The <em>third rule</em> is that every independent use of the function input is the opportunity to start a new tree. (It may merge with a known tree.)</p>
<p>And that’s it. Should be simple enough<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> to implement.</p>
<p>I’m feeling like running this bad boy backwards, so let’s do that. One of the assumption we have made is that the function we are tracing has a single output and that is always in the last node and defined in the last equation. So first off, lets get our terminal combination expressions.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="co">## Find the terminal combination expressions</span></span>
<span id="cb26-2"><a href="#cb26-2"></a>comb_eqns <span class="op">=</span> []</span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="cf">for</span> eqn <span class="kw">in</span> jpr.eqns[::<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb26-4"><a href="#cb26-4"></a>  <span class="cf">if</span> <span class="bu">any</span>(node_list[a.count].depends_on_input <span class="cf">for</span> a <span class="kw">in</span> eqn.invars <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(a, Literal)) <span class="kw">and</span> (eqn.primitive.name <span class="op">==</span> <span class="st">"sum"</span> <span class="kw">or</span> eqn.primitive.name <span class="op">==</span> <span class="st">"sub"</span>):</span>
<span id="cb26-5"><a href="#cb26-5"></a>    comb_eqns.append(eqn)</span>
<span id="cb26-6"><a href="#cb26-6"></a>  <span class="cf">else</span>:</span>
<span id="cb26-7"><a href="#cb26-7"></a>    <span class="cf">break</span></span>
<span id="cb26-8"><a href="#cb26-8"></a></span>
<span id="cb26-9"><a href="#cb26-9"></a><span class="bu">print</span>(comb_eqns)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[a:f32[] = sub b c]</code></pre>
</div>
</div>
<p>Now for each of the terminal combination expressions, we will trace their parent back until we run out of tree. While we are doing this, we can also keep track of runs of linear operations. We also have to visit each equation once, so we need to keep track of our visited equations. This is, whether we like it or not, a depth-first search. It’s always a bloody depth-first search, isn’t it.</p>
<p>So what we are going to do is go through each of the combiner nodes and trace the graph down from it and note the path and it’s parent. If we run into a portion of the graph we have already traced, we will note that for later. These paths will either be merged or, if the ancestral path from that point is all linear, will be used as a linear subgraph.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="kw">def</span> dfs(visited, graph, subgraph, to_check, node):</span>
<span id="cb28-2"><a href="#cb28-2"></a>  <span class="cf">if</span> node <span class="kw">in</span> visited:</span>
<span id="cb28-3"><a href="#cb28-3"></a>    to_check.add(node)</span>
<span id="cb28-4"><a href="#cb28-4"></a>  <span class="cf">else</span>:</span>
<span id="cb28-5"><a href="#cb28-5"></a>    visited.add(node)</span>
<span id="cb28-6"><a href="#cb28-6"></a>    subgraph.add(graph[node].eqn)</span>
<span id="cb28-7"><a href="#cb28-7"></a>    <span class="cf">for</span> neighbour <span class="kw">in</span> graph[node].parents:</span>
<span id="cb28-8"><a href="#cb28-8"></a>      dfs(visited, graph, subgraph, to_check, neighbour)</span>
<span id="cb28-9"><a href="#cb28-9"></a>  </span>
<span id="cb28-10"><a href="#cb28-10"></a></span>
<span id="cb28-11"><a href="#cb28-11"></a>visited <span class="op">=</span> <span class="bu">set</span>(<span class="bu">range</span>(n_invars <span class="op">+</span> n_const))</span>
<span id="cb28-12"><a href="#cb28-12"></a>to_check <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb28-13"><a href="#cb28-13"></a>subgraphs <span class="op">=</span> []</span>
<span id="cb28-14"><a href="#cb28-14"></a><span class="cf">for</span> ce <span class="kw">in</span> comb_eqns:</span>
<span id="cb28-15"><a href="#cb28-15"></a>  <span class="cf">for</span> v <span class="kw">in</span> (v <span class="cf">for</span> v <span class="kw">in</span> ce.invars <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(v, Literal)):</span>
<span id="cb28-16"><a href="#cb28-16"></a>    <span class="cf">if</span> v.count <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb28-17"><a href="#cb28-17"></a>      subgraphs.append(<span class="bu">set</span>())</span>
<span id="cb28-18"><a href="#cb28-18"></a>      dfs(visited, node_list, subgraphs[<span class="op">-</span><span class="dv">1</span>], to_check, v.count)</span>
<span id="cb28-19"><a href="#cb28-19"></a></span>
<span id="cb28-20"><a href="#cb28-20"></a>to_check <span class="op">=</span> to_check.intersection(<span class="bu">set</span>(<span class="bu">range</span>(n_invars <span class="op">+</span> n_const <span class="op">+</span> <span class="dv">1</span>, n_nodes)))</span>
<span id="cb28-21"><a href="#cb28-21"></a><span class="bu">print</span>(<span class="ss">f"Subgraphs: </span><span class="sc">{</span>subgraphs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-22"><a href="#cb28-22"></a><span class="bu">print</span>(<span class="ss">f"Danger nodes: </span><span class="sc">{</span>to_check<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Subgraphs: [{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {17, 18}]
Danger nodes: {8}</code></pre>
</div>
</div>
<p>The <code>to_check</code> nodes are only dangerous insofar as we need to make sure that if they are in one of the linear subgraphs they are terminal nodes of a subgraph. To that end, let’s make the linear subgraphs.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>linear_subgraph <span class="op">=</span> []</span>
<span id="cb30-2"><a href="#cb30-2"></a>nonlin_subgraph <span class="op">=</span> []</span>
<span id="cb30-3"><a href="#cb30-3"></a><span class="cf">for</span> subgraph <span class="kw">in</span> subgraphs:</span>
<span id="cb30-4"><a href="#cb30-4"></a>  <span class="bu">print</span>(subgraph)</span>
<span id="cb30-5"><a href="#cb30-5"></a>  split <span class="op">=</span> <span class="bu">next</span>((i <span class="cf">for</span> i, lin <span class="kw">in</span> <span class="bu">enumerate</span>(linear_eqn) <span class="cf">if</span> <span class="kw">not</span> lin <span class="kw">and</span> i <span class="kw">in</span> subgraph))</span>
<span id="cb30-6"><a href="#cb30-6"></a>  <span class="cf">if</span> <span class="bu">any</span>(chk <span class="kw">in</span> subgraph <span class="cf">for</span> chk <span class="kw">in</span> to_check):</span>
<span id="cb30-7"><a href="#cb30-7"></a>    split <span class="op">=</span> <span class="bu">min</span>(split, <span class="bu">min</span>(chk <span class="cf">for</span> chk <span class="kw">in</span> to_check <span class="cf">if</span> chk <span class="kw">in</span> subgraph))</span>
<span id="cb30-8"><a href="#cb30-8"></a></span>
<span id="cb30-9"><a href="#cb30-9"></a>  linear_subgraph.append(subgraph.intersection(<span class="bu">set</span>(<span class="bu">range</span>(split))))</span>
<span id="cb30-10"><a href="#cb30-10"></a>  nonlin_subgraph.append(subgraph.intersection(<span class="bu">set</span>(<span class="bu">range</span>(split, n_eqns))))</span>
<span id="cb30-11"><a href="#cb30-11"></a></span>
<span id="cb30-12"><a href="#cb30-12"></a><span class="bu">print</span>(<span class="ss">f"Linear subgraphs: </span><span class="sc">{</span>linear_subgraph<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-13"><a href="#cb30-13"></a><span class="bu">print</span>(<span class="ss">f"Nonlinear subgraphs: </span><span class="sc">{</span>nonlin_subgraph<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}
{17, 18}
Linear subgraphs: [{0, 1, 2, 3, 4}, set()]
Nonlinear subgraphs: [{5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {17, 18}]</code></pre>
</div>
</div>
<p>The only interesting thing here is making sure that if there is a linear node in the graph that was visited twice, it is the terminal node of the linear graph. The better thing would be to actually split the linear graph, but I’m getting a little bit sick of this post and I don’t really want to deal with multiple linear subgraphs. So I shan’t. But hopefully it’s relatively clear how you would do that.</p>
<p>The final step is to form and assemble the Hessian. To do this, we need to form these subgraphs into new expressions. The main thing that we need to do is re-number all of the variables so that they’re consistent with the subgraph. This, of course, involves another bloody depth-first search.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="kw">def</span> fun_from_subgraph(jpr, subgraph):</span>
<span id="cb32-2"><a href="#cb32-2"></a>  <span class="kw">def</span> _dfs(visited, node):</span>
<span id="cb32-3"><a href="#cb32-3"></a>    <span class="cf">if</span> node <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb32-4"><a href="#cb32-4"></a>      visited.add(node)</span>
<span id="cb32-5"><a href="#cb32-5"></a>      <span class="cf">for</span> neighbour <span class="kw">in</span> node_list(node).parents:</span>
<span id="cb32-6"><a href="#cb32-6"></a>        _dfs(visited, node)</span>
<span id="cb32-7"><a href="#cb32-7"></a>  </span>
<span id="cb32-8"><a href="#cb32-8"></a>  all_vars <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb32-9"><a href="#cb32-9"></a>  <span class="cf">for</span> j <span class="kw">in</span> subgraph:</span>
<span id="cb32-10"><a href="#cb32-10"></a>    <span class="cf">for</span> var <span class="kw">in</span> jpr.eqns[j].invars:</span>
<span id="cb32-11"><a href="#cb32-11"></a>      <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(var, Literal):</span>
<span id="cb32-12"><a href="#cb32-12"></a>        _dfs(all_vars, var.count)</span>
<span id="cb32-13"><a href="#cb32-13"></a>  </span>
<span id="cb32-14"><a href="#cb32-14"></a>  all_vars <span class="op">=</span> <span class="bu">sorted</span>(all_vars)</span>
<span id="cb32-15"><a href="#cb32-15"></a>  relab <span class="op">=</span> {old: new <span class="cf">for</span> (new, old) <span class="kw">in</span> <span class="bu">enumerate</span>(all_vars)}</span>
<span id="cb32-16"><a href="#cb32-16"></a>  </span>
<span id="cb32-17"><a href="#cb32-17"></a>  subeqns <span class="op">=</span> [jpr.eqns[j] <span class="cf">for</span> j <span class="kw">in</span> subgraph]</span>
<span id="cb32-18"><a href="#cb32-18"></a></span>
<span id="cb32-19"><a href="#cb32-19"></a>  <span class="cf">for</span> eqn <span class="kw">in</span> subeqns:</span>
<span id="cb32-20"><a href="#cb32-20"></a>    <span class="cf">for</span> var <span class="kw">in</span> eqn.invars:</span>
<span id="cb32-21"><a href="#cb32-21"></a>      var.count <span class="op">=</span> relab[var.count]</span>
<span id="cb32-22"><a href="#cb32-22"></a>    <span class="cf">for</span> var <span class="kw">in</span> eqn.outvars:</span>
<span id="cb32-23"><a href="#cb32-23"></a>      var.count <span class="op">=</span> relab[var.count]</span>
<span id="cb32-24"><a href="#cb32-24"></a>  </span>
<span id="cb32-25"><a href="#cb32-25"></a>  <span class="co"># Make the inputs</span></span>
<span id="cb32-26"><a href="#cb32-26"></a>  constvars <span class="op">=</span> []</span>
<span id="cb32-27"><a href="#cb32-27"></a>  invars <span class="op">=</span> []</span>
<span id="cb32-28"><a href="#cb32-28"></a>  <span class="cf">for</span> j <span class="kw">in</span> all_vars:</span>
<span id="cb32-29"><a href="#cb32-29"></a>    <span class="cf">if</span> j <span class="op">&lt;</span> n_const:</span>
<span id="cb32-30"><a href="#cb32-30"></a>      constvars.append(jpr.jaxpr.constvars[j])</span>
<span id="cb32-31"><a href="#cb32-31"></a>      constvars[<span class="op">-</span><span class="dv">1</span>].count <span class="op">=</span> relab[consvars[<span class="op">-</span><span class="dv">1</span>].count]</span>
<span id="cb32-32"><a href="#cb32-32"></a>    <span class="cf">elif</span> j <span class="op">&lt;</span> n_const <span class="op">+</span> n_invars:</span>
<span id="cb32-33"><a href="#cb32-33"></a>      invars.append(jpr.jaxpr.invars[j])</span>
<span id="cb32-34"><a href="#cb32-34"></a>      invars[<span class="op">-</span><span class="dv">1</span>].count <span class="op">=</span> relab[invars[<span class="op">-</span><span class="dv">1</span>].count]</span>
<span id="cb32-35"><a href="#cb32-35"></a>    <span class="cf">else</span>:</span>
<span id="cb32-36"><a href="#cb32-36"></a>      <span class="cf">break</span></span>
<span id="cb32-37"><a href="#cb32-37"></a>  </span>
<span id="cb32-38"><a href="#cb32-38"></a>  <span class="co"># Make the output</span></span>
<span id="cb32-39"><a href="#cb32-39"></a>  outvars <span class="op">=</span> subeqns[<span class="bu">max</span>(all_vars)].outvars</span>
<span id="cb32-40"><a href="#cb32-40"></a>  <span class="cf">assert</span> <span class="bu">len</span>(outvars) <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb32-41"><a href="#cb32-41"></a>  outvars[<span class="dv">0</span>].count <span class="op">=</span> relab[outvars[<span class="dv">0</span>].count]</span>
<span id="cb32-42"><a href="#cb32-42"></a>  </span>
<span id="cb32-43"><a href="#cb32-43"></a>  <span class="co"># Make the jaxpr</span></span>
<span id="cb32-44"><a href="#cb32-44"></a>  sub_jaxpr <span class="op">=</span> jax.core.Jaxpr(constvars, invars, outvars, subeqns, jpr.effects)</span>
<span id="cb32-45"><a href="#cb32-45"></a></span>
<span id="cb32-46"><a href="#cb32-46"></a>  <span class="co"># Close the jaxpr</span></span>
<span id="cb32-47"><a href="#cb32-47"></a>  consts <span class="op">=</span> [jpr.consts[j] <span class="cf">for</span> j <span class="kw">in</span> all_vars <span class="cf">if</span> j <span class="op">&lt;</span> n_const]</span>
<span id="cb32-48"><a href="#cb32-48"></a>  closed_sub_jaxpr <span class="op">=</span> jax.core.ClosedJaxpr(sub_jaxpr, consts)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>keep an list of all linear nodes and independent nodes</li>
<li>for each lin set check that there is at most one future linear</li>
<li>for an interior list, check that the parents only depend on OLD THINGS chain it together</li>
</ul>
<p>Here are the rules:</p>
<ul>
<li><p>All inputs must be <em>either</em> linear <em>or</em> function inputs</p></li>
<li><p>A linear can have a non-linear input <em>only</em> if it ends at the terminal node</p></li>
<li><p>If there are more than one input to a function it <em>can’t</em> have common anscestors</p></li>
</ul>
</section>
<section id="compressing-a-sparse-hessian" class="level2">
<h2 class="anchored" data-anchor-id="compressing-a-sparse-hessian">Compressing a sparse Hessian</h2>
<p>A great deal of the time, the layer linking your observations to your latent representation is farily sparse. This naturally leads to a sparse Hessian matrix. In the logistic case, it is diagonal, but in a lot of cases it will depend on more than one element of the latent representation.</p>
<p>In this section, we are going to look at generalising the observation about logistic regression that replaces a full <span class="math inline">\(n \times n\)</span> Hessian with a single Hessian-vector product. In general, we won’t be able to get away with a single product and will instead need a specially constructed set of <span class="math inline">\(k\)</span> probing vectors, where <span class="math inline">\(k\)</span> is a number to be determined (that is hopefully <em>much</em> smaller than <span class="math inline">\(n\)</span>).</p>
<p>To do this, we need to construct our set of probing vectors in a very special way. Each <span class="math inline">\(s_k\)</span> will be a vector containing zeros and ones. The set of indices with <span class="math inline">\([s_k]_j = 1\)</span> have colour <span class="math inline">\(k\)</span>. The aim is to associate each index with a unique colour in such a way that we can recover the algorithm. We can do this with a structurally symmetric orthogonal partiion that satisfies</p>
<blockquote class="blockquote">
<p>For every non-zero <span class="math inline">\(H_{ij}\)</span>, <strong>either</strong> <span class="math inline">\(j\)</span> has colour <span class="math inline">\(k\)</span> and no other index in <span class="math inline">\(k\)</span> has a non-zero in row <span class="math inline">\(i\)</span> <strong>or </strong> <span class="math inline">\(i\)</span> has colour <span class="math inline">\(k'\)</span> and no othr index in <span class="math inline">\(k'\)</span> has a non-zero in column <span class="math inline">\(j\)</span>.</p>
</blockquote>
<p>In the first case, this ensures<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> that <span class="math inline">\([H s_k]_j = H_{ij}\)</span>, while in the second it ensures that <span class="math inline">\([H s_{k'}]_i = H_{ij}\)</span>.</p>
<p>It my no longer come as a surprise, given my <a href="https://dansblog.netlify.app/posts/2022-11-27-sparse7/sparse7.html">previous posts on sparse matrices</a>, that graph theorists came along and solved this problem. It turns out that the partioning of the indices can be achieved by solving the star graph colouring problem.</p>
<div id="def-star-colour" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span>A star colouring of a graph <span class="math inline">\(\mathcal{G} = (\mathcal{V}, \mathcal{E})\)</span> is a function <span class="math inline">\(\phi:\mathcal{V} \rightarrow \{1, \ldots, K}\)</span> such that <span class="math display">\[
\phi(v_1) \neq \phi(v_2), \quad (v_1,v_2) \in \mathcal{E}
\]</span> and every path of length 4 in the graph has at least 3 different colours.</p>
</div>
<p>This definition is exactly like an ordinary graph colouring (if it’s connected they’re coloured differently!), but it has an additional constraint that, it turns out, is mathematically equivalent to structurally symmetric orthogonality. Yes, that’s not extremely obvious, but if you want to look it up, there’s a <a href="http://www.ii.uib.no/~fredrikm/fredrik/papers/sirev2005.pdf">great review article</a> on the topic.</p>
<p>If we have such a colouring, it’s pretty easy to construct the Hessian as a sparse matrix and then plug it into the system.</p>
<p>So we need two things: a sparsity structure of <span class="math inline">\(H\)</span> and a star colouring of the associated adjacency graph. The first of those things we will talk about later. But for the second we can just use an<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> algorithm from <a href="http://www.ii.uib.no/~fredrikm/fredrik/papers/sirev2005.pdf">Gebremedhin, Manne, and Pothe (2005)</a>.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="co">#def star_colour(A):</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="finding-the-sparsity-pattern-by-parsing-jaxs-expression-graph" class="level3">
<h3 class="anchored" data-anchor-id="finding-the-sparsity-pattern-by-parsing-jaxs-expression-graph">Finding the sparsity pattern by parsing JAX’s expression graph</h3>
</section>
<section id="putting-it-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-together">Putting it together</h3>
</section>
</section>
<section id="but-is-it-really-faster" class="level2">
<h2 class="anchored" data-anchor-id="but-is-it-really-faster">But is it really faster?</h2>
<p>All of this was a fair bit of work so I’m tempted to throw myself at the sunk-cost fallacy and just declare it to be good. But there is a problem. Because JAX doesn’t do a symbolic transformation of the program (only a trace through paths associated with specific values), there is no guarantee that the sparsity pattern for <span class="math inline">\(H\)</span> remains the same at each step. And there is nothing wrong with that. It’s an expressive, exciting language.</p>
<p>But all of the code transformation to make a sparsity-exploiting Hessian doesn’t come for free. And the idea of having to do it again every time a Hessian is needed is … troubling. If we could guarantee that the sparsity pattern was static, then we could factor all of this complex parsing and colouring code away and just run it once for each problem.</p>
<p>To do that, we need a library that performs <em>symbolic</em> manipulations and can compile them into an expression graph. JAX is not quite<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> that language.</p>
<p>Thankfully, there is a such a library: <a href="https://aesara.readthedocs.io/en/latest/">Aesara</a>. My next job is to look into Aesara’s guts and work out how to use them for this task. In particular, it will allow me to finally implement the thing that I <em>actually</em> care aboud: Laplace approximations nested inside other computational algorithms.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>that probably converges. Think of it like <span class="math inline">\(n^{-1} \sum_{i=1}^n p(y_i \mid x) + p(x)\)</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The first-order term disappears because at the mode <span class="math inline">\(x^*\)</span> <span class="math inline">\(\nabla f(x^*)=0\)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>After normalising it to make sure that we get a proper density<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>or has one dominant mode<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Something isn’t always better than nothing but sometimes it is<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>You could say reproducible code but I won’t because that word means something pretty specific. I mean, this is not the place for a rant, but it is <em>very</em> difficult to write strictly reproducible code and I am frankly not even going to try to take a bite out of that particular onion.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The maths under this is very interesting and surpringly accessible (in a very advanced sort of way). I guess it depends on what you think of as accessible, but it’s certainly much nicer than entropy and VC-classes. A lovely set of notes that cover everything you’ve ever wanted to know is <a href="https://arxiv.org/abs/1011.3027">here</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Unless someone’s been doing their design of experiments<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>With 100 observations, we expect our data-driven variation (aka the frequentist version) to be about one decimal place, so the Laplace approxiamtion is accurate within that tolerance. In fact, clever maths types can analyse the error in the Laplace approximation and show that the error is about <span class="math inline">\(\mathcal{O}(n^{-1})\)</span>, which is asympotoically much smaller than the sampling variability of <span class="math inline">\(\mathcal{O}(n^{-1/2})\)</span>, which suggests that the error introdcued by the Laplace approximation isn’t catastrophic. At least with enough data.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Be still my beating heart.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Ok. You caught me. They’re not technically the same model. The symbolic code doesn’t include an intercept. I just honestly cannot be arsed to do the very minor matrix algebra to add it in. Nor can I be arsed to add a column of ones to <code>X</code>.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>So many tuples<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>If I was grinding for a coding interview and I came up against a problem like this I would be like “there’s no way I’d ever have to do a task like this”. But here we are. In hell.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>For example, there’s no call to <code>logistic</code> in the code, but a quick look at <code>jax.lax.logistic</code> shows that it’s the same thing as <code>expit</code>.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>This basically <em>just works</em> as long as you’ve got graphviz installed on your system. And once you find the right regex to strip out the <em>terrible</em> autogenerated title.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>If I wasn’t sure, I deleted them from the linear list. There were also <code>scatter_mul</code>, <code>reduce_window</code>, and <code>reduce_window_shape_tuple</code>, which are all sometimes linear but frankly I didn’t want to work out the logic. <a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>I hate writing initialisers<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>I cannot stress enough that this is not my strength.&nbsp;So I’m going to run through these graphs as many times as I need to. I am sure that almost anyone could optimise this code.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>Do the matrix-vector product. If <span class="math inline">\(j \in k\)</span>, then all the non-zero elements in row <span class="math inline">\(i\)</span> will be multiplied by a zero of <span class="math inline">\(s_k\)</span>, so the only element that we pick up in the <span class="math inline">\(j\)</span>th component of <span class="math inline">\(Hs_k\)</span> is <span class="math inline">\(H_{ij}\)</span><a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>This is not the only way to do it. I’ve gone for ease of implementation over speed. A faster and better algorithm, and a <em>very</em> detailed comparison of all of the available options can be found <a href="http://www.ii.uib.no/~fredrikm/fredrik/papers/SISC2007.pdf">here</a>. And I am not implementing that for a fucking blog.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>And it’s not trying to. Their bread and butter is autodiff and what they’re doing is absolutely natural for that.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2023,
  author = {Dan Simpson},
  editor = {},
  title = {Lord Won’t You Buy Me a {Laplace} Approximation; or {An}
    Unexpected Detour into Sparity-Expoiting Autodiff},
  date = {2023-02-23},
  url = {https://dansblog.netlify.app/posts/2022-11-27-sparse7/sparse7.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2023" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Dan Simpson. 2023. <span>“Lord Won’t You Buy Me a Laplace Approximation;
or An Unexpected Detour into Sparity-Expoiting Autodiff.”</span>
February 23, 2023. <a href="https://dansblog.netlify.app/posts/2022-11-27-sparse7/sparse7.html">https://dansblog.netlify.app/posts/2022-11-27-sparse7/sparse7.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>