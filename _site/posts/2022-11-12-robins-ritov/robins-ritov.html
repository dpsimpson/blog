<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dan Simpson">
<meta name="dcterms.date" content="2022-11-15">
<meta name="description" content="Look, it’s a dull example of Bayes being bad. But it comes up often enough to be worth talking about. I’m going to, unsurprisingly, argue that Bayes isn’t bad. Neither are Robings/Ritov/Wasserman wrong. They’re just looking at the problem through a different lens.">

<title>Un garçon pas comme les autres (Bayes) - On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Un garçon pas comme les autres (Bayes) - On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for">
<meta property="og:description" content="Look, it’s a dull example of Bayes being bad. But it comes up often enough to be worth talking about. I’m going to, unsurprisingly, argue that Bayes isn’t bad. Neither are Robings/Ritov/Wasserman wrong. They’re just looking at the problem through a different lens.">
<meta property="og:image" content="https://dansblog.netlify.app/posts/2022-11-12-robins-ritov/misandrists.JPG">
<meta property="og:site_name" content="Un garçon pas comme les autres (Bayes)">
<meta name="twitter:title" content="On that example by Robins and Ritov">
<meta name="twitter:description" content="Look, it’s a dull example of Bayes being bad. But it comes up often enough to be worth talking about. I’m going to, unsurprisingly, argue that Bayes isn’t bad. Neither are Robings/Ritov/Wasserman wrong. They’re just looking at the problem through a different lens.">
<meta name="twitter:image" content="https://dansblog.netlify.app/posts/2022-11-12-robins-ritov/misandrists.JPG">
<meta name="twitter:creator" content="@dan_p_simpson">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Un garçon pas comme les autres (Bayes)</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About this blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/dan_p_simpson"> <i class="bi bi-twitter" role="img" aria-label="twitter">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dpsimpson"> <i class="bi bi-github" role="img" aria-label="github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://dansblog.netlify.app"> <i class="bi bi-person-circle" role="img" aria-label="website">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">On that example of Robins and Ritov; or A sleeping dog in harbor is safe, but that’s not what sleeping dogs are for</h1>
                  <div>
        <div class="description">
          <p>Look, it’s a dull example of Bayes being bad. But it comes up often enough to be worth talking about. I’m going to, unsurprisingly, argue that Bayes isn’t bad. Neither are Robings/Ritov/Wasserman wrong. They’re just looking at the problem through a different lens.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Fundamentals</div>
                <div class="quarto-category">Survey sampling</div>
                <div class="quarto-category">MRP</div>
                <div class="quarto-category">Bayes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://dansblog.netlify.app">Dan Simpson</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 15, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sometimes-its-the-parable-of-the-barren-fig-tree.-sometimes-youre-just-pissed-at-a-shrub." id="toc-sometimes-its-the-parable-of-the-barren-fig-tree.-sometimes-youre-just-pissed-at-a-shrub." class="nav-link active" data-scroll-target="#sometimes-its-the-parable-of-the-barren-fig-tree.-sometimes-youre-just-pissed-at-a-shrub.">Sometimes it’s the parable of the barren fig tree. Sometimes you’re just pissed at a shrub.</a>
  <ul class="collapse">
  <li><a href="#a-counterexample-always-proceedes-from-the-least-interesting-premise" id="toc-a-counterexample-always-proceedes-from-the-least-interesting-premise" class="nav-link" data-scroll-target="#a-counterexample-always-proceedes-from-the-least-interesting-premise">A counterexample always proceedes from the least interesting premise</a></li>
  <li><a href="#robins-and-ritov-toss-an-ancillary-coin-and-let-slip-the-dogs-of-war" id="toc-robins-and-ritov-toss-an-ancillary-coin-and-let-slip-the-dogs-of-war" class="nav-link" data-scroll-target="#robins-and-ritov-toss-an-ancillary-coin-and-let-slip-the-dogs-of-war">Robins and Ritov toss an ancillary coin and let slip the dogs of war</a></li>
  </ul></li>
  <li><a href="#the-likelihood-principle-and-the-death-of-nuance" id="toc-the-likelihood-principle-and-the-death-of-nuance" class="nav-link" data-scroll-target="#the-likelihood-principle-and-the-death-of-nuance">The likelihood principle and the death of nuance</a></li>
  <li><a href="#can-we-save-bayes" id="toc-can-we-save-bayes" class="nav-link" data-scroll-target="#can-we-save-bayes">Can we save Bayes?</a>
  <ul class="collapse">
  <li><a href="#a-simple-posterior-and-its-post-processing" id="toc-a-simple-posterior-and-its-post-processing" class="nav-link" data-scroll-target="#a-simple-posterior-and-its-post-processing">A simple posterior and its post-processing</a></li>
  <li><a href="#modelling-the-effect-of-the-ancillary-coin" id="toc-modelling-the-effect-of-the-ancillary-coin" class="nav-link" data-scroll-target="#modelling-the-effect-of-the-ancillary-coin">Modelling the effect of the ancillary coin</a></li>
  <li><a href="#is-it-bayesian" id="toc-is-it-bayesian" class="nav-link" data-scroll-target="#is-it-bayesian">Is it Bayesian?</a></li>
  </ul></li>
  <li><a href="#is-it-true-am-i-a-chaser" id="toc-is-it-true-am-i-a-chaser" class="nav-link" data-scroll-target="#is-it-true-am-i-a-chaser">Is it true? Am I a chaser?</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="sometimes-its-the-parable-of-the-barren-fig-tree.-sometimes-youre-just-pissed-at-a-shrub." class="level1">
<h1>Sometimes it’s the parable of the barren fig tree. Sometimes you’re just pissed at a shrub.</h1>
<p>Paradoxes and counterexamples live in statistics as our morality plays and our ghost stories. They serve as the creepy gas station attendants that populate the roads leading to the curséd woods; existing not to force change on the adventurer, but to signpost potential danger.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>As a rule, we should also look in askance at attempts to resolve these paradoxes and counterexamples. That is not what they are for. They are community resources, objects of our collective culture, monuments to thwarted desire.</p>
<p>But sometimes, driven by the endless thirst for content, it’s worth diving down into a counterexample and resolving it. This quixotic quest is not to somehow patch a hole, but to rather expand the hole until it can comfortably encase our wants, needs, and prayers.</p>
<p>To that end, let’s gather ’round the campfire and attend the tale of The Bayesian and the Ancillary Coin.</p>
<p>This example<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> was introduced by Robins and Ritov, and greatly popularised (and frequently reformulated) by Larry Wasserman<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. It says<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> this:</p>
<blockquote class="blockquote">
<p>A committed subjective Bayesian (one who cleaves to the likelihood priniciple tighter than Rose clings to that door) will sometimes get a very wrong answer under some simple, but realistic, forms of randomization. Only a less committed Bayesian will be able to skirt the danger.</p>
</blockquote>
<p>So this is what we’re going to do now. First let’s introduce a version of the problem that does not trigger the counterexample. We then introduce the randomization scheme that leads to the error and talk about exactly how things go wrong. As someone who is particular skeptical of any claims to purity<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, the next job is going to be deconstructing this idea of a committed<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> subjective Bayesian. I will, perhaps unsurprisingly, argue that this is the only part of the Robins and Ritov (and Wasserman) conclusions that are somewhat questionable. In fact, a <em>true</em> committed subjective Bayesian<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> can solve the problem. It’s just a matter of looking at it through the correct lens.</p>
<section id="a-counterexample-always-proceedes-from-the-least-interesting-premise" class="level2">
<h2 class="anchored" data-anchor-id="a-counterexample-always-proceedes-from-the-least-interesting-premise">A counterexample always proceedes from the least interesting premise</h2>
<p>This example exists in a number of forms, that each add important corners to the problem, but in the interest of simplicity, we will start with a simple situation where no problems occur.</p>
<p>Assume that there is a large, but fixed, finite number <span class="math inline">\(J\)</span>, and <span class="math inline">\(J\)</span> unknown parameters <span class="math inline">\(\mu_j\)</span>, <span class="math inline">\(j=1,\ldots, J\)</span>. The large number <span class="math inline">\(J\)</span> can be thought of as the number of strata in a population, while <span class="math inline">\(\mu_j\)</span> are the means of the corresponding stratum. Now construct an experiment where you draw <span class="math display">\[
y_i \mid \mu,x = j \sim N(\mu_j, 1).
\]</span> To close out the generative model, we assume that the covariates have a known distribution <span class="math inline">\(x_i \sim \text{Unif}\{1,\ldots, p\}\)</span>.</p>
<p>A classical problem in mathematical statistics is to construct a <span class="math inline">\(\sqrt{n}\)</span>-consistent<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> estimator <span class="math inline">\(\hat\mu_n\)</span> of the vector <span class="math inline">\(\mu\)</span>. But in the setting of this problem, this is quite difficult. The challenge is that if <span class="math inline">\(J\)</span> is a very large number, then we would need a gargantuan<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> number of observations (<span class="math inline">\(n \gg J\)</span>) in order to resolve all of the parameters properly.</p>
<p>But there is a saving grace! The <em>population</em><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> average <span class="math display">\[
\mu = \mathbb{E}(y) = \sum_{j=1}^J \mu_j \Pr(x = j)= \frac{1}{J}\sum_{j=1}^J \mu_j
\]</span> can be estimated fairly easily. In fact, the sample mean (aka the most obvious estimator) <span class="math inline">\(\bar{y} = n^{-1} \sum_{i=1}^n y_i\)</span> is going to be <span class="math inline">\(\sqrt{n}\)</span>-consistent.</p>
<p>Similarly, if we were to construct a Bayesian estimate of the population mean based off the prior <span class="math inline">\(\mu_j \mid m \sim N(m, 1)\)</span> and <span class="math inline">\(m \sim N(0,\tau^2)\)</span>, then the posterior estimate of the population mean is, for large enough<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> <span class="math inline">\(n\)</span>, <span class="math display">\[
\hat \mu_{\text{Bayes},n}= \mathbb{E}(\mu \mid y) \approx \frac{1}{n + 2/\tau} \sum_{i=1}^n y_i.
\]</span> This means that the<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> Bayesian resolution of this problem is roughly the same as the classical resolution. This is a nice thing. For very simple problems, these estimators should be fairly similar. It’s only when shit gets complicated where things become subtle.</p>
<p>This scenario, where a model is parameterized by an extremely high dimensional parameter <span class="math inline">\(\mu\)</span> but the quantity of inferential inference is a low-dimensional summary of <span class="math inline">\(\mu\)</span>, is widely and deeply studied under the name of semi-parametric statistics.</p>
<p>Semi-parametric statistics is, unsurprisingly, harder than parametric statistics, but it also quite a bit more challenging than non-parametric statistics. The reason is that if we want to guarantee a good estimate of a particular finite dimensional summary, it turns out that it’s not enough to generically get a “good” estimate of the high-dimensional parameter. In fact, getting a good estimate of the high-dimensional parameter is often not possible (see the example we just considered).</p>
<p>Instead understanding semi-parametric models becomes the fine art of understanding what needs to be done well and what we can half arse. A description of this would take us <em>well</em> outside the scope of a mere blog post, but if you want to learn more about the topic, that’s what to google.</p>
</section>
<section id="robins-and-ritov-toss-an-ancillary-coin-and-let-slip-the-dogs-of-war" class="level2">
<h2 class="anchored" data-anchor-id="robins-and-ritov-toss-an-ancillary-coin-and-let-slip-the-dogs-of-war">Robins and Ritov toss an ancillary coin and let slip the dogs of war</h2>
<p>In order to destroy all that is right and good about the previous example, we only need to do one thing: randomize in a nefarious way. Robins and Ritov (actually, Wasserman who proposed the case with a finite <span class="math inline">\(J\)</span>) add to their experiment <span class="math inline">\(J\)</span> biased coins <span class="math inline">\(r_j\)</span> with the property that <span class="math display">\[
\Pr(r_j = 1 \mid X=j) = \xi_j,
\]</span> for some <em>known</em> <span class="math inline">\(0 &lt; \delta \leq \xi_j &lt; 1-\delta\)</span>, <span class="math inline">\(j=1,\ldots, J\)</span> and some <span class="math inline">\(c&gt;0\)</span>.</p>
<p>They then go through the data and add a column <span class="math inline">\(r_i \sim \text{Bernouili}(\xi_{x_i})\)</span>. The new data is now a three dimensional vector <span class="math inline">\((y_i, x_i, r_i)\)</span>. It’s important to this problem that the <span class="math inline">\(\xi_j\)</span> are known and that we have the conditional independence structure <span class="math inline">\(y \perp r \mid x\)</span>.</p>
<p>Robins, Ritov, and Wasserman all ask the same question: Can we still estimate the population mean if we only observe samples from the <em>conditional</em> distribution <span class="math inline">\((y_i, x_i) \sim p(x,y \mid r=1)\)</span>?</p>
<p>The answer is going to turn out that there is a perfectly good estimator from classical survey statistics, but a Bayesian estimator is a bit more challenging to find.</p>
<p>Before we get there, it’s worth noting that unlike the problem in the previous section, this problem is at least a little bit interesting. It’s a cartoon of a very common situation where there is covariate-dependent randomization in a clinical trial. Or, maybe even more cleanly, a cartoon of a simple probability survey.</p>
<p>A critical feature of this problem is that because the <span class="math inline">\(\xi_j\)</span> are known and <span class="math inline">\(p(x)\)</span> is known, the joint likelihood factors as <span class="math display">\[
p(y,x,r \mid \mu) = p(x)p(r\mid x) p(y \mid x, \mu) = p(r , x) p(y \mid x, \mu),
\]</span> so <span class="math inline">\(r\)</span> is ancillary<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> for <span class="math inline">\(\mu\)</span>.</p>
<p>The simplest classical estimator for <span class="math inline">\(\mathbb{E}(y)\)</span> is the Horvitz-Thompson estimator <span class="math display">\[
\bar{y}_\text{HT} = \frac{1}{n} \sum_{i=1}^n \frac{y_i}{\xi_{x_i}}.
\]</span> It’s easy to show that this is a <span class="math inline">\(\sqrt{n}\)</span>-consistent estimator. Better yet, <em>uniform</em> over <span class="math inline">\(\mu\)</span> in the sense that the convergence of the estimator isn’t affected (to leading order) by the specific <span class="math inline">\(\mu_j\)</span> values. This uniformity is quite useful as it gives some hope of good finite-data behaviour.</p>
<p>So now that we know that the problem <em>can</em> be solved, let’s see if we can solve it in a Bayesian way. Robins and Ritov gave the following result.</p>
<blockquote class="blockquote">
<p>There is no uniformly consistent Baysesian estimator of the parameter <span class="math inline">\(\mu\)</span> unless the prior depends on the <span class="math inline">\(\xi_j\)</span> values.</p>
</blockquote>
<p>Robins and Ritov argue that a “committed subjective Bayesian” would, by the Likelihood Principle, never allow their prior to depend on the ancillary statistic <span class="math inline">\(\xi\)</span> as the Likelihood Principle clearly states that inference should be independent on ancillary information.</p>
<p>There are, of course, ways to construct priors that depend on the sampling probabilities. Wasserman calls this “frequentist chasing”</p>
<p>So let’s investigate this, by talking about what went wrong, how to fix it, and whether fixing it makes us bad Bayesians.</p>
</section>
</section>
<section id="the-likelihood-principle-and-the-death-of-nuance" class="level1">
<h1>The likelihood principle and the death of nuance</h1>
<p>So what is the likelihood principle and why is it being such a bastard to us poor liddle bayesians?</p>
<p>The likelihood principle says, roughly, that the all of the information needed for parameter inference<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> should be contained in the likelihood function.</p>
<p>In particular, if we follow the likelihood principle, then if we have two likelihoods that are scalar multiples of each other, our estimates of the parameters should be the same.</p>
<p>Ok. Sure.</p>
<p>Why on earth do people care about the likelihood principle? I guess it’s because they aren’t happy with the fact that Bayesian methods actually work in practice and instead want to do some extremely boring philosophy-ish stuff to “prove” the superiority and purity of Bayesian methods. And you know all power<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> to them. Your kink is not my kink.</p>
<p>In this context, it means that because <span class="math inline">\(r\)</span> is ancillary to <span class="math inline">\(y\)</span> for estimating <span class="math inline">\(\mu\)</span> we should avoid using the <span class="math inline">\(r_i\)</span>s (and the <span class="math inline">\(\xi_j\)</span>s) to estimate <span class="math inline">\(\mu\)</span>. This is in direct opposition to what the Horvitz-Thompson estimator uses.</p>
<p>What happens if we follow this principle? We get a bad estimate.</p>
<p>It’s pretty easy to see that the posterior mean will, eventually, converge to the true value. All that has to happen is you need to see enough observations in each category. So if you get enough data, you will eventually get a good estimate.</p>
<p>Unfortunately, when <span class="math inline">\(J\)</span> is large, this will potentially take a very very long<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> time.</p>
<p>Let’s go a bit deeper and see why this behaviour is not wrong, <em>per se</em>, it’s just Bayesian.</p>
<p>Bayesian inference produces a posterior distribution, which is conditional on an observed sample. This posterior distribution is an update to the prior that describes how compatible different parameter configurations are with the observed sample.</p>
<p>The thing is, our sample only sees a small sample of the values of <span class="math inline">\(x\)</span>. This means that we are, essentially, estimating <span class="math display">\[
\mathbb{E}_x (\mathbb{E}(y \mid x) 1_{x \in A_{r}} \mid r),
\]</span> where <span class="math inline">\(A_r\)</span> is the set observed values of <span class="math inline">\(x\)</span>, which depends on <span class="math inline">\(r\)</span>. This target changes as we get more data and see more levels of <span class="math inline">\(x\)</span> and eventually coalesces towards the thing we are trying to compute.</p>
<p>But, and this is critical, we <em>cannot</em> say <em>anything</em> about <span class="math inline">\(\mu_j\)</span> for <span class="math inline">\(j \not \in A_r\)</span> unless we can assume that they are, in some sense, very strongly related. Unfortunately, the whole point of this example is that we are not allowed<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> to assume that!</p>
<p>In this extremely flexible model, it’s possible to have a sequence <span class="math inline">\(\xi_j\)</span> that is highly correlated<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> with <span class="math inline">\(\mu_j\)</span>. If, for instance, <span class="math inline">\(\operatorname{expit}(\mu_j) = \xi_j\)</span> were<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> equally spaced on <span class="math inline">\([\delta, 1-\delta]\)</span> for some small <span class="math inline">\(\delta&gt;0\)</span>, you would have the situation where you are very likely to see the largest values of <span class="math inline">\(y\)</span> and quite unlikely to see any of the smaller values. This would gravely bias your sample mean upwards.</p>
<p>This construction is the basis similar to the one that Robins and Ritov use to prove that there is always at parameter value where the posterior mean converges<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> to the true mean at a rate no faster than <span class="math inline">\(\mathcal{O}((\log \log n)^2 \log n)\)</span>, which would require an exponentially large number of samples to do any sort of inference.</p>
<p>A reasonable criticism of this argument is that surely most problems will not have strong correlation between the sampling probabilities and the conditional means. In a follow up paper, <a href="https://projecteuclid.org/journals/statistical-science/volume-29/issue-4/The-Bayesian-Analysis-of-Complex-High-Dimensional-Models--Can/10.1214/14-STS483.full">Ritov <em>et al.</em></a> argue that it’s not necessarily all that rare. For instance, if they are both realisations of independent GPs<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> the empirical correlation between the two observed sequences can be far from zero! Less abstractly, it’s pretty easy to imagine something that is more popular with old people (who often answer their phones) than with young people (who don’t typically answer their phones). So this type of adversarial correlation certainly can happen in practice.</p>
</section>
<section id="can-we-save-bayes" class="level1">
<h1>Can we save Bayes?</h1>
<p>No.</p>
<p>Bayes does not need to be saved. She is doing exactly what it set out to do and is living her best life. Do not interfere<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>.</p>
<p>So let’s look at why we don’t need to fix things.</p>
<section id="a-simple-posterior-and-its-post-processing" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-posterior-and-its-post-processing">A simple posterior and its post-processing</h2>
<p>Once again, recall the setting: we are observing the triple<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> <span class="math display">\[
z_i = (x_i,r_i,y_i) = (x_i, r_i, \texttt{r[i]==1? y[i]: NA}).
\]</span> In particular, we can process this data to get some quantities:</p>
<ul>
<li><span class="math inline">\(N\)</span>: The total sample size</li>
<li><span class="math inline">\(n= \sum_{i=1}^N r_i\)</span>: The number of observed <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(N_j = \sum_{i=1}^N 1_{x_i = j}\)</span>: The total number of times group <span class="math inline">\(j\)</span> was sampled</li>
<li><span class="math inline">\(n_j = \sum_{i=1}^N r_i1_{x_i = j}\)</span>: The number of times an observation from group <span class="math inline">\(j\)</span> was recorded.</li>
</ul>
<p>Because of the structure of the problem, most observed values of <span class="math inline">\(N_j\)</span> and <span class="math inline">\(n_j\)</span> will be zero or one.</p>
<p>Nevertheless, we persist.</p>
<p>We now need priors on the <span class="math inline">\(\mu_j\)</span>. There are probably a tonne of options here, but I’m going to go with the simplest one, which is just to make them iid <span class="math inline">\(N(0, \tau^2)\)</span> for some fixed and known value <span class="math inline">\(\tau\)</span>. We can then fit the resulting model and get the posterior for each <span class="math inline">\(\mu_j\)</span>. Note that because of the data sparsity, most of the posteriors will just be the same as the prior.</p>
<p>Then we can ask ourselves a much more Bayesian question: What would the average in our sample have been if we had recorded every <span class="math inline">\(y_i\)</span>? Our best estimate of that quantity is <span class="math display">\[
\frac{1}{N}\sum_{j=1}^J N_j \mu_j
\]</span></p>
<p>That’s all well and good. And, again, if I had small enough <span class="math inline">\(J\)</span> or large enough <span class="math inline">\(N\)</span> that I had a good estimate for all of the <span class="math inline">\(\mu_j\)</span>, this would be a good estimate. Moreover, for finite data this is likely to be a much better estimator than <span class="math inline">\(J^{-1}\sum_{j=1}^J \mu_j\)</span> as it at least partially corrects for any potential imbalance in the covariate sampling.</p>
<p>It’s also worth noting here that there is nothing “Bayesian” about this. I am simply taking the knowledge I have from the sample I observed and processing the posterior to compute a quantity that I am interested in.</p>
<p>But, of course, that isn’t actually the quantity that I’m interested in. I’m interested in that quantity averaged over realisations of <span class="math inline">\(r\)</span>. We can compute this if we can quantify the effect that <span class="math inline">\(n_j\)</span> has on <span class="math inline">\(\mu_j\)</span>.</p>
<p>We can do this pretty easily. Our priors are iid<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>, so this decouples into <span class="math inline">\(J\)</span> independent normal-normal models.</p>
<p>For any <span class="math inline">\(j\)</span>, denote <span class="math inline">\(y^{(j)}\)</span> as the subset of <span class="math inline">\(y\)</span> that are in category <span class="math inline">\(j\)</span>. We have that<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> <span class="math display">\[\begin{align*}
p(\mu_j \mid y) &amp;\propto \exp\left(-\frac{1}{2}\sum_{i=1}^{n_j}(y^{(j)}_i - \mu_j)^2 - \frac{1}{2\tau^2}\mu_j^2\right)\\
&amp;\propto \exp\left[-\frac{1}{2}\left(\frac{1}{\tau} + n_j\right)\mu_j^2 + \mu_j\sum_{i=1}^{n_j}y_i^{(j)}\right].
\end{align*}\]</span></p>
<p>If we expand the density for a <span class="math inline">\(\mu_j \mid y \sim N(m,v^2)\)</span> we get <span class="math display">\[
p(\mu_j \mid y) \propto \exp\left(-\frac{1}{2v^2}\mu_j^2 + \frac{1}{v^2}m\mu_j\right).
\]</span> Matching terms in these two expressions we get that <span class="math display">\[
v_j^\text{post} = \operatorname{Var}(\mu_j \mid y, n_j) =  \frac{1}{n_j + \tau^{-2}},
\]</span> while the posterior mean is <span class="math display">\[
m_j^\text{post} = \mathbb{E}(\mu_j \mid y, n_j) = \frac{1}{n_j + \tau^{-2}}\sum_{i=1}^{n_j}y_i^{(j)},
\]</span> where I’ve suppressed the dependence on the sample <span class="math inline">\(y\)</span> in the <span class="math inline">\(m_j\)</span> and <span class="math inline">\(v_j\)</span> notation because, as a true<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> Bayesian, my sample is fixed and known. Hence <span class="math display">\[
\mu_j \mid y \sim N(m_j^{\text{post}}, v_j^{\text{post}}).
\]</span></p>
<p>Then I get the following estimator for the mean of the complete sample <span class="math display">\[
\mathbb{E}\left(\frac{1}{N}\sum_{j=1}^JN_j\mu_j \mid y \right)= \frac{1}{N}\sum_{j=1}^JN_jm_j^\text{post}.
\]</span> We can also compute the posterior variance<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> <span class="math display">\[
\operatorname{Var}\left(\frac{1}{N}\sum_{j=1}^JN_j\mu_j \mid y \right)=\sum_{j=1}^J\frac{N_j^2}{N^2}v_j^\text{post}.
\]</span> Note that most of the groups won’t have a corresponding observation, so, recalling that <span class="math inline">\(A_r\)</span> is the set of <span class="math inline">\(j\)</span>s that have been updated in the sample, we get <span class="math display">\[
\operatorname{Var}\left(\frac{1}{N}\sum_{j=1}^JN_j\mu_j \mid y \right)=\sum_{j\in A_r}\frac{N_j^2}{N^2}v_j^\text{post} + \tau^2\sum_{j \not \in A_r}\frac{N_j^2}{N^2},
\]</span> where the term that multiplies <span class="math inline">\(\tau^2\)</span> is less than 1.</p>
<p>So that’s all well and good, but that isn’t really the thing we were trying to estimate. We are actually interested in estimating the population mean, which we will get if we let <span class="math inline">\(N\rightarrow \infty\)</span>.</p>
<p>So let’s see if we can do this without violating any of the universally agreed upon sacred strictures of Bayes.</p>
</section>
<section id="modelling-the-effect-of-the-ancillary-coin" class="level2">
<h2 class="anchored" data-anchor-id="modelling-the-effect-of-the-ancillary-coin">Modelling the effect of the ancillary coin</h2>
<p>Here’s the thing, though. We have computed our posterior distributions <span class="math inline">\(p(\mu_j \mid y)\)</span> and we can now use them as a generative model<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> for our data. We also have the composition of the complete data set (the <span class="math inline">\(N_j\)</span>s) and full knowledge about how a new sample of the <span class="math inline">\(n_j\)</span>s would come into our world.</p>
<p>We can put these things together! And that’s not in anyway violating our Bayesian oaths! We are simply using our totally legally obtained posterior distribution to compute things. We are still true committed<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> subjective Bayesians.</p>
<p>So we are going to ask ourselves a simple question. Imagine, for a given <span class="math inline">\(N_j\)</span>, we have <span class="math inline">\(n_j \sim \text{Binom}(N_j, \xi_j)\)</span> iid samples<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> <span class="math display">\[
\tilde{y}^{(j)}_i \sim N(m_j^\text{post}, v_j^\text{post} + 1).
\]</span> What is the posterior mean <span class="math inline">\(\mathbb{E}(\mu_j \mid \tilde{y}^{(j)}, N_j)\)</span>? In fact, because this is random data drawn from a hypothetical sample, we can (and should<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>) ask questions about its distribution! To be brutally francis with you, I am too lazy to work out the variance of the posterior mean. So I’m just going to look at the mean of the posterior mean.</p>
<p>First things first, we need to look at the (average) posterior for <span class="math inline">\(\mu_j\)</span> when <span class="math inline">\(n_j = n\)</span>. The exact calculation we did before gives us <span class="math display">\[
m_j(n) = \left(1-\frac{1}{\tau^2n + 1}\right) m_j^\text{post}.
\]</span> And, while I said I wasn’t going to focus on the variance, it’s easy enough to write down as <span class="math display">\[
v_j(n) = \frac{1}{n + \tau^{-2}} + \left(1 - \frac{1}{\tau^2n + 1}\right)(1 + v^\text{post}_j),
\]</span> where the second term takes into account the variance due to the imputation.</p>
<p>With this, we can estimate sample mean for any number <span class="math inline">\(\tilde N\)</span> and any set of <span class="math inline">\(\tilde N_j\)</span> that sum to <span class="math inline">\(\tilde N\)</span> and any set of <span class="math inline">\(\tilde n_j \sim \text{Binom}(\tilde N_j, \xi_j)\)</span> as <span class="math display">\[\begin{align*}
\frac{1}{\tilde N}\sum_{j=1}^J \tilde N_j m_j(n_j) &amp;= \frac{1}{\tilde N}\sum_{j=1}^J \frac{\tilde N_j}{\tilde n_j} \tilde n_j \tilde m_j(n_j) \\
&amp;= \frac{1}{\tilde N}\sum_{j=1}^J \frac{1}{\xi_j} \tilde n_j m_j^\text{post} + o(1),
\end{align*}\]</span> where in the last line I’ve used the fact that the empirical proportion converges to <span class="math inline">\(\xi_j\)</span> and the posterior mean converges to <span class="math inline">\(m_j^\text{post}\)</span>. The little-o<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> error term is as <span class="math inline">\(\tilde N\)</span> (and hence <span class="math inline">\(\tilde N_j\)</span> and <span class="math inline">\(\tilde n_j\)</span>) goes to infinity.</p>
<p>To turn this into a practical estimate, we can plug in our values of <span class="math inline">\(n_j\)</span> and <span class="math inline">\(N\)</span> to get our Bayesian approximation to the population mean <span class="math display">\[\begin{align*}
\hat \mu &amp;= \frac{1}{N}\sum_{j=1}^J \frac{n_j}{\xi_j}m_j^{\text{post}} \\
&amp;=\frac{1}{N} \sum_{j \in A_r} \frac{n_j}{\xi_j}m_j^\text{post} \\
&amp;=\frac{1}{N}\sum_{j=1}^J\sum_{i=1}^{n_j} \frac{1}{\xi_j}\left(1 - \frac{\tau^{-2}}{n_j}\right)y_i^{(j)},
\end{align*}\]</span> which is (up to the small term in brackets) the Horvitz-Thompson estimator!</p>
</section>
<section id="is-it-bayesian" class="level2">
<h2 class="anchored" data-anchor-id="is-it-bayesian">Is it Bayesian?</h2>
<p>I stress, again, that there is nothing inherently non-Bayesian about this derivation. Except possibly the question that it is asking. What I did was compute the posterior distribution and then I took it seriously and used it to compute a quantity of interest.</p>
<p>The only oddity is that the quantity of interest (the population mean) has a slightly awkward link to the observed sample. Hence, I estimated something that had a more direct link to the population mean: the sample mean of the completely observed sample under different realisations of the randomisation <span class="math inline">\(r_i\)</span>.</p>
<p>In order to estimate the sample mean under different realisations of the randomisation, I needed to use the posterior predictive distribution to impute these fictional samples. I then averaged over the imputed samples and sent the sample size to infinity to get an estimator<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>.</p>
<p>Or, to put it differently, I used Bayes to get a posterior estimate for new data <span class="math display">\[
p(\tilde y, \tilde r, \tilde x) = \int_{\mathbb{R}^J}p(\tilde y \mid \tilde x, \mu)\,d\mu p(\tilde r \mid \tilde x) p(\tilde x)
\]</span> and then used this probabilistic model to estimate <span class="math inline">\(\mathbb{E}(\tilde y)\)</span>. There was no reason to use Bayesian methods to do this. Non-Bayesian questions do not invite Bayesian answers.</p>
<p>Now, would I go to all of this effort in real life? Probably not. And in the applications that I’ve come across, I’ve never had to. I’ve done a bunch of MRP<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a>, which is structurally quite similar to this problem except we can reasonably model the dependence structure between the <span class="math inline">\(\mu_j\)</span>s. <a href="https://arxiv.org/abs/1908.06716">This paper</a> I wrote with Alex Gao, Lauren Kennedy, and Andrew Gelman is an example of the type of modelling you can do.</p>
</section>
</section>
<section id="is-it-true-am-i-a-chaser" class="level1">
<h1>Is it true? Am I a chaser?</h1>
<p>Wasserman derides “frequentist chasing” Bayesians, making the point that if they want a frequentist guarantee so badly, why not just do it the easy way.</p>
<p>Now. Laz. Mate.</p>
<p>Let me tell you that a lot of my self esteem has been traditionally gathered from chasers, so I absolutely refuse to be party to the slander.</p>
<p>But more than that, let’s be clear. Bayes is a way to probabilistically describe data. That is not enough in and of itself to be useful. For it to be useful, we need to <em>do something</em> with that posterior distribution.</p>
<p>So really, let’s talk about what a <em>true committed subjective Bayesian</em> does about this. Firstly, I mean really. There is no such thing<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a>. But leaving that aside, the closest I can get to a working definition is that a true committed subjective Bayesian is a person who understands that parameters are polite fictions that are used to describe the data. They are not, inherently, linked to any population quantity (for a true committed subjective Bayesian, such a thing does not exist).</p>
<p>The <em>only</em> way to link parameters in a Bayesian model to a population quantity of interest is to use some sort of extra-Bayesian<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> information.</p>
<p>For instance, in the first example (the one without the ancillary coin), I made that link in secret using assumptions about the sample. We all know that those types of assumptions are fraught and the reason that people spend so much time whispering DAG into the ears of their sleeping lovers.</p>
<p>For the ancillary coin example, we used the given information about the sampling mechanism as our extra information to link our posterior distribution to the population quantity of interest. None of this changes the <em>purity</em><a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> of the Bayesian analysis. Or makes a non-Bayesian solution preferable. (Although, in this case, a non-Bayesian solution is a fuckload easier to come up with.)</p>
<p>Of course Wasserman (and I presume Robins and Ritov) know all of this. But it’s fun to write it all down.</p>
<p>Moreover, I think that the three lessons here are fairly transferable:</p>
<ol type="1">
<li>If you’re going to go to the trouble of computing a posterior, take it seriously. Use it to do things! You can even put it in as part of a probabilistic model.</li>
<li>If you’re going to make Bayes work for you, think in terms of observables (eg the mean of the complete sample) rather than parameters.</li>
<li>Appeals to purity are a bit of a waste of time.</li>
</ol>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Huge thanks to Sameer Deshpande for great comments!<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I first came across this in a <a href="https://normaldeviate.wordpress.com/2012/10/11/the-robins-ritov-example-a-post-mortem/">series of posts</a> on Larry Wasserman’s now defunct but quite excellent blog.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>It’s worth saying that these three people do fabulous statistics of the form that I don’t usually do. But that doesn’t make it less important to understand their contributions. You could say that while I am not a Lazbian, I think it’s important to know the theory.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>I might have slightly reworded it.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Purity is needed in good olive oil and that’s it<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>A committed subjective Bayesian prefers Dutch baby to a Dutch book.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>A true committed subjective Bayesian doesn’t wear anything under his kilt.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>That is, an estimator where <span class="math inline">\(\Pr(|\hat \mu_n - \mu| &gt; \sqrt{n}\epsilon) \rightarrow 0\)</span> for all <span class="math inline">\(\epsilon&gt;0\)</span>. This, roughly, means, that you can find a <span class="math inline">\(C\)</span> such that <span class="math inline">\(\mu \in [ \hat \mu_n - C\sqrt{n}, \hat \mu_n + C\sqrt{n}]\)</span> with high probability.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The asymptotics say that we should count our data in multiples of <span class="math inline">\(J\)</span>, so we’d <span class="math inline">\(n &gt; 100J\)</span> to get even one decimal place of accuracy.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Remember <span class="math inline">\(\mu_j = \mathbb{E}(y \mid x=j)\)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Theorem 2 of <a href="https://argmin.lis.tu-berlin.de/papers/07-harmeling-tr.pdf">Harmeling and Toussaint</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>a<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>If you’ve not come across it, <em>ancillary</em> is the term used for parts of the data that don’t influence parameter estimates. It’s the opposite of a sufficient statistic. One way to see that it’s ancillary for <em>any</em> model <span class="math inline">\(p(y\mid x, \theta)\)</span>, is to consider the log of the joint density <span class="math display">\[
\log(p(x,y,r \mid \theta)) = \log p(y\mid x, \theta) + \log p(r \mid x) + \log p(x)
\]</span>, where the last two terms are constant in <span class="math inline">\(\theta\)</span>.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>You need to be specific here. Obviously this would be false if you were trying to do a statistical prediction. Or if you were trying to make a decision. Those things necessarily depend on extra stuff!<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>This is a lie. Insisting on talking about this shit rather than actually making Bayes useful and using it in new and exciting ways to do things that are hard to do without Bayesian methods is a waste of time. Worse than that, when you start pretending your method of choice is the only possible thing that a sensible and principled person would use, you start to look like a bit of a dickhead. It also turns people off trying these very flexible and useful methods. So yeah. I maybe do care a little bit. <a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>The expected number of samples to see one draw where <span class="math inline">\(x_i =j\)</span> is <span class="math inline">\(J\)</span>. The expected number of draws where <span class="math inline">\(x_i = j\)</span> that you need to actually observe the corresponding <span class="math inline">\(y_i\)</span> is <span class="math inline">\(\xi_j^{-1}\)</span>. This suggests it will potentially take <em>a lot</em> of draws to even have effectively one sample from each category, let alone the 20-100 you’d need to, practically, get some sort of reasonable estimate.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>Robins and Ritov have always been open that if there is a true parametric model for the <span class="math inline">\(\mathbb{E}(Y \mid x = j)\)</span> (or if that function is “very smooth” in some technical sense, eg a realisation of a smooth Gaussian process) then the Bayesian estimator that incorporates this information will do perfectly well. <a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>So the RR example uses binary data, so then it’s the correlation between <span class="math inline">\(\mathbb{E}(y \mid x=j)\)</span> and <span class="math inline">\(\xi_j\)</span>, but the exact same argument works if <span class="math inline">\(\xi_j\)</span> is correlated something like <span class="math inline">\(\operatorname{expit}(\mu_j)\)</span>. I went with the Gaussian version because at one point I thought I might end up having to derive posteriors and I’m all about simplicity.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>expit is the inverse of the logit transform<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>Check <a href="https://cdn1.sph.harvard.edu/wp-content/uploads/sites/343/2013/03/coda.pdf">the paper</a> for the details as the situation is slightly different to the one I’m sketching out here, but there’s no real substantive difference.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>Of course, if this were true we could use a GP prior for the <span class="math inline">\(\mu_j\)</span>s and we’d probably get a decent estimator anyway.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>If you want to interfere, there are plenty of ways to build priors that incorporate the <span class="math inline">\(\xi_j\)</span> information. The <a href="https://projecteuclid.org/journals/statistical-science/volume-29/issue-4/The-Bayesian-Analysis-of-Complex-High-Dimensional-Models--Can/10.1214/14-STS483.full">Ritov etc paper</a> has nice references to the various things that sprung up from this example. Are these useful beyond simply making sure the posterior mean of <span class="math inline">\(\mu\)</span> estimates <span class="math inline">\(\mathbb{E}(y)\)</span>? Not really. They are priors designed to solve exactly one problem.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>I’m using the C/C++ ternary operator. In R this would be parsed as <code>ifelse(r[i] == 1, y[i], NA)</code>. <a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>Not exchangeable—there are no shared parameters!<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>Remember that <span class="math inline">\(y \mid x = j \sim N(\mu_j, 1)\)</span>. If we wanted a more flexible variance, we could obviously have one, but it makes not real difference to anything.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>I promise I’m just rolling my eyes to see if I can see my brain.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>Remember everything is independent!<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>This is the posterior predictive distribution!<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>A true committed subjective Bayesian knows that DP stands for Dirichlet Process. No matter the context.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>The variance is <span class="math inline">\(v_j^\text{post} + 1\)</span> because this is the posterior predictive distribution.<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>Does this seem like a frequentist question? I guess. But really it’s a question we can always ask about the posterior. Should we? Well if you are trying to estimate a population quantity you sort of have to. Because there isn’t really a concept of a population parameter within a Bayesian framework (true committed subjective or otherwise).<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p>Remember that this means that the error (which is a random variable) goes to 0 as <span class="math inline">\(n\rightarrow \infty\)</span>. A more careful person could probably work out how fast it would happen.<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p>I only computed the mean, so feel free to pretend that I’m minimizing a loss function<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p>Multilevel regression with poststratification, a survey modelling technique<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p>No true Scotsman etc<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p>or meta-Bayesian in the event that we are doing things like building a Bayesian pseudo-model of on the space of all considered model that just happens to give every model equal probability because Harold Fucking Jeffreys gave you an erection and you could either process that event like an adult or build a whole personality around it. And you chose the latter.<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37"><p>Can you tell that I hate this entire discussion?<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2022,
  author = {Simpson, Dan},
  title = {On That Example of {Robins} and {Ritov;} or {A} Sleeping Dog
    in Harbor Is Safe, but That’s Not What Sleeping Dogs Are For},
  date = {2022-11-15},
  url = {https://dansblog.netlify.app/posts/2022-11-12-robins-ritov/robins-ritov.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2022" class="csl-entry quarto-appendix-citeas" role="listitem">
Simpson, Dan. 2022. <span>“On That Example of Robins and Ritov; or A
Sleeping Dog in Harbor Is Safe, but That’s Not What Sleeping Dogs Are
For.”</span> November 15, 2022. <a href="https://dansblog.netlify.app/posts/2022-11-12-robins-ritov/robins-ritov.html">https://dansblog.netlify.app/posts/2022-11-12-robins-ritov/robins-ritov.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dansblog\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>