---
title: "Getting fast linear mixed models into Stan; or I'm ok with C++ but C++ isn't ok with me."
description: "Sure. You've been saying you were going to do this for year"
  
date: 2024-06-05
image: "scooby.JPG"
categories: [Stan, Sparse matrices, Autodiff]
twitter-card:
  title: "Getting fast linear mixed models into Stan"
  creator: "@dan_p_simpson"
citation: 
  url: https://dansblog.netlify.app/posts/2024-05-08-laplace/laplace.html
format: 
  html:
    df-print: paged
jupyter: python3
draft: true
execute:
  eval: true

---

Sometimes when you want something done you need to do it yourself. And sometimes
you are unemployed and need something to do that isn't interviewing for jobs.
And sure. Something that demonstrates my LLM prowess or my MLE capabilities 
would be a sensible thing to do. But fuck that. This blog is not for the sensible.
This blog is for whims and regrets^[and unnecessary gay shit.].

And it is my whim to clean up one of my regrets: that I never got fast linear
mixed models^[Gaussian Multilevel Models.] into Stan. 

There a pile of reasons for that. It's a fuck-tonne of work that requires someone
to actually go down the sparse matrix mines. Furthermore, it's gotta be done
in modern C++, which I think we can all agree is not a language that one casusually
codes in only on occasion. C++ is a lifestyle. It is a calling. 

But thankfully I am old as shit^[Actually I got a discounted "youth ticket" the other night because I'm not yet 40. So maybe I'm not ready to star in the remake of one foot in the grave.]
and I have a PhD in numerical linear algebra. So I have more than a passing, if
rusty familiarity with C++.

Over the years, I've contributed occasionally to the Stan codebase, but that was
never my core contribution---I was an algorithm and methods man. Also the amount
of time it would take me to add a feautre like sparse matrices was impossible to
justify when I was an academic^[vs the reward] or in industry^[vs the weekend].
But, as I said before, I am _unemployed_ and don't have to justify my time to 
anyone. So let's fucking do this.

## What do we want from this?
First of all, we need to set up some guidelines. I know that the vibe of this 
blog is jsut me writing text until I'm sick of it, which is (I am told), a lot 
later than most readers are sick of it. But this is actually a piece of 
software I want people to use, so I'm going to have at least some
structure before I get fully on my bullshit.

So let's look at what we want.

1. There should be functionality equivalent, at least, to the linear mixed 
effects models implemented in the R package `lme4`.
This is a much lower bar than trying to match the various extensions to the basic
formula implemented in things like `brms` or `INLA`. This should hopefully make 
the scope reasonable. Future extensions can be left for the future.

1. This should pre-compute as many of the common operations as possible. For 
example, when there is a lot of data, it does not make sense to compute $X^TX$
at every iteration of the dynamic HMC sampler. Unfortunately, the fundamental
design decisions^[In keeping the code stateless we remove the possiblity for a pile of annoying and subtle bugs. So this is a good thing, even when it is inconvenient!] underlying Stan 
do not let us cache information from iterations. Hence, we are going to need
to pass all of these static quantities to the function. This _will_ complicate
the signature!

1. We want the function signature and the code to be as useable and maintainable
as possible. Hence, we may want to deal with a simplified model rather than the
most general possible case. It is better to have useable code that covers 90% of 
the cases than general code that is such a mess people simply want to cry.

With these things in mind, our first trick will be to work out exactly how we want 
to formulate this problem. It's going to turn out that our first attempt is pretty 
quickly going to break our third rule, so we will go with a slightly more restrictive
model that still covers the majority of use cases.



## Attempt 1: A general formualtion



Taking a leaf out of the Stan functions that are already built to speed up
linear regression (`normal_id_lm_lpdf`), I'm going to go with the following 
eventual Stan syntax
```{stan}
y ~ normal_id_lmm_lpdf(alpha, beta, X, Z, W, \Lambda, sigma);
```
to fit the model
\begin{align*}
y \mid \alpha, \beta,u,\sigma &\sim N(\alpha + X\beta + Zu, \sigma^2 W)\\
u \mid \Lambda &\sim N(0, \Lambda \Lambda^T).
\end{align*}
This is the same model (and the same notation) as the R package `lme4`, which is
the gold standard for non-Bayesian GLMMs. If you want to dive into, eg, what 
$Z$ is, [this paper](https://www.jstatsoft.org/article/view/v067i01) is an excelelnt resource.

(The canny reader might notice that I have lazily introduced broadcast semantics to make sure that eg the scalar $\alpha$ is interpereted as $\alpha 1$, where $1$ is a vector of 1s.)

The user has free choice for priors on $\beta$, $\Lambda$ 
and^[Note that $\sigma$ is a scalar, which is different to the usual Stan specification, where it can be a vector.] $\sigma$.

There is one thing that you might notice above: I am  explicitly assuming that $X$,
$Z$, and $W$ are _constants_. This is in contrast to the usual Stan thing where
anything to the right of `~` can be a parameter. There are a bunch of reasons for 
this, but the main one is that I want to make this as fast as possible. And if 
those matrices are fixed I can do lots of sick tricks.

Just to close out, I want to zoom in on one more assumtion. I am assumeing that 
we are passed the Cholesky factorisation of the covariance matrix of $u$. This is
a good formulation for mixed effects models (and also aligns with the output
you can get from `lme4` in R). It is not, however, the right formulation for spatial or temporal
models, where you often want to specify the precision matrix^[For Markovian models, this is sparse, which allows for fast linear algbra.] of $u$ instead. As much as I love it, I am shelving that application
for the moment in favour of the more commonly used one.

### Marginalizing out $u$

The way that we make this computationally efficient is to marginalise out the vector
of random effects $u$. This can be done efficiently using sparse linear algebra
and the resulting marginal posterior 
$$
p(\beta, \theta \mid y)
$$
is both lower-dimensional that the full posterior $p(u,\beta,\theta\mid y)$ and 
it has a considerably nicer geometry. This means that MCMC methods will be easier
to tune and faster to run on the marginal posterior compared to the full posterior.

Furthermore, given a sample $(\beta_i, \theta_i)$ from the marginal posterior,
we can sample 
$$
u_i \sim N(\mu(\beta_i, \theta_i), Q(\beta_i, \theta_i)^{-1})
$$
to recover the corresponding sample $(u_i, \beta_i, \theta_i)$ from the full
posterior. This conditional distriubiton is multivariate Gaussian and can be sampled
efficently.

So let's compute these things.

The core thing that any true Bayesian needs to know is that constants are 
not our problem. So lets dump them and write the joint distribuiton for the 
data and the parameters as 
\begin{multline}
\log p(y, \alpha, \beta, u, \Lambda,\sigma) = K - N\log \sigma\\ - \frac{1}{2\sigma^2} (y-\alpha-X\beta-Zu)^TW(y-\alpha-X\beta-Zu)\\
- \frac{1}{2}u^T(\Lambda\Lambda^T)^{-1}u + \log p(\alpha, \beta, \Lambda,\sigma),
\end{multline}
where $K$ is our unimportant constant term and $N$ is the number of observations. I will be using $K$ for every constant
because there is truly no point^[If you're a crypto-frequentist who needs to compute a Bayes factor, do it on your own time.] in tracking them. 

### Computing $p(u \mid \alpha, \beta, \Lambda, \sigma, y)$

This joint distriubiton contains all of our information.
We can get conditional distributions by just treating any
term on the RHS of the conditioning bar as a constant. 
For example,
$$
\log p(u \mid y, \alpha, \beta, \sigma ) = K - \frac{1}{2}u^T(\sigma^{-2}Z^TWZ + (\Lambda\Lambda^T)^{-1})u + \frac{1}{\sigma^2}(y - \alpha-X\beta)^TWZu.
$$

If you've been doing this for long enough (or if you read the last post where
I did this calculation), you'll see that this is normal distribution with mean
$$
(\sigma^{-2}Z^TWZ + (\Lambda\Lambda^T)^{-1})^{-1}Z^TW(y - \alpha - X\beta)
$$
covariance matrix 
$$
(\sigma^{-2}Z^TWZ + (\Lambda\Lambda^T)^{-1})^{-1}.
$$

It's probably useful to clean up that covariance matrix bit to remove that 
inverse-of-an-inverse bullshit. The easiest way is to write it as 
$$
\sigma^2\Lambda(\Lambda^TZ^TWZ\Lambda + \sigma^2 I)^{-1}\Lambda^T.
$$

The canny amongst you will notice that this is the covariance
matrix of $u = \Lambda w$,
where 
$$
w \sim N\left((\Lambda^TZ^TWZ\Lambda + I)^{-1}\Lambda^TZ^TW(y - \alpha - X\beta), \sigma^2 (\Lambda^TZ^TWZ\Lambda + \sigma^2 I)^{-1}\right).
$$

Looking at all of this, it's pretty clear that it's a lot nicer computationally to work with $w$ rather than $u$, which is
equivalent to writing the model as
$$
y \mid \alpha,\beta, w \sim N(\alpha + X\beta + Z\Lambda w, \sigma^2 W^{-1}),
$$
where $w\sim N(0,I)$.

This reformulation is a consequence of how the random effect has 
been parameterized. If it was parameterized with its precsion
 matrix instead of its covariance, we wouldn't need to do this.

### Computing $p(\alpha, \beta, \Lambda, \sigma \mid y)$

So now we've got to do an integral. Luckliy, it's Gaussian so we can do it. 
The trick with this part of the computation is to express the full conditional
as 
$$
\log p(u,\theta \mid y) = K - \frac{1}{2}(u - m)^T Q (u - m) + f(\theta),
$$
where I have defined $\theta = (\alpha, \beta, \sigma, \Lambda)$ because frankly I 
got sick of typing, $f(\theta)$ is all of the non-constant terms that don't involve $u$,
  $m$ is some vector, and $Q$ is some precision matrix. With this in hand,
we can integrate out $u$ directly and get 
$$
\log p(u,\theta \mid y) = K + \frac{1}{2}\log |Q| + \log p(\theta),
$$
by dint of the Gaussian integral being the only multivaraite integral I remember.

So how do we find this mystical $(m, Q)$? Well the godo news is that we have 
technically already done this to get the conditional distribution. 
Behind the scenes I added and subtracted
$$
\frac{1}{2}\mu_{u \mid y, \theta}^TQ_{u\mid y,\theta}\mu_{u \mid y, \theta},
$$
where 
$$
\mu_{u \mid y, \theta} = Q_{u\mid y,\theta}^{-1}Z^T(y-\alpha - X\beta) = \Lambda(\sigma^{-2}\Lambda^TZ^TWZ\Lambda + I)^{-1}(y-\alpha -X\beta)
$$ 
is the mean of $p(u \mid \theta, y)$, and 
$$
Q_{u\mid y,\theta} = \sigma^{-2}Z^TWZ + (\Lambda \Lambda^T)^{-1}.
$$

This means that if we drop all of the terms in the joint that depend on $u$ and add 
$$
\frac{1}{2}\log |\Lambda^TZ^TWZ\Lambda + I| - \log |
\Lambda| + \frac{1}{2} (y - \alpha - X\beta)^T\Lambda(\sigma^{-2}\Lambda^TZ^TWZ\Lambda + I)^{-1}\Lambda^T(y - \alpha - X\beta)
$$
to the leftovers, we will get the marginal distribution.



This leads to the marginal distribution
\begin{align*}
\log p(\alpha, \beta, \Lambda,\sigma \mid y) = K - N\log \sigma  
- \frac{1}{2\sigma^2} \alpha^T W \alpha  - \frac{1}{2\sigma^2} \beta^TX^TWX\beta&
\\ + \frac{1}{\sigma^2}y^TW(\alpha + X\beta) - \frac{1}{\sigma^2} \alpha^TWX\beta &\\
+ \frac{1}{2} (y-\alpha - X\beta)^T\Lambda(\sigma^{-2}\Lambda^TZ^T W Z\Lambda + I)^{-1}\Lambda^T(y - \alpha - X\beta)&
\\ + \frac{1}{2}\log|\Lambda^TZ^TZ\Lambda + I| - \log|\Lambda|
+ \frac{1}{2}\log |Q_{u\mid \alpha,\beta,y}| + \log p(\alpha, \beta, \Lambda, \sigma).&
\end{align*}

This is a fantastically ugly expression. We could clean it up a bit, but there's one thing that it fails
to do for us.

When doing any sort of linear regression, there computation of $X^TX$ and terms of its ilk is a potentially huge bottleneck, especially when the number of observations is large. Ideally, we would be able to pre-compute and pass all of the operations that involve $X$ so that we don't have to repeat these potentially expensive computations. Sadly this is impossible here as we still need $X\beta$. And even
if it wasn't, the number of things we would need to pass in is prohibative.

## Attempt 2: A less flexible model that's acutally useful

In
a [previous post](https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/a-linear-mixed-effects-model),
where I started off down this direction in JAX^[Didn't work. XLA isn't optimized for these sorts of methods.],
I defined a version of the problem where I have a data likelihood
$$
y \mid u, \sigma \sim N(Au, \Sigma),
$$
and $u$ was defined as 
$$
u \mid \theta \sim N(0, Q(\theta)^{-1}).
$$

This is a very computationally convenient way to write the problem, as it abstracts
away the regression term and the random effects term(s) into $A$  and $Q$, which
are block sparse matrices.


In this case, we do not distinguish between $\alpha$, $\beta$, and $u$ as they are all 
(conditionally) Gaussian. This means that we can just pretend $\alpha$ and $\beta$
are zero in the previous formulation and define^[The ordering is important her--[in a previous post](https://dansblog.netlify.app/posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices) I talked about how important it is to put the dense rows at the back!] 
$$
\tilde{Z} = (Z\vdots X), \tilde{u} = \begin{pmatrix} u \\ \beta \end{pmatrix}.
$$
Then we have the model
$$
y \sim N(\tilde{Z} \tilde{u}, \sigma^2 W^{-1}),
$$
where 
$$
\tilde{u} \sim N\left(0, \begin{pmatrix}\Lambda \Lambda^T &0 \\0 & \Sigma_\beta\end{pmatrix}\right).
$$

Unfortunately this is a) at a level of abstraction that is offputting to people
who are thinking of using the method rather than just coding them, and b) making
the key assumption that the priors on the regression coeffients are Gaussian 
(conditinal on $\theta$).

This is one of those things that isn't actually a huge loss of generality; the
set of probability densities given by 
$$
p(\beta) \propto \int_\Theta |\Sigma(\theta)|^{-1/2} \exp\left(-\frac{1}{2}\beta^T\Sigma(\theta)^{-1}\beta\right) \, p(\theta)\, d\theta
$$
is extremely expressive. So if you choose the correct $p(\theta)$ you can still
have basically any prior^[This is how eg the horseshoe prior is always implemented] on the regression parameters you want.



This greatly simplifies our previous calculation and we get, after a bit of 
coersion, that 
$$
\log p(y \mid \theta) = K  + \frac{1}{2}\log |Q_{\tilde{u}\mid y,\theta}(\theta)| + \frac{1}{2} y^TW\tilde{Z} \tilde{\mu}_{\tilde{u}\mid y,\theta} + \log p(\theta),
$$
where
$$
Q_{\tilde{u}\mid y,\theta}(\theta) = \tilde{Z}^TW\tilde{Z} = \begin{pmatrix}
\Lambda^TZ^TWZ\Lambda + I & \Lambda^T Z^T WX \\ X^TWZ\Lambda & X^TWX  + \Sigma_\beta^{-1}
\end{pmatrix}.
$$
The most important thing about this matrix is that all of the terms except $\Lambda$ and $\Sigma_\beta$ are constants and only need to be computed once.

The posterior mean for $\tilde{u} \mid y,\theta$ is given by the solution to 
$$
\begin{pmatrix}
\Lambda^TZ^TWZ\Lambda + I & \Lambda^T Z^T WX \\ X^TWZ\Lambda & X^TWX  + \Sigma_\beta^{-1}
\end{pmatrix} \tilde{\mu}_{\tilde{u}\mid y,\theta} = \begin{pmatrix} \Lambda^T Z^TWy \\ X^TWy\end{pmatrix}.
$$

As the notation suggests, 
$$
u \mid y, \theta \sim N(\mu_{\tilde{u}\mid y,\theta}(\theta), Q_{\tilde{u}\mid y,\theta}(\theta)^{-1}).
$$

Although they look quite different, both of these formulations are^[Unlike in non-Bayesian mixed effects modelling, where the order of marginalization, profiling, and maximization matter _a lot_, Bayesian coherence means that it's all describling joint distribution.] equivalent^[If you pretend I didn't shit-can the intercept.]. 

## C++ Plumbing: Building a block sparse matrix

The first thing that we need to do is build a block-sparse matrix. We know
that this matrix is symmetric so we only need to store the lower-triangle.

In general, this is not the most difficult task in the world. We have [already talked about how we store sparse matrices](https://dansblog.netlify.app/posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices#so-how-do-we-store-a-sparse-matrix) 
and, in particular, have had some fun with the Compressed Column Storage (CCS) 
scheme, which stores sparse matrices column-by-column. In the lingo, we call this
_column major_ storage.

When any array of numbers is stored in memory by a program, it is stored as 
a long vector and when you index into it (using something like `A[i,j]`) this
is just some syntactic sugar for finding the correct value in that long vector.

Some languages, such as Fortran and Matlab, and libraries, such as [Eigen](https://gitlab.com/libeigen/eigen), store
arrays in column major order. Others, like C/C++ and Python, use row-major storage.
Stan is written in C++ but all of its linear algebra is done using Eigen, so we are 
going to use column-major storage.

It may seem catastrophically nerdy to be talking about internal storage 
orders for arrays in different languages, but I promise you this is _incredibly_
important. If you want to write any sort of performant code, it's extremely 
important that your algorithms are aligned with the internal storage order.
That means that we need to prefer algorithms that run down columns of matrices
over ones that run across rows.

This is because computers are clever and when you ask them for, eg `A[0,0]`, the CPU 
will actually load the first few entries of the 0th _column_^[Or row if it's row major] of `A` in anticipation^[Let's anthopomorphise. I don't want to write a blog about caches.] 
that you will need `A[0,1]` and its friends next. If you instead next ask for 
`A[1,0]`, the CPU has to throw its pre-loaded stuff out, reach out to some potentially
distant memory and try again. When an array has a lot of rows, these cache misses^[Drag name: Cache Mx]
noticably degrade the performance of a program.


All of that is to say that this is actually not too too hard to implement
because we are just interleaving some contiguous chunks of a vector. While 
the main loop is pretty straightforward, C++ is truly a journey. So it's gonna 
be like 100 lines of code.

The structure is 

1. Allocate 3 arrays to store the outer index (which column?), the inner index (which row?), and the value.

2. Iterate through each column of the matrix, only storing the lower triangle.

3. Return an `Eigen::SparseMatrix<double>` built from those arrays.

There are essentially two challenges in doing this. Firstly, the number of 
columns and then number of non-zeros are not known at compile time so we need
to allocate dynamic memory on the heap. This is always a risky proposition in 
C++ as it's pretty easy to screw up and end up with a memory leak. To get around
this, I'm using the RAII (resource aquisition is instantiation) pattern, which
basically encapsulates all the memory usage inside a functor, who's call method
return a sparse symmetric matrix.

The second challenge is that the Eigen API demands raw pointers. So this is 
going to have that good old fashioned `*ptr++` action.

Without further ado, here is the code. I'll explain some key bits after.

```c++
#include <stan/math/prim/meta/is_eigen_sparse_base.hpp>
#include <stan/math/prim/meta/is_eigen.hpp>
#include <stan/math/prim/meta/is_stan_scalar.hpp>
#include <stan/math/prim/meta/base_type.hpp>
#include <stan/math/prim/err/check_size_match.hpp>
#include <stan/math/prim/fun/to_ref.hpp>
#include <Eigen/SparseCore>

namespace stan {
namespace math {

typedef Eigen::SparseMatrix<double>::StorageIndex StorageIndex;

// The require_ statements are defined in the first #include
template<typename SpMat, typename EigMat1, typename EigMat2, 
require_eigen_sparse_base_t<SpMat>* = nullptr,
require_all_eigen_t<EigMat1, EigMat2>* = nullptr,
require_all_stan_scalar_t<base_type_t<SpMat>,
                          base_type_t<EigMat1>,
                          base_type_t<EigMat2>>* = nullptr>  
class Block_sparse_lower {
    /* 
    A RAII functor class because Jesus hates memory leaks
    Make this encapsulate the whole thing.
    You may be asking why I'm using arrays and pointers
    like I'm writing in C, and the answer is 
    "that's the interface to Map". The dream of the 
    C-90 is alive and well in the eigen code base.

    Anyway, `operator ()` returns a sparseMatrixMap
    */
    using T = typename base_type<SpMat>::type;
   
    StorageIndex* m_outer;
    StorageIndex* m_inner;
    T* m_val;
    StorageIndex m_cols;
    StorageIndex m_nnz;

    public:

    Block_sparse_lower(
        const SpMat& top_left, 
        const EigMat1& bottom_left, 
        const EigMat2& bottom_right
        ) 
    {
        // only eval once
        const auto& tl_ref = to_ref(top_left);
        const auto& bl_ref = to_ref(bottom_left);
        const auto& br_ref = to_ref(bottom_right);

        // Get sizes.
        // NB tmp_nnz is an upper bound. Will only be correct if `top_left` is lower 
        // triangular. We will compute the real value on the fly.
        const StorageIndex ncols_tl = tl_ref.cols();
        const StorageIndex ncols_br = br_ref.cols();
        const StorageIndex tmp_nnz = (tl_ref.nonZeros() + ncols_tl * ncols_br 
                                        + (ncols_br + 1) * ncols_br / 2);

        // check sizes
        check_size_match("Block_sparse_lower", "Columns of ", "top_left ", tl_ref.cols(), "Columns of ", "Bottom Left", bl_ref.cols());
        check_size_match("Block_sparse_lower", "Rows of ", "bottom-left ", bl_ref.rows(), "Rows of ", "Bottom-right", br_ref.rows());
        
        // Allocate!
        m_cols = ncols_tl + ncols_br;

        m_outer = new StorageIndex[m_cols + 1];
        m_outer[0] = *top_left.outerIndexPtr();
        m_inner = new StorageIndex[tmp_nnz];
        m_val = new T[tmp_nnz];
        
        T* p_val = m_val;
        StorageIndex* p_inner = m_inner;
        StorageIndex out_nnz = 0;
        
        for (StorageIndex j = 0; j < ncols_tl; ++j) {
            StorageIndex col_cnt = 0;
            for (typename SpMat::InnerIterator it(tl_ref, j); it; ++it) {
                if (it.row() < j) continue; // lower triangle only
                *p_val++ = it.value();
                *p_inner++ = it.row();
                ++out_nnz;
                ++col_cnt;
            }

            for (StorageIndex i = 0; i < ncols_br; ++i) {
                *p_val++ = bl_ref.coeff(i, j);
                *p_inner++ = ncols_tl + i;
                ++out_nnz;
                ++col_cnt;
            }
        
            m_outer[j+1] = m_outer[j] + col_cnt;
        }
        
        for (StorageIndex j = 0; j < ncols_br; ++j) {
            // only need lower triangle
            for (StorageIndex i = j; i < ncols_br; ++i) {
                *p_val++ = br_ref.coeff(i,j);
                *p_inner++ = ncols_tl + i;
                ++out_nnz;
            }
            m_outer[ncols_tl+j+1] = m_outer[ncols_tl + j] + ncols_br - j;
        }
        m_nnz = out_nnz;
    } // constructor

    ~Block_sparse_lower() {
        delete[] m_outer;
        delete[] m_inner;
        delete[] m_val;
    } // destructor

    Eigen::SparseMatrix<T> operator () () {
        return typename Eigen::SparseMatrix<T>::Map(
            m_cols, 
            m_cols,
            m_nnz,
            m_outer,
            m_inner,
            m_val
        );   
    } //operator ()
}; // Block_sparse_lower
} // namespace math
} // namespace stan
```
The first thing you probably noticed was all the templates. Templates are 
a beautiful^[Until you're rooting around a seventy page compiler error that really just means you forgot a typename on the final `return`.] feature of C++ and pretty much all that bit just alllows us to have 
any matrix and sparse matrix from Eigen as long as they contain scalars (as
opposed to autodiff variables). They also allow us to hack together a pre-C++20
version of [concepts](https://en.wikipedia.org/wiki/Concepts_(C%2B%2B)), 
which is all of the `require_` statements.

Once we are actually in the class, it has three methods. The constructor
takes in the three matrices, one sparse and two dense. It checks at compile time
that they are all column-major and then starts doing its work. There's nothing too
exciting happening here. Some size checking, and then we run through the loop
stacking the relevant vector parts onto each other.

The destructor frees the allocated memory (a core part of the RAII pattern).

Finally, we need to actually get access to this sparse matrix, which I implemented
as a call operator. It returns a self-adjoint view (aka it will pretend to be
symmetric when doing operations even though only the lower triangle is filled)
of a `Map` of the three pointers. `Map`s are a nice way for Eigen to tell its
internal `SparseMatrix` representation to look at the pieces of memory defined 
in this class when it is looking for inner indices, outer indices, or values.
This doesn't create a copy so it's memory efficient.

So let's test it. I'm going to run the following code .
```c++
#include <iostream>
#include "sp_block.hpp"
#include "Eigen/SparseCore"
#include "Eigen/Dense"


int main() {
    std::cout << "-----------matrix test---------" << std::endl;
    double values[] = { 1., 2., 3., 4. };
    int inner[] = { 4, 3, 2, 1 }; // nonzero row indices
    int outer[] = { 0, 1, 2, 3, 4, 4 }; // start index per column + 1 for last col
    Eigen::SparseMatrix<double> A = Eigen::SparseMatrix<double>::Map(
        5 /*rows*/, 5 /*cols*/, 4 /*nonzeros*/, outer, inner, values);
    
    std::cout << Eigen::MatrixXd(A) << std::endl << std::endl;

    Eigen::Matrix<double, 2, 5> B;
    B << 1.,2.,3.,4.,5.,1.,2.,3.,4.,5.;
    std::cout << B << std::endl << std::endl;
    Eigen::Matrix<double, 2, 2> C;
    C << 1.,1.,1.,1.;
    std::cout << C << std::endl << std::endl;
    
    std::cout << "   -------ans-------" << std::endl;
    Eigen::SparseMatrix<double> D = 
        stan::math::Block_sparse_lower<decltype(A), decltype(B),decltype(C)>(
            A.triangularView<Eigen::Lower>(), B, C.triangularView<Eigen::Lower>())();
    std::cout << Eigen::MatrixXd(D) << std::endl << std::endl;

    std::cout << "-----------to_ref test---------" << std::endl;
    Eigen::SparseMatrix<double> E = A * A;
    std::cout << Eigen::MatrixXd(A) << std::endl << std::endl;
    std::cout << "   -------ans-------" << std::endl;
    Eigen::SparseMatrix<double> F = 
        stan::math::Block_sparse_lower<decltype(E), decltype(B),decltype(C)>(A, B, C)();
    std::cout << Eigen::MatrixXd(F) << std::endl << std::endl;
}
```

After compilation, the output is 
```
-----------matrix test---------
0 0 0 0 0
0 0 0 4 0
0 0 3 0 0
0 2 0 0 0
1 0 0 0 0

1 2 3 4 5
1 2 3 4 5

1 1
1 1

   -------ans-------
0 0 0 0 0 0 0
0 0 0 0 0 0 0
0 0 3 0 0 0 0
0 2 0 0 0 0 0
1 0 0 0 0 0 0
1 2 3 4 5 1 0
1 2 3 4 5 1 1

-----------to_ref test---------
0 0 0 0 0
0 0 0 4 0
0 0 3 0 0
0 2 0 0 0
1 0 0 0 0

   -------ans-------
0 0 0 0 0 0 0
0 0 0 0 0 0 0
0 0 3 0 0 0 0
0 2 0 0 0 0 0
1 0 0 0 0 0 0
1 2 3 4 5 1 0
1 2 3 4 5 1 1

```
This is exactly what we expect! Hooray.

### How about the derivative?

This is all lovely, unfortunately it is not enough. We still need a way to relate
derivatives with respect to this block matrix to derivatives with respect to 
the components. In order to do this, we need to think a little bit more about
how reverse-mode autodiff works.

The most important thing about the Stan math library from a user perspective
is that its functions are _differentiable_. The only way to make this true
is to specialize the functions so that they behave correctly when touched by
and autodiff variable. In Stan math, these are called `var`s. These have two
important member functions `var.val()`, which returns its variable and `var.adj()`,
which returns its adjoint. 

This is not really the place to go through how autodiff works. I recommend 
[Charles' review](https://arxiv.org/abs/1811.05031), or for more Stan-specific
information we have the [old paper](https://arxiv.org/pdf/1509.07164) or this
[blog post describing the most recent implementation of autodiff in Stan math](https://blog.mc-stan.org/2020/11/23/thinking-about-automatic-differentiation-in-fun-new-ways/).

But the very very very short version is that if we want to backward differentiate 
the formula 
```
z = f(x, y)
```
we need to update 
```
x.adj() += df_dxt * z.adj()
y.adj() += df_dyt * z.adj()
```
where the variable `df_dxt` is the transpose of the Jacobian matrix.

We need to compute the derivative of the log-determinant (which involves the 
partial inverse above), the derivative of the linear solve used to compute the mean,
and the derivative of the blocking operation.

The reason that we need to explicitly consider the blocking operation is that 
we are interested in the adjoint $\bar \Lambda$, but when we differentiate the 
log-determinant and the linear solve, we will be computing $\bar Q$. So we 
need to work out the link.

For instance, we need the derivative of 
$$
\log \left|\begin{pmatrix}
\Lambda^TZ^TWZ\Lambda + I & \Lambda^T Z^T WX \\ X^TWZ\Lambda & X^TWX  + \Sigma_\beta^{-1}
\end{pmatrix}\right|
$$
with respect to $\Lambda$. 

To do this we will break this into two steps (in pesudocode):
```
Q = BlockMatrix(Lambda.T* A * Lambda + I, Lambda.T * B, C)
log_det = log(det(Q))
```
where `A`, `B`, and `C` are constant matrices. The reverse-mode autodiff procedure 
would then be
```
Q.adj() -= partial_inverse(Q) * log_det.adj()
Lambda.adj() += [derivative] * [adjoint of the block matrix] 
```
So we need to work otu what that second line actually is!

### Differentiating the block matrix

In order to do this, we need to work from first principles. We can do this using
the notation and concepts from Mike Giles' [fabulous paper](https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf.)

The adjoints $\bar X$ and $\bar Y$ of the equation $Z = f(X,Y)$ satisfy
$$
\operatorname{tr}\left(\bar{Z}^TdZ\right) = \operatorname{tr}\left(\bar{X}^TdX\right) + \operatorname{tr}\left(\bar{Y}^TdY\right),
$$
where $\bar{X}$ and $\bar{Y}$ going to depend on $f$ and $\bar{C}$. The order is the reverse of evaluation order, which gives this mode of autodiff its name. In fact, we can do a lil maths and get
$$
\bar{X} = \frac{\partial f}{\partial X}^T \bar{Z},
$$
where $\frac{\partial f}{\partial X}$ is the Jacobian of $f$ wrt $X$.

To get 
the update rule for $\bar{\Lambda}$ wrt $\bar Q$, we use the definition of the adjoint
$$
\operatorname{tr}(\bar{Q}^TdQ) = \operatorname{tr}(\bar{\Lambda}d\Lambda).
$$

We know that 
$$
dQ = \begin{pmatrix} (d\Lambda)^T Z^TWZ\Lambda + \Lambda^TZ^TWZd\Lambda & (d\Lambda)^TZ^TWX \\ X^TWZd\Lambda & 0 \end{pmatrix},
$$
which means we need to calculate
$$
\operatorname{tr}\left[\begin{pmatrix}
\bar{Q}_{11} & \bar{Q}_{21}^T \\ \bar{Q}_{21} & \bar{Q}_{22}
\end{pmatrix}^T\begin{pmatrix} (d\Lambda)^T Z^TWZ\Lambda + \Lambda^TZ^TWZd\Lambda & (d\Lambda)^TZ^TWX \\ X^TWZd\Lambda & 0 \end{pmatrix}\right].
$$
Some quick matrix multiplication and the cyclical property of the trace gives
$$
\operatorname{tr}(\bar{Q}^TdQ) = \operatorname{tr}\left[2\left(Z^TWZ\Lambda \bar{Q}_{11} + Z^TWX\bar{Q}_{{21}} \right)^Td\Lambda\right] 
$$
and hence 
$$
\bar \Lambda = 2Z^TWZ\Lambda \bar{Q}_{11} + Z^TWX\bar{Q}_{{21}}
$$

In order to compute this, we need to _un-block_ our block matrix. This will, once
again, just be a bunch of looping through iterators and building `Eigen::Map`s.




## C++ Plumbing: A partial inverse
In order to compute the relevant derivatives, we need access to the elements of 
$Q^{-1}$ that correspond to the non-zero elements of $Q$. While it seems like 
this should be extremely expensive, it is cheap and straightforward to get these
elements from the Cholesky triangle of $Q$.

For a description of this algorithm and a background on why this is necessary, look at the [previous post](https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative#computing-the-partial-inverse) where I derived it and implemented it in Python.

The C++ implementation (which is strongly indebted to Finn Lindgren!) is similar, but uses the Eigen iterators.

Essentially, we need to implement the following pseudocode.
```
 for i = n-1, ..., 0
   for j = n-1, ..., i
   if (L[j,i] not known to be 0)
      Sigma[j,i] = Sigma[i,j] = (I(i==j)/L[i,i] 
        - sum_{k=i+1}^{n-1} L[k,i] Sigma[k,j] ) / L[i,i]
```

Using iterators to do this is, perhaps, a bit annoying, but it is
very safe. It might look like you need three iterators to do this, but we atually need four. Because the matrix is stored in Column-major order, we are going to need a new iterator for every distinct column index. In this case, that is 
1. A reverse iterator going up column `i` of `Sigma`
2. A reverse iterator going up column `i` of `L`
3. A reverse iterator going up colunn `j` of `Sigma`
4. A reverse iterator going up column `i` in sync with iterator 3.

The C++ code is pretty straightforward after that, you just need to keep your 
iterators in sync. One wrinkle that I forgot about the first time I coded this 
is that there are a few things that I need to be true: firstly, I need the output
to be the lower-triangle of a symmetric matrix, and secondly I need that matrix
to have the same sparsity pattern as $Q$. To do this, I wrote another RAII class,
mainly because if I'm going to manipulate raw pointers I'm gonna want some safety.

The code is below

```c++
#include <Eigen/SparseCore>
#include <Eigen/SparseCholesky>

namespace stan {
namespace math {
typedef Eigen::SparseMatrix<double>::StorageIndex StorageIndex;

template<typename SpMat> class MatchPattern {
    using T = typename base_type<SpMat>::type;
    StorageIndex* m_outer;
    StorageIndex* m_inner;
    T* m_val;
    StorageIndex m_cols;
    StorageIndex m_nnz;

    public:

    MatchPattern(const SpMat& A, const SpMat& pattern) {
        m_cols = pattern.cols();
        m_nnz = pattern.nonZeros();
        m_outer = pattern.outerIndexPtr(); 
        m_inner = 
        m_val = new T[m_nnz];

        T* valptr = m_val;
        for (int j = 0; j < m_cols; ++j) {
            typename SpMat::InnerIterator Acol(A, j);
            for (typename SpMat::InnerIterator pattern_col(pattern, j);
                pattern_col; ++pattern_col) {
                    while (Acol & (Acol.row() < pattern_col.row())){
                        ++Acol;
                    }
                    valptr++ = Acol.value();
                    ++Acol;
                }
        }
    }

    ~MatchPattern() {
        delete[] m_val;
    }

    SpMat operator () () {
        return typename SpMat::Map(
            m_cols,
            m_cols,
            m_nnz,
            m_inner,
            m_val
        );
    }
};


template<typename SpChol, typename SpMat>
typename SpChol::MatrixType partial_inverse(
    const SpChol& llt,
    const SpMat& Q
) {
    typedef typename SpMat::ReverseInnerIterator reverse_it;
    StorageIndex ncols = llt.cols();
    SpMat Qinv = llt.matrixL().template selfAdjointView();

    for (int i = ncols - 1; i >= 0; --i) {
        reverse_it QinvcolI(Qinv, i);
        for (reverse_it LcolI_slow(llt.matrixL(), i); LcolI_slow; --LcolI_slow) {
            // inner sum iterators
            reverse_it LcolI(llt.matrixL(), i);
            reverse_it QinvcolJ(Qinv, LcolI_slow.row());
            
            // Initialize Qinv[j,i]
            QinvcolI.valueRef() = 0.0;

            // Inner-most sum
            while (LcolI.row() > i) {
                // First up, sync the iterators
                while ( QinvcolJ & (LcolI.row() < QinvcolJ.row())){
                    --QinvcolJ;
                }
                if (QinvcolJ & (QinvcolJ.row() == LcolI.row())) {
                    QinvcolI.valueRef() -= LcolI.value() * QinvcolJ.value();
                    --QinvcolJ;
                }
                --LcolI;
            }
            // At this point LcolI is the diagonal value
            if (i == LcolI.row()) {
                QinvcolI.vaueRef() +=  1/ LcolI.value();
                QinvcolI.vaueRef() /=  LcolI.value();
            } else{
                QinvcolI.vaueRef() /=  LcolI.value();
                // Set Qinv[i,j] = Qinv[j,i]
                while (QinvcolJ.row() > i) {
                    --QinvcolJ;
                }
                QinvcolJ.valRef() = QinvcolI.value();
            }
        }
        --QinvcolI;
    }
    // Undo the permuatation
    Qinv = Qinv.twistedBy(llt.permutationP().inverse());

    // Return the non-zero elements of Qinv corresponding to the non-zero
    // elements of Q
    return MatchPattern(Qinv, Q)();

}

} // namespace math
} // namespace stan

```

You'll probably notice that there are far fewer template shennigans here than
in the block matrix code. That is because this only needs to work with scalar
types and doesn't need to be part of the `math` API. If needed, I guess we could always 
work out what teh derivitave is and implement its reverse-mode specialization,
but frankly why^[One reason would be to use gradient descent on the score function for a Gaussian MLE. Another is that this might be useful inside the `generated quantities` block to compute things like the marginal variances of the model, but, as the great lady said, not today Satan.] bother.

**Do some testing**



## C++ Plumbing: The linear solve

The final primitive that we need to implement is the linear solve. One way
to do this would be to autodiff directly through the Cholesky factorisation and 
the triangular solve. But instead, we note that if $c = A^{-1}b$, then^[If you don't believe me, differentiate both sides of $AC = I$.]
$$
dc = A^{-1}db + A^{-1}dAc
$$
and thus
$$
\operatorname{tr}(\bar{c}^Tdc) = \operatorname{tr}((A^{-1}\bar{c})^Tdb) - \operatorname{tr}(c\bar{c}^TA^{-1}dA)
$$
and so we get the adjoints
$$
\bar{b} = A^{-1}\bar{c}, \qquad \bar{A} = - A^{-1}\bar{c}c^T = -\bar{b}c^T,
$$
where once again if $A$ is sparse then we only need to track the elements of $\bar{A}$ 
that correspond to the non-zero values of $A$.


This should be fairly simple to implement: the only tricky part is making 
sure that we have a mechanism to only get the sparse lower triangle of $\bar{A}$.

This problem is extremely similar to `MatchPattern` defined above, so let's just
add a new constructor to that class.

```c++
MatchPattern(
    const typename Eigen::Vector<T>& b, 
    const typename Eigen::Vector<T>& c, 
    const SpMat& pattern
) {
    m_cols = pattern.cols();
    m_nnz = pattern.nonZeros();
    m_outer = pattern.outerIndexPtr(); 
    m_inner = 
    m_val = new T[m_nnz];

    T* valptr = m_val;
    for (int j = 0; j < m_cols; ++j) {
        for (typename SpMat::InnerIterator pattern_col(pattern, j);
            pattern_col; ++pattern_col) {
                valptr++ = b.coeff(pattern_col.row()) * c.coeff(j);
            }
    }
    
}
```

There's a little bit of code repeated here, which probably suggests that there
is a way to refactor this a little more cleanly, but I really can't be bothered.

## Adding the log-density to Stan

