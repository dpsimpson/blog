{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Sparse matrices part 7a: Another shot at JAX-ing the Cholesky decomposition'\n",
        "description: |\n",
        "  I work in R a lot so I should be used to weird syntax. This part looks at the non-zero pattern.\n",
        "date: '2022-12-02'\n",
        "image: south_pac.png\n",
        "categories:\n",
        "  - JAX\n",
        "  - Sparse matrices\n",
        "  - Autodiff\n",
        "twitter-card:\n",
        "  title: 'Sparse matrices part 7a: Another shot at JAX-ing the Cholesky decomposition'\n",
        "  creator: '@dan_p_simpson'\n",
        "citation:\n",
        "  url: 'https://dansblog.netlify.app/posts/2022-11-27-sparse7/sparse7.html'\n",
        "format:\n",
        "  html:\n",
        "    df-print: paged\n",
        "draft: false\n",
        "execute:\n",
        "  eval: true\n",
        "keep-ipynb: true\n",
        "---"
      ],
      "id": "b77ac104"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The time has come once more to resume my journey into sparse matrices. There's been a bit of a pause, mostly because I realised that I didn't know how to implement the sparse Cholesky factorisation in a JAX-traceable way. But now the time has come. It is time for me to get on top of JAX's weird control-flow constructs.\n",
        "\n",
        "And, along the way, I'm going to re-do the sparse Cholesky factorisation to make it, well, better.\n",
        "\n",
        "In order to temper expectations, I will tell you that this post does not do the numerical factorisation, only the symbolic one. Why? Well I wrote most of it on a long-haul flight and I didn't get to the numerical part. And this was long enough. So hold your breaths for Part 7b, which will come as soon as I write it.\n",
        "\n",
        "You can consider this a _much_ better re-do of [Part 2](https://dansblog.netlify.app/posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html). This is no longer my first python coding exercise in a decade, so hopefully the code is better. And I'm definitely trying a lot harder to think about the limitations of JAX.\n",
        "\n",
        "Before I start, I should probably say why I'm doing this. JAX is a truly magical thing that will compute gradients and every thing else just by clever processing of the Jacobian-vector product code. Unfortunately, this is only possible if the Jacobian-vector product code is JAX traceable and this code is structurally extremely similar^[If you're wondering about the break between sparse matrix posts, I realised this pretty much immediately and just didn't want to deal with it!] to the code for the sparse Cholesky factorisation.\n",
        "\n",
        "I am doing this in the hope of (eventually getting to) autodiff. But that won't be this blog post. This blog post is complicated enough.\n",
        "\n",
        "## Control flow of the damned\n",
        "\n",
        "The first an most important rule of programming with JAX is that loops will break your heart. I mean, whatever, I guess they're fine. But there's a problem. Imagine the following function\n"
      ],
      "id": "ace9ff38"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def f(x: jax.Array, n: Int) -> jax.Array:\n",
        "  out = jnp.zeros_like(x)\n",
        "  for j in range(n):\n",
        "    out = out + x\n",
        "  return out"
      ],
      "id": "f74da6e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is, basically, the worst implementation of multiplication by an integer that you can possibly imagine. This code will run fine in Python, but if you try to JIT compile it, JAX is gonna get _angry_. It will produce the machine code equivalent of \n"
      ],
      "id": "40026c01"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def f_n(x):\n",
        "  out = x\n",
        "  out = out + x\n",
        "  out = out + x\n",
        "  // do this n times\n",
        "  return out"
      ],
      "id": "a12c7876",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are two bad things happening here. First, note that the \"compiled\" code depends on `n` and will have to be compiled anew each time `n` changes. Secondly, the loop has been replaced by `n` copies of the loop body. This is called _loop unrolling_ and, when used judiciously by a clever compiler, is a great way to speed up code. When done completely for _every_ loop this is a nightmare and the corresponding code will take a geological amount of time to compile.\n",
        "\n",
        "A similar thing^[If a person who actually knows how the JAX autodiff works happens across this blog, I'm so sorry.] happens when you need to run autodiff on `f(x,n)`. For each `n` an expression graph is constructed that contains the unrolled for loop. This suggests that autodiff might also end up being quite slow (or, more problematically, more memory-hungry).\n",
        "\n",
        "So the first rule of JAX is to avoid for loops. But if you can't do that, there are three built-in loop structures that play nicely with JIT compilation and sometimes^[omg you guys. So many details] differentiation. These three constructs are\n",
        "\n",
        "1. A while loop `jax.lax.while(cond_func, body_func, init)`\n",
        "2. An accumulator `jax.lax.scan(body_func, init, xs)`\n",
        "3. A for loop `jax.lax.fori_loop(lower, upper, body_fun, init)`\n",
        "\n",
        "Of those three, the first and third work mostly as you'd expect, while the second is a bit more hairy. The `while` function is roughly equivalent to "
      ],
      "id": "f421b856"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "`\n",
        "def jax_lax_while_loop(cond_func, body_func, init):\n",
        "  x  = init\n",
        "  while cond_func(x):\n",
        "    x = body_func(x)\n",
        "  return x"
      ],
      "id": "848ab83c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So basically it's just a while loop. The thing that's important is that it compiles down to a single XLA operation^[These are referred to as HLOs (Higher-level operations)] instead of some unrolled mess. \n",
        "\n",
        "One thing that is important to realise is that while loops are only forwards-mode differentiable, which means that it is _very_ expensive^[Instead of doing one pass of reverse-mode, you would need to do $d$ passes of forwards mode to get the gradient with respect to a d-dimensional parameter.] to compute gradients. The reason for this is that we simply do not know how long that loop actually is and so it's impossible to build a fixed-size expression graph.\n",
        "\n",
        "The `jax.lax.scan` function is probably the one that people will be least familiar with. That said, it's also the one that is roughly \"how a for loop should work\". The concept that's important here is a for-loop with _carry over_. Carry over is information that changes from one step of the loop to the next. This is what separates us from a `map` statement, which would apply the same function independently to each element of a list.\n",
        "\n",
        "The scan function looks like "
      ],
      "id": "07934d62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "def jax_lax_scan(body_func, init, xs):\n",
        "  len_x0 = len(x0)\n",
        "  if not all(len(x) == len_x0 for x in xs):\n",
        "    raise ValueError(\"All x must have the same length!!\")\n",
        "  carry = init\n",
        "  ys = []\n",
        "  for x in xs:\n",
        "    carry, y = body_func(carry, x)\n",
        "    ys.append(y)\n",
        "  \n",
        "  return carry, np.stack(ys)"
      ],
      "id": "0e8b0fb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A critically important limitation to `jax.lax.scan` is that  is that every `x` in `xs` must have the same shape! This mean, for example, that "
      ],
      "id": "b0a9971d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "xs = [[1], [2,3], [4], 5,6,7]"
      ],
      "id": "f6fd46e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "is not a valid argument. Like all limitations in JAX, this serves to make the code transformable into efficiently compiled code across various different processors.\n",
        "\n",
        "For example, if I wanted to use `jax.lax.scan` on my example from before I would get"
      ],
      "id": "623b90ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from jax import lax\n",
        "from jax import numpy as jnp\n",
        "\n",
        "def f(x, n):\n",
        "  init = jnp.zeros_like(x)\n",
        "  xs = jnp.repeat(x, n)\n",
        "  def body_func(carry, y):\n",
        "    val = carry + y\n",
        "    return (val, val)\n",
        "  \n",
        "  final, journey = lax.scan(body_func, init, xs)\n",
        "  return (final, journey)\n",
        "\n",
        "final, journey = f(1.2, 7)\n",
        "print(final)\n",
        "print(journey)"
      ],
      "id": "f2f0743b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This translation is a bit awkward compared to the for loop but it's the sort of thing that you get used to.\n",
        "\n",
        "This function can be differentiated^[Unlike `jax.lax.while`, which is only forwards differentiable, `jax.lax.scan` is fully differentiable.] and compiled. To differentiate it, I need a version that returns a scalar, which is easy enough to do with a lambda.\n"
      ],
      "id": "7dbea9d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| error: true\n",
        "from jax import jit, grad\n",
        "\n",
        "f2 = lambda x, n: f(x,n)[0]\n",
        "f2_grad = grad(f2, argnums = 0)\n",
        "\n",
        "print(f2_grad(1.2, 7))"
      ],
      "id": "6c42dc3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `argnums` option tells JAX that we are only differentiating wrt the first argument.\n",
        "\n",
        "JIT compilation is a tiny bit more delicate. If we try the natural thing, we are going to get an error.\n"
      ],
      "id": "afb28dff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| error: true\n",
        "f_jit_bad = jit(f)\n",
        "bad = f_jit_bad(1.2, 7)"
      ],
      "id": "b1ed9a88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compile a function, JAX needs to know how big everything is. And right now it does not know what `n` is. This shows itself through the `ConcretizationTypeError`, which basically says that as JAX was looking through your code it found something it can't manipulate. In this case, it was in the `jnp.repeat` function.\n",
        "\n",
        "We can fix this problem by declaring this parameter `static`.\n"
      ],
      "id": "176f7bfe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f_jit = jit(f, static_argnums=(1,))\n",
        "print(f_jit(1.2,7)[0])"
      ],
      "id": "48c5453e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " A static parameter is a parameter value that is known at compile time. If we define `n` to be static, then the first time you call `f_jit(x, 7)` it will compile and then it will reuse the compiled code for any other value of `x`. If we then call `f_jit(x, 9)`, the code will _compile again_. \n",
        "\n",
        " To see this, we can make use of a JAX oddity: if a function prints something^[In general, if the function has state.], then it will only be printed upon compilation and never again. This means that we can't do _debug by print_. But on the upside, it's easy to check, when things are compiling.\n"
      ],
      "id": "556f9064"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f2(x, n):\n",
        "  print(f\"compiling: n = {n}\")\n",
        "  return f(x,n)[0]\n",
        "\n",
        "f2_jit = jit(f2, static_argnums=(1,))\n",
        "print(f2_jit(1.2,7))\n",
        "print(f2_jit(1.8,7))\n",
        "print(f2_jit(1.2,9))\n",
        "print(f2_jit(1.8,7))"
      ],
      "id": "1ea8a500",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a perfectly ok solution as long as the static parameters don't change very often. In our context, this is going to have to do with the sparsity pattern.\n",
        "\n",
        "Finally, we can talk about `jax.lax.fori_loop`, the in-built for loop. This is basically a convenience wrapper for `jax.lax.scan` (when `lower` and `upper` are static) or `jax.lax.while` (when they are not). The Python pseudocode is "
      ],
      "id": "8c316aa1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def jax_lax_fori_loop(lower, upper, body_func, init):\n",
        "  out = init\n",
        "  for i in range(lower, upper):\n",
        "    out = body_func(i, out)\n",
        "  return out"
      ],
      "id": "6ece207a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To close out this bit where I repeat the docs, there is also a traceable if/else: `jax.lax.cond` which has the pseudocode"
      ],
      "id": "cab2a044"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def jax_lax_cond(pred, true_fun, false_fun, val):\n",
        "  if pred:\n",
        "    return true_fun(val)\n",
        "  else:\n",
        "    return false_fun(val)"
      ],
      "id": "fbd0901c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a JAX-traceable symbolic sparse Choleksy factorisation\n",
        "\n",
        "In order to build a JAX-traceable sparse Cholesky factorisation $A = LL^T$, we are going to need to build up a few moving parts.\n",
        "\n",
        "1. Build the elimination tree of $A$ and find the number of non-zeros in each column of $L$\n",
        "\n",
        "2. Build the _symbolic factorisation_^[This is the version of the symbolic factorisation that is most appropriate for us, as we will be doing a lot of Cholesky factorisations with the same sparsity structure. If we rearrange the algorithm to the up-looking Cholesky decomposition, we only need the column counts and this is also called the symbolic factorisation. This is, incidentally, how Eigen's sparse Cholesky works.] of $L$ (aka the location of the non-zeros of $L$)\n",
        "\n",
        "3. Do the actual numerical decomposition.\n",
        "\n",
        "In the [previous post](https://dansblog.netlify.app/posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices.html) we did not explicitly form the elimination tree. Instead, I used dynamic memory allocation. This time I'm being more mature.\n",
        "\n",
        "### Building the expression graph\n",
        "\n",
        "The elimination tree^[Actually it's a forest] $\\mathcal{T}_A$ is a (forest of) rooted tree(s) that compactly represent the non-zero pattern of the Cholesky factor $L$. In particular, the elimination tree has the property that, for any $k > j$ , $L_{kj} \\neq 0$ if and only if there is a path from $j$ to $k$ in the tree. Or, in the language of trees, $L_{kj} \\neq 0$ if and only if $j$ is a descendant of $k$ in the tree $\\mathcal{T}_A$.\n",
        "\n",
        "We can describe^[Because we are talking about a tree, each child node has at most one parent. If it doesn't have a parent it's the root of the tree. I remember a lecturer saying that it should be called \"father and son\" or \"mother and daughter\" because every child has 2 parents but only one mother or one father. The 2000s were a wild time.] $\\mathcal{T}_A$ by listing the parent of each node. The parent node of $j$ in the tree is the smallest $i > j$ with $L_{ij} \\neq 0$.\n",
        "\n",
        "We can turn this into an algorithm. An efficient version, which is described in Tim Davies book takes about $\\mathcal{O(\\text{nnz}(A))}$ operations. But I'm going to program up a slower one that takes $\\mathcal{O(\\text{nnz}(L))}$ operations, but has the added benefit^[These can also be computed in approximately $\\mathcal{O(\\text{nnz}(A))}$ time, which is much faster. But the algorithm is, frankly, pretty tricky and I'm not in the mood to program it up. This difference would be quite important if I wasn't storing the full symbolic factorisation and was instead computing it every time, but in my context it is less clear that this is worth the effort.] of giving me the column counts for free.\n",
        "\n",
        "To do this, we are going to walk the tree and dynamically add up the column counts as we go. \n",
        "\n",
        "To start off, let's do this in standard python so that we can see what the algorithm look like. The key concept is that if we write $\\mathcal{T}_{j-1}$ as the elimination tree encoding the structure of^[Python notation! This is rows/cols 0 to `j-1`] `L[:j, :j]`, then we can ask about how this tree connects with node `j`.\n",
        "\n",
        "A theorem gives a very simple answer to this.\n",
        "\n",
        "::: {#thm-tree}\n",
        "If $j > i$, then $A_{j,i} \\neq 0$ implies that $i$ is a descendant of $j$ in $\\mathcal{T}_A$. In particular, that means that there is a directed path in $\\mathcal{T}_A$ from $i$ to $j$.\n",
        ":::\n",
        "\n",
        "This tells us that the connection between $\\mathcal{T}_{j-1}$ and node $j$ is that for each non-zero elements $i$ of the $j$th row of $A$, we can walk $\\mathcal{T} must have a path in $\\mathcal{T}_{j-1}$ from $i$ and we will eventually get to a node that has no parent in $\\{0,\\ldots, j-1\\}$. Because there _must_ be a path from $i$ to $j$ in $T_j$, it means that the parent of this terminal node must be $j$.\n",
        "\n",
        "As with everything Cholesky related, this works because the algorithm proceeds from left to right, which in this case means that the node label associated with _any_ descendant of $j$ is always less than $j$.\n",
        "\n",
        "The algorithm is then a fairly run-of-the-mill^[Python, it turns out, does not have a `do while` construct because, apparently, everything is empty and life is meaningless.] tree traversal, where we keep track of where we have been so we don't double count our columns.\n",
        "\n",
        "Probably the most important thing here is that I am using the _full_ sparse matrix rather than just its lower triangle. This is, basically, convenience. I need access to the left half of the $j$th row of $A$, which is conveniently the same as the top half of the $j$th column. And sometimes you just don't want to be dicking around with swapping between row- and column-based representations.\n"
      ],
      "id": "ff2489fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def etree_base(A_indices, A_indptr):\n",
        "  n = len(A_indptr) - 1\n",
        "  parent = [-1] * n\n",
        "  mark = [-1] * n\n",
        "  col_count = [1] * n\n",
        "  for j in range(n):\n",
        "    mark[j] = j\n",
        "    for indptr in range(A_indptr[j], A_indptr[j+1]):\n",
        "      node = A_indices[indptr]\n",
        "      while node < j and mark[node] != j:\n",
        "        if parent[node] == -1:\n",
        "          parent[node] = j\n",
        "        mark[node] = j\n",
        "        col_count[node] += 1\n",
        "        node = parent[node]\n",
        "  return (parent, col_count)"
      ],
      "id": "a4472e53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To convince ourselves this works, let's run an example and compare the column counts we get to our previous method.\n"
      ],
      "id": "4dae0422"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Some boilerplate from previous editions.\"\n",
        "      \n",
        "from scipy import sparse\n",
        "import scipy as sp\n",
        "    \n",
        "\n",
        "def make_matrix(n):\n",
        "  one_d = sparse.diags([[-1.]*(n-2), [2.]*n, [-1.]*(n-2)], [-2,0,2])\n",
        "  A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n))\n",
        "  A_csc = A.tocsc()\n",
        "  A_csc.eliminate_zeros()\n",
        "  A_lower = sparse.tril(A_csc, format = \"csc\")\n",
        "  A_index = A_lower.indices\n",
        "  A_indptr = A_lower.indptr\n",
        "  A_x = A_lower.data\n",
        "  return (A_index, A_indptr, A_x, A_csc)\n",
        "\n",
        "def _symbolic_factor(A_indices, A_indptr):\n",
        "  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n",
        "  n = len(A_indptr) - 1\n",
        "  L_sym = [np.array([], dtype=int) for j in range(n)]\n",
        "  children = [np.array([], dtype=int) for j in range(n)]\n",
        "  \n",
        "  for j in range(n):\n",
        "    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n",
        "    for child in children[j]:\n",
        "      tmp = L_sym[child][L_sym[child] > j]\n",
        "      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n",
        "    if len(L_sym[j]) > 1:\n",
        "      p = L_sym[j][1]\n",
        "      children[p] = np.append(children[p], j)\n",
        "        \n",
        "  L_indptr = np.zeros(n+1, dtype=int)\n",
        "  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n",
        "  L_indices = np.concatenate(L_sym)\n",
        "  \n",
        "  return L_indices, L_indptr"
      ],
      "id": "d8b7dc78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A_indices/A_indptr are the lower triangle, A is the entire matrix\n",
        "A_indices, A_indptr, A_x, A = make_matrix(37)\n",
        "parent, col_count = etree_base(A.indices, A.indptr)\n",
        "L_indices, L_indptr = _symbolic_factor(A_indices, A_indptr)\n",
        "\n",
        "true_parent = L_indices[L_indptr[:-2] + 1]\n",
        "true_parent[np.where(np.diff(L_indptr[:-1]) == 1)] = -1\n",
        "print(all(x == y for (x,y) in zip(parent[:-1], true_parent)))\n",
        "\n",
        "true_col_count  = np.diff(L_indptr)\n",
        "print(all(true_col_count == col_count))"
      ],
      "id": "0f835e6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Excellent. Now we just need to convert it to JAX. \n",
        "\n",
        "Or do we?\n",
        "\n",
        "To be honest, this is a little pointless. This function is only run once per matrix so we won't really get much speedup^[The argument for JIT works by amortizing the compile time over several function evaluations. If I wanted to speed this algorithm up, I'd implement the more complex $\\mathcal{O}(\\operatorname{nnz}(A))$ version.] from compilation.\n",
        "\n",
        "Nevertheless, we might try.\n"
      ],
      "id": "0d309ceb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@jit\n",
        "def etree(A_indices, A_indptr):\n",
        " # print(\"(Re-)compiling etree(A_indices, A_indptr)\")\n",
        "  ## innermost while loop\n",
        "  def body_while(val):\n",
        "  #  print(val)\n",
        "    j, node, parent, col_count, mark = val\n",
        "    update_parent = lambda x: x[0].at[x[1]].set(x[2])\n",
        "    parent = lax.cond(lax.eq(parent[node], -1), update_parent, lambda x: x[0], (parent, node, j))\n",
        "    mark = mark.at[node].set(j)\n",
        "    col_count = col_count.at[node].add(1)\n",
        "    return (j, parent[node], parent, col_count, mark)\n",
        "\n",
        "  def cond_while(val):\n",
        "    j, node, parent, col_count, mark = val\n",
        "    return lax.bitwise_and(lax.lt(node, j), lax.ne(mark[node], j))\n",
        "\n",
        "  ## Inner for loop\n",
        "  def body_inner_for(indptr, val):\n",
        "    j, A_indices, A_indptr, parent, col_count, mark = val\n",
        "    node = A_indices[indptr]\n",
        "    j, node, parent, col_count, mark = lax.while_loop(cond_while, body_while, (j, node, parent, col_count, mark))\n",
        "    return (j, A_indices, A_indptr, parent, col_count, mark)\n",
        "  \n",
        "  ## Outer for loop\n",
        "  def body_out_for(j, val):\n",
        "     A_indices, A_indptr, parent, col_count, mark = val\n",
        "     mark = mark.at[j].set(j)\n",
        "     j, A_indices, A_indptr, parent, col_count, mark = lax.fori_loop(A_indptr[j], A_indptr[j+1], body_inner_for, (j, A_indices, A_indptr, parent, col_count, mark))\n",
        "     return (A_indices, A_indptr, parent, col_count, mark)\n",
        "\n",
        "  ## Body of code\n",
        "  n = len(A_indptr) - 1\n",
        "  parent = jnp.repeat(-1, n)\n",
        "  mark = jnp.repeat(-1, n)\n",
        "  col_count = jnp.repeat(1,  n)\n",
        "  init = (A_indices, A_indptr, parent, col_count, mark)\n",
        "  A_indices, A_indptr, parent, col_count, mark = lax.fori_loop(0, n, body_out_for, init)\n",
        "  return parent, col_count"
      ],
      "id": "e8c21906",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wow. That is _ugly_. But let's see^[Obviously it did not work the first time. A good way to debug JIT'd code is to use the python translations of the control flow literals. Why? Well for one thing there is an annoying tendency for JAX to fail silently when their is an out-of-bounds indexing error. Which happens, just for example, if you replace `node = A_indices[indptr]` with `node = A_indices[A_indptr[indptr]]` because you got a text message half way through the line.] if it works!\n"
      ],
      "id": "9e5efa51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parent_jax, col_count_jax = etree(A.indices, A.indptr)\n",
        "\n",
        "print(all(x == y for (x,y) in zip(parent_jax[:-1], true_parent)))\n",
        "print(all(true_col_count == col_count_jax))"
      ],
      "id": "d2c8b3d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success!\n",
        "\n",
        "I guess we could ask ourselves if we gained any speed.\n",
        "\n",
        "Here is the pure python code.\n"
      ],
      "id": "ab1c8ace"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import timeit\n",
        "A_indices, A_indptr, A_x, A = make_matrix(20)\n",
        "\n",
        "times = timeit.repeat(lambda: etree_base(A.indices, A.indptr),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(50)\n",
        "times = timeit.repeat(lambda: etree_base(A.indices, A.indptr),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(200)\n",
        "times = timeit.repeat(lambda: etree_base(A.indices, A.indptr),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")"
      ],
      "id": "a58438d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here is our JAX'd and JIT'd code.\n"
      ],
      "id": "ebc8d0e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A_indices, A_indptr, A_x, A = make_matrix(20)\n",
        "times = timeit.repeat(lambda: etree(A.indices, A.indptr),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(50)\n",
        "times = timeit.repeat(lambda: etree(A.indices, A.indptr),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(200)\n",
        "times = timeit.repeat(lambda: etree(A.indices, A.indptr),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "parent, col_count= etree(A.indices, A.indptr)\n",
        "L_indices, L_indptr = _symbolic_factor(A_indices, A_indptr)\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(1000)\n",
        "times = timeit.repeat(lambda: etree(A.indices, A.indptr),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")"
      ],
      "id": "b032fda5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that there is some decent speedup. For the first three examples, the computation time is dominated by the compilation time, but we see when the matrix has a million unknowns the compilation time is negligible. At this scale it would probably be worth using the fancy algorithm. That said, it is probably not worth sweating a three second that is only done once when your problem is that big! \n",
        "\n",
        "### The non-zero pattern of $L$\n",
        "\n",
        "Now that we know how many non-zeros there are, it's time to populate them. Last time, I used some dynamic memory allocation to make this work, but JAX is certainly not going to allow me to do that. So instead I'm going to have to do the worst thing possible: think.\n",
        "\n",
        "The way that we went about it last time was, to be honest, a bit arse-backwards. The main reason for this is that I did not have access to the elimination tree. But now we do, we can actually use it.\n",
        "\n",
        "The trick is to slightly rearrange^[We will still use the left-looking algorithm for the numerical computation. The two algorithms are equivalent in exact arithmetic and, in particular, have identical sparsity structures.] the order of operations to get something that is more convenient for working out the structure. \n",
        "\n",
        "Recall from last time that we used the _left-looking_ Cholesky factorisation, which can be written in the dense case as "
      ],
      "id": "9f9f00cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def dense_left_cholesky(A):\n",
        "  n = A.shape[0]\n",
        "  L = np.zeros_like(A)\n",
        "  for j in range(n):\n",
        "    L[j,j] = np.sqrt(A[j,j] - np.inner(L[j, :j], L[j, :j]))\n",
        "    L[(j+1):, j] = (A[(j+1):, j] - L[(j+1):, :j] @ L[j, :j].transpose()) / L[j,j]\n",
        "  return L"
      ],
      "id": "fcd7770e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is not the only way to organise those operations. An alternative is the _up-looking_ Cholesky factorisation, which can be implemented in the dense case as"
      ],
      "id": "4857ee6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def dense_up_cholesky(A):\n",
        "  n = A.shape[0]\n",
        "  L = np.zeros_like(A)\n",
        "  L[0,0] = np.sqrt(A[0,0])\n",
        "  for i in range(1,n):\n",
        "    #if i > 0:\n",
        "    L[i, :i] = (np.linalg.solve(L[:i, :i], A[:i,i])).transpose()\n",
        "    L[i, i] = np.sqrt(A[i,i] - np.inner(L[i, :i], L[i, :i]))\n",
        "  return L"
      ],
      "id": "e19bb987",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is quite a different looking beast! It scans row by row rather than column by column. And while the left-looking algorithm is based on matrix-vector multiplies, the up-looking algorithm is based on triangular solves. So maybe we should pause for a moment to check that these are the same algorithm!\n"
      ],
      "id": "efb5ae54"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A = np.random.rand(15, 15)\n",
        "A = A + A.transpose()\n",
        "A = A.transpose() @ A + 1*np.eye(15)\n",
        "\n",
        "L_left = dense_left_cholesky(A)\n",
        "L_up = dense_up_cholesky(A)\n",
        "\n",
        "print(round(sum(sum(abs((L_left - L_up)[:])))),2)"
      ],
      "id": "0624645d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They are the same!!\n",
        "\n",
        "The reason for considering the up-looking algorithm is that it gives a slightly nicer description of the non-zeros of row `i`, which will let us find the location of the non-zeros in the whole matrix. In particular, the non-zeros to the left of the diagonal on row `i` correspond to the non-zero indices of the solution to  the lower triangular linear system^[I'm mixing 1-based indexing in the maths with 0-based in the code because I think we need more chaos in our lives.] \n",
        "$$\n",
        "L_{1:(i-1),1:(i-1)} x^{(i)} = A_{1:i-1, i}.\n",
        "$$\n",
        "Because $A$ is sparse, this is a system of $\\operatorname{nnz}(A_{1:i-1,i})$ linear equations, rather than $(i-1)$ equations that we would have in the dense case. That means that the sparsity pattern of $x^{(i)}$ will be the union of the sparsity patterns of the columns of $L_{1:(i-1),1:(i-1)}$  that correspond to the non-zero entries of $A_{1:i-1, i}$.\n",
        "\n",
        "This means two things. Firstly, if $A_{ji}\\neq 0$, then $x^{(i)}_j \\neq 0$. Secondly, if $x^{(i)}_j \\neq 0 $ _and_ $L_{kj}\\neq 0$, then $x_k \\neq 0$. These two facts give us a way of finding the non-zero set of $x^{(i)}$ if we remember just one more fact: a definition of the elimination tree is that $L_{kj} \\neq 0$ if $j$ is a descendant of $k$ in the elimination tree.\n",
        "\n",
        "This reduces the problem of finding the non-zero elements of $x^{(i)}$ to the problem of finding all of the descendants of $\\{j: A_{ji} \\neq 0\\}$ in the subtree $\\mathcal{T}_{i-1}$. And if there is one thing that people who are ok at programming are _excellent_ at it is walking down a damn tree.\n",
        "\n",
        "So let's do that. Well, I've already done it. In fact, that was how I found the column counts in the first place! With this interpretation, the outer loop is taking us across the rows. And once I am in row `j`^[Yes. I know. I'm swapping the meaning of $i$ and $j$ but you know that's because in a symmetric matrix rows and columns are a bit similar. The upper half of column $$ is the left half of row $j$ after all.], I then find a starting node `node` (which is a non-zero in $A_{1:(i-1),i}$) and I walk along that node checking each time if I've actually seen that node^[If `mark[node]==j` then I have already found `node` and all of its ancestors in my sweep of row `j`] before. If I haven't seen it before, I added one to the column count of column `node`^[This is because `L[j,node] != 0` by our logic.]. \n",
        "\n",
        "To allocate the non-zero structure, I just need to replace that counter increment with an assignment.\n",
        "\n",
        "### Attempt 1: Lord that's slow\n",
        "\n",
        "We will do the pure python version first."
      ],
      "id": "5194dc6e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def symbolic_cholesky_base(A_indices, A_indptr, parent, col_count):\n",
        "  n = len(A_indptr) - 1\n",
        "  col_ptr = np.repeat(1, n+1)\n",
        "  col_ptr[1:] += np.cumsum(col_count) \n",
        "  L_indices = np.zeros(sum(col_count), dtype=int)\n",
        "  L_indptr = np.zeros(n+1, dtype=int)\n",
        "  L_indptr[1:] = np.cumsum(col_count)\n",
        "  mark = [-1] * n\n",
        "\n",
        "  for i in range(n):\n",
        "    mark[i] = i\n",
        "    L_indices[L_indptr[i]] = i\n",
        "\n",
        "    for indptr in range(A_indptr[i], A_indptr[i+1]):\n",
        "      node = A_indices[indptr]\n",
        "      while node < i and mark[node] != i:\n",
        "        mark[node] = i\n",
        "        L_indices[col_ptr[node]] = i\n",
        "        col_ptr[node] += 1\n",
        "        node = parent[node]\n",
        "  \n",
        "  return (L_indices, L_indptr)"
      ],
      "id": "40b90f75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Does it work?\n"
      ],
      "id": "96e52483"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A_indices, A_indptr, A_x, A = make_matrix(13)\n",
        "parent, col_count = etree_base(A.indices, A.indptr)\n",
        "\n",
        "L_indices, L_indptr = symbolic_cholesky_base(A.indices, A.indptr, parent, col_count)\n",
        "L_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n",
        "\n",
        "print(all(x==y for (x,y) in zip(L_indices, L_indices_true)))\n",
        "print(all(x==y for (x,y) in zip(L_indptr, L_indptr_true)))"
      ],
      "id": "cde9efa6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fabulosa!\n",
        "\n",
        "Now let's do the compiled version.\n"
      ],
      "id": "5a426263"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functools import partial\n",
        "@partial(jit, static_argnums = (4,))\n",
        "def symbolic_cholesky(A_indices, A_indptr, L_indptr, parent, nnz):\n",
        "  \n",
        "  ## innermost while loop\n",
        "  def body_while(val):\n",
        "    i, L_indices, L_indptr, node, parent, col_ptr, mark = val\n",
        "    mark = mark.at[node].set(i)\n",
        "    #p = \n",
        "    L_indices = L_indices.at[col_ptr[node]].set(i)\n",
        "    col_ptr = col_ptr.at[node].add(1)\n",
        "    return (i, L_indices, L_indptr, parent[node], parent, col_ptr, mark)\n",
        "\n",
        "  def cond_while(val):\n",
        "    i, L_indices, L_indptr, node, parent, col_ptr, mark = val\n",
        "    return lax.bitwise_and(lax.lt(node, i), lax.ne(mark[node], i))\n",
        "\n",
        "  ## Inner for loop\n",
        "  def body_inner_for(indptr, val):\n",
        "    i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = val\n",
        "    node = A_indices[indptr]\n",
        "    i, L_indices, L_indptr, node, parent, col_ptr, mark = lax.while_loop(cond_while, body_while, (i, L_indices, L_indptr, node, parent, col_ptr, mark))\n",
        "    return (i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n",
        "  \n",
        "  ## Outer for loop\n",
        "  def body_out_for(i, val):\n",
        "     A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = val\n",
        "     mark = mark.at[i].set(i)\n",
        "     L_indices = L_indices.at[L_indptr[i]].set(i)\n",
        "     i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = lax.fori_loop(A_indptr[i], A_indptr[i+1], body_inner_for, (i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark))\n",
        "     return (A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n",
        "\n",
        "  ## Body of code\n",
        "  n = len(A_indptr) - 1\n",
        "  col_ptr = L_indptr + 1\n",
        "  L_indices = jnp.zeros(nnz, dtype=int)\n",
        "  \n",
        "  mark = jnp.repeat(-1, n)\n",
        "  \n",
        "  init = (A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n",
        "  A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = lax.fori_loop(0, n, body_out_for, init)\n",
        "  return L_indices"
      ],
      "id": "93079be2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's check it works\n"
      ],
      "id": "ab05d3df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A_indices, A_indptr, A_x, A = make_matrix(20)\n",
        "parent, col_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "\n",
        "\n",
        "L_indices = symbolic_cholesky(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1])\n",
        "L_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n",
        "print(all(L_indices == L_indices_true))\n",
        "print(all(L_indptr == L_indptr_true))\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(31)\n",
        "\n",
        "parent, col_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "\n",
        "\n",
        "L_indices = symbolic_cholesky(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1])\n",
        "L_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n",
        "\n",
        "print(all(L_indices == L_indices_true))\n",
        "print(all(L_indptr == L_indptr_true))"
      ],
      "id": "5f56fde9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success!\n",
        "\n",
        "One _minor_ problem. This is slow. as. balls.\n"
      ],
      "id": "576ca4c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "A_indices, A_indptr, A_x, A = make_matrix(50)\n",
        "parent, col_count = etree_base(A.indices, A.indptr)\n",
        "times = timeit.repeat(lambda: symbolic_cholesky_base(A.indices, A.indptr, parent, col_count),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(200)\n",
        "parent, col_count = etree_base(A.indices, A.indptr)\n",
        "times = timeit.repeat(lambda: symbolic_cholesky_base(A.indices, A.indptr, parent, col_count),number = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")"
      ],
      "id": "1d55716b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here is our JAX'd and JIT'd code.\n"
      ],
      "id": "ae88c9e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "A_indices, A_indptr, A_x, A = make_matrix(50)\n",
        "parent, col_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda:symbolic_cholesky(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1]),number = 1, repeat = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(200)\n",
        "parent, col_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda:symbolic_cholesky(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1]),number = 1, repeat = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")"
      ],
      "id": "6e6915eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oooof. Something is going horribly wrong.\n",
        "\n",
        "### Why is it so slow?\n",
        "\n",
        "The first thing to check is if it's the compile time. We can do this by explicitly _lowering_ the the JIT'd function to its XLA representation and then compiling it.\n"
      ],
      "id": "820e4e49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "A_indices, A_indptr, A_x, A = make_matrix(50)\n",
        "parent, col_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda: jit(partial(symbolic_cholesky, nnz=int(L_indptr[-1]))).lower(A.indices, A.indptr, L_indptr, parent).compile(),number = 1, repeat = 5)\n",
        "print(f\"Compilation time: n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(200)\n",
        "parent, col_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda: jit(partial(symbolic_cholesky, nnz=int(L_indptr[-1]))).lower(A.indices, A.indptr, L_indptr, parent).compile(),number = 1, repeat = 5)\n",
        "print(f\"Compilation time: n = {A.shape[0]}: {[round(t,2) for t in times]}\")"
      ],
      "id": "8539ca35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is not the compile time.\n",
        "\n",
        "And that is actually a good thing because that suggests that we aren't having problems with the compiler unrolling all of our wonderful loops! But that does mean that we have to look a bit deeper into the code. Some smart people would probably be able to look at the `jaxpr` intermediate representation to diagnose the problem. But I couldn't see anything there.\n",
        "\n",
        "Instead I thought _if I were a clever, efficient compiler, what would I have problems with?_. And the answer is the classic sparse matrix answer: indirect indexing. \n",
        "\n",
        "The only structural difference between the `etree` function and the `symbolic_cholesky` function is this line in the `body_while()` function:\n"
      ],
      "id": "496477f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|eval: false\n",
        " L_indices = L_indices.at[col_ptr[node]].set(i)"
      ],
      "id": "522256a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to evaluate this code, the compiler has to resolve _two levels_ of indirection. By contrast, the indexing in `etree()` was always direct. So let's see what happens if we take the same function and remove that double indirection.\n"
      ],
      "id": "7c4026ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@partial(jit, static_argnums = (4,))\n",
        "def test_fun(A_indices, A_indptr, L_indptr, parent, nnz):\n",
        "  \n",
        "  ## innermost while loop\n",
        "  def body_while(val):\n",
        "    i, L_indices, L_indptr, node, parent, col_ptr, mark = val\n",
        "    mark = mark.at[node].set(i)\n",
        "    L_indices = L_indices.at[node].set(i)\n",
        "    col_ptr = col_ptr.at[node].add(1)\n",
        "    return (i, L_indices, L_indptr, parent[node], parent, col_ptr, mark)\n",
        "\n",
        "  def cond_while(val):\n",
        "    i, L_indices, L_indptr, node, parent, col_ptr, mark = val\n",
        "    return lax.bitwise_and(lax.lt(node, i), lax.ne(mark[node], i))\n",
        "\n",
        "  ## Inner for loop\n",
        "  def body_inner_for(indptr, val):\n",
        "    i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = val\n",
        "    node = A_indices[indptr]\n",
        "    i, L_indices, L_indptr, node, parent, col_ptr, mark = lax.while_loop(cond_while, body_while, (i, L_indices, L_indptr, node, parent, col_ptr, mark))\n",
        "    return (i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n",
        "  \n",
        "  ## Outer for loop\n",
        "  def body_out_for(i, val):\n",
        "     A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = val\n",
        "     mark = mark.at[i].set(i)\n",
        "     L_indices = L_indices.at[L_indptr[i]].set(i)\n",
        "     i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = lax.fori_loop(A_indptr[i], A_indptr[i+1], body_inner_for, (i, A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark))\n",
        "     return (A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n",
        "\n",
        "  ## Body of code\n",
        "  n = len(A_indptr) - 1\n",
        "  col_ptr = L_indptr + 1\n",
        "  L_indices = jnp.zeros(nnz, dtype=int)\n",
        "  \n",
        "  mark = jnp.repeat(-1, n)\n",
        "  \n",
        "  init = (A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark)\n",
        "  A_indices, A_indptr, L_indices, L_indptr, parent, col_ptr, mark = lax.fori_loop(0, n, body_out_for, init)\n",
        "  return L_indices\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(50)\n",
        "parent, col_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda:test_fun(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1]),number = 1, repeat = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(200)\n",
        "parent, col_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda:test_fun(A.indices, A.indptr, L_indptr, parent, nnz = L_indptr[-1]),number = 1, repeat = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")"
      ],
      "id": "d2fbcfe2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That isn't conclusive, but it does indicate that this might^[I mean, I'm pretty sure it is. I'm writing this post in order, so I don't know yet. But surely the compiler can't reason about the possible values of `node`, which would be the only thing that would speed this up.] be the problem. \n",
        "\n",
        "And this is a _big_ problem for us! The sparse Cholesky algorithm has similar amounts of indirection. So we need to fix it.\n",
        "\n",
        "### Attempt 2: After some careful thought, things stayed the same\n",
        "\n",
        "Now. I want to pretend that I've got elegant ideas about this. But I don't. So let's just do it. The most obvious thing to do is to use the algorithm to get the non-zero structure of the _rows_ of $L$. These are the things that are being indexed by `col_ptr[node]]`, so if we have these explicitly we don't need multiple indirection. We also don't need a while loop.\n",
        "\n",
        "In fact, if we have the non-zero structure of the rows of $L$, we can turn that into the non-zero structure of the columns in linear-ish^[Convert from CSR to `(i, j, val)` (called COO, which has a convenient implementation in `jax.experimental.sparse`) to CSC. This involves a linear pass, a sort, and another linear pass. So it's $n \\log n`ish. Hire me fancy tech companies. I can count. Just don't ask me to program quicksort.] time.\n",
        "\n",
        "All we need to do is make sure that our `etree()` function is also counting the number of nonzeros in each row.\n"
      ],
      "id": "883a9125"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@jit\n",
        "def etree(A_indices, A_indptr):\n",
        " # print(\"(Re-)compiling etree(A_indices, A_indptr)\")\n",
        "  ## innermost while loop\n",
        "  def body_while(val):\n",
        "  #  print(val)\n",
        "    j, node, parent, col_count, row_count, mark = val\n",
        "    update_parent = lambda x: x[0].at[x[1]].set(x[2])\n",
        "    parent = lax.cond(lax.eq(parent[node], -1), update_parent, lambda x: x[0], (parent, node, j))\n",
        "    mark = mark.at[node].set(j)\n",
        "    col_count = col_count.at[node].add(1)\n",
        "    row_count = row_count.at[j].add(1)\n",
        "    return (j, parent[node], parent, col_count, row_count, mark)\n",
        "\n",
        "  def cond_while(val):\n",
        "    j, node, parent, col_count, row_count, mark = val\n",
        "    return lax.bitwise_and(lax.lt(node, j), lax.ne(mark[node], j))\n",
        "\n",
        "  ## Inner for loop\n",
        "  def body_inner_for(indptr, val):\n",
        "    j, A_indices, A_indptr, parent, col_count, row_count, mark = val\n",
        "    node = A_indices[indptr]\n",
        "    j, node, parent, col_count, row_count, mark = lax.while_loop(cond_while, body_while, (j, node, parent, col_count, row_count, mark))\n",
        "    return (j, A_indices, A_indptr, parent, col_count, row_count, mark)\n",
        "  \n",
        "  ## Outer for loop\n",
        "  def body_out_for(j, val):\n",
        "     A_indices, A_indptr, parent, col_count, row_count, mark = val\n",
        "     mark = mark.at[j].set(j)\n",
        "     j, A_indices, A_indptr, parent, col_count, row_count, mark = lax.fori_loop(A_indptr[j], A_indptr[j+1], body_inner_for, (j, A_indices, A_indptr, parent, col_count, row_count, mark))\n",
        "     return (A_indices, A_indptr, parent, col_count, row_count, mark)\n",
        "\n",
        "  ## Body of code\n",
        "  n = len(A_indptr) - 1\n",
        "  parent = jnp.repeat(-1, n)\n",
        "  mark = jnp.repeat(-1, n)\n",
        "  col_count = jnp.repeat(1,  n)\n",
        "  row_count = jnp.repeat(1, n)\n",
        "  init = (A_indices, A_indptr, parent, col_count, row_count, mark)\n",
        "  A_indices, A_indptr, parent, col_count, row_count, mark = lax.fori_loop(0, n, body_out_for, init)\n",
        "  return (parent, col_count, row_count)"
      ],
      "id": "66fcd940",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check that the code is actually doing what I want.\n"
      ],
      "id": "4464df38"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A_indices, A_indptr, A_x, A = make_matrix(57)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indices, L_indptr = _symbolic_factor(A_indices, A_indptr)\n",
        "\n",
        "true_parent = L_indices[L_indptr[:-2] + 1]\n",
        "true_parent[np.where(np.diff(L_indptr[:-1]) == 1)] = -1\n",
        "print(all(x == y for (x,y) in zip(parent[:-1], true_parent)))\n",
        "\n",
        "true_col_count  = np.diff(L_indptr)\n",
        "print(all(true_col_count == col_count))\n",
        "\n",
        "true_row_count = np.array([len(np.where(L_indices == i)[0]) for i in range(57**2)])\n",
        "print(all(true_row_count == row_count))"
      ],
      "id": "26a9fe8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Excellent! With this we can modify our previous function to give us the row-indices of the non-zero pattern instead. Just for further chaos, please note that we are using a CSC representation of $A$ to get a CSR representation of $L$. \n",
        "\n",
        "Once again, we will prototype in pure python and then translate to JAX. The thing to look out for this time is that we _know_ how many non-zeros there are in a row and we know where we need to put them. This suggests that we can compute these things in `body_inner_for` and then do a vectorised version of our indirect indexing. This should compile down to a single [XLA `scatter` call](https://www.tensorflow.org/xla/operation_semantics#scatter). This will reduce the number of overall `scatter` calls from $\\operatorname(nnz)(L)$ to $n$. And hopefully this will fix things.\n"
      ],
      "id": "abb33041"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def symbolic_cholesky2_base(A_indices, A_indptr, L_indptr, row_count, parent, nnz):\n",
        "\n",
        "  n = len(A_indptr) - 1\n",
        "  col_ptr = L_indptr + 1\n",
        "  L_indices = np.zeros(L_indptr[-1], dtype=int)\n",
        "  mark = [-1] * n\n",
        "\n",
        "  for i in range(n):\n",
        "    mark[i] = i\n",
        "    row_ind = np.repeat(nnz+1, row_count[i])\n",
        "    row_ind[-1] = L_indptr[i]\n",
        "    counter = 0\n",
        "    for indptr in range(A_indptr[i], A_indptr[i+1]):\n",
        "      node = A_indices[indptr]\n",
        "      while node < i and mark[node] != i:\n",
        "        mark[node] = i\n",
        "        row_ind[counter] = col_ptr[node]\n",
        "        col_ptr[node] += 1\n",
        "        node = parent[node]\n",
        "        counter +=1\n",
        "    L_indices[row_ind] = i\n",
        "  \n",
        "  return L_indices\n",
        "\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(13)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "\n",
        "\n",
        "L_indices = symbolic_cholesky2_base(A.indices, A.indptr, L_indptr, row_count, parent, L_indptr[-1])\n",
        "L_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n",
        "\n",
        "print(all(x==y for (x,y) in zip(L_indices, L_indices_true)))\n",
        "print(all(x==y for (x,y) in zip(L_indptr, L_indptr_true)))"
      ],
      "id": "758c79c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Excellent. Now let's JAX this. The JAX-heads among you will notice that we have a subtle^[Replace \"subtle\" with \"fairly obvious once I realised how it's converted to a `lax.scan`, but not at all obvious to me originally\".] problem: in a `fori_loop`, JAX does not treat `i` as static, which means that the length of the repeat (`row_count[i]`) can never be static and it therefore can't be traced.\n",
        "\n",
        "Shit.\n",
        "\n",
        "It is hard to think of a good option here. A few months back Junpeng Lao^[Who demanded a footnote.] [sent me a script](https://gist.github.com/junpenglao/f5b48c34dd8ea5029202fb607806ea0f#file-sparse-cholesky-in-jax-ipynb) with his attempts at making the Cholesky stuff JAX transformable. And he hit the same problem. I was, in an act of hubris, trying very hard to not end up here. But that was tragically slow. So here we are.\n",
        "\n",
        "He came up with two methods.\n",
        "\n",
        "1. Pad out `row_ind` so it's always long enough. This only costs memory. The maximum size of `row_ind` is `n`. Unfortunately, this happens whenever $A$ has a dense row. Sadly, for Bayesian^[This also happens with the profile likelihood in non-Bayesian methods.] linear mixed models this will happen if we put Gaussian priors on the covariate coefficients^[the $\\beta$s] and we try to marginalise them out with the other multivariate Gaussian parts. It is possible to write the routines that deal with dense rows and columns explicitly, but it's a pain in the arse.\n",
        "\n",
        "1. Do some terrifying work with `lax.scan` and dynamic slicing.\n",
        "\n",
        "I'm going to try the first of these options.\n"
      ],
      "id": "bcef67ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@partial(jit, static_argnums = (5, 6))\n",
        "def symbolic_cholesky2(A_indices, A_indptr, L_indptr, row_count, parent, nnz, max_row):\n",
        "  ## innermost while loop\n",
        "  def body_while(val):\n",
        "    i, counter, row_ind, node, col_ptr, mark = val\n",
        "    mark = mark.at[node].set(i)\n",
        "    row_ind = row_ind.at[counter].set(col_ptr[node])\n",
        "    col_ptr = col_ptr.at[node].add(1)\n",
        "    return (i, counter + 1, row_ind, parent[node], col_ptr, mark)\n",
        "\n",
        "  def cond_while(val):\n",
        "    i, counter, row_ind, node, col_ptr, mark = val\n",
        "    return lax.bitwise_and(lax.lt(node, i), lax.ne(mark[node], i))\n",
        "\n",
        "  ## Inner for loop\n",
        "  def body_inner_for(indptr, val):\n",
        "    i, counter, row_ind, parent, col_ptr, mark = val\n",
        "    node = A_indices[indptr]\n",
        "    i, counter, row_ind, node, col_ptr, mark = lax.while_loop(cond_while, body_while, (i, counter, row_ind, node, col_ptr, mark))\n",
        "    return (i, counter, row_ind, parent, col_ptr, mark)\n",
        "  \n",
        "  ## Outer for loop\n",
        "  def body_out_for(i, val):\n",
        "     L_indices, parent, col_ptr, mark = val\n",
        "     mark = mark.at[i].set(i)\n",
        "     row_ind = jnp.repeat(nnz+1, max_row)\n",
        "     row_ind = row_ind.at[row_count[i]-1].set(L_indptr[i])\n",
        "     counter = 0\n",
        "\n",
        "     i, counter, row_ind, parent, col_ptr, mark = lax.fori_loop(A_indptr[i], A_indptr[i+1], body_inner_for, (i, counter, row_ind, parent, col_ptr, mark))\n",
        "\n",
        "     L_indices = L_indices.at[row_ind].set(i, mode = \"drop\")\n",
        "     return (L_indices, parent, col_ptr, mark)\n",
        "\n",
        "  ## Body of code\n",
        "  n = len(A_indptr) - 1\n",
        "\n",
        "  col_ptr = jnp.array(L_indptr + 1)\n",
        "  L_indices = jnp.ones(nnz, dtype=int) * (-1)\n",
        "  mark = jnp.repeat(-1, n)\n",
        "\n",
        "  ## Make everything a jnp array. Really should use jaxtyping\n",
        "  A_indices = jnp.array(A_indices)\n",
        "  A_indptr = jnp.array(A_indptr)\n",
        "  L_indptr = jnp.array(L_indptr)\n",
        "  row_count = jnp.array(row_count)\n",
        "  parent = jnp.array(parent)\n",
        "\n",
        "  init = (L_indices, parent, col_ptr, mark)\n",
        "  L_indices, parent, col_ptr, mark = lax.fori_loop(0, n, body_out_for, init)\n",
        "  return L_indices"
      ],
      "id": "8189a4b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok. Let's see if that worked.\n"
      ],
      "id": "3b88026a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A_indices, A_indptr, A_x, A = make_matrix(20)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "\n",
        "\n",
        "L_indices = symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count)))\n",
        "L_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n",
        "print(all(L_indices == L_indices_true))\n",
        "print(all(L_indptr == L_indptr_true))\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(31)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "\n",
        "\n",
        "L_indices = symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count)))\n",
        "L_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n",
        "print(all(L_indices == L_indices_true))\n",
        "print(all(L_indptr == L_indptr_true))"
      ],
      "id": "652279fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok. Once more into the breach. Is this any better?\n"
      ],
      "id": "8c7e7f27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "A_indices, A_indptr, A_x, A = make_matrix(50)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda:symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count))),number = 1, repeat = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(200)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda:symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count))),number = 1, repeat = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "# A_indices, A_indptr, A_x, A = make_matrix(300)\n",
        "# parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "# L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "# L_indptr[1:] = np.cumsum(col_count)\n",
        "# times = timeit.repeat(lambda:symbolic_cholesky2(A.indices, A.indptr, L_indptr, row_count, parent, nnz = int(L_indptr[-1]), max_row = int(max(row_count))),number = 1, repeat = 1)\n",
        "# print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")"
      ],
      "id": "a8c71f46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fuck. \n",
        "\n",
        "### Attempt 3: A desperate attempt to make this bloody work\n",
        "\n",
        "Right. Let's try again. What if instead of doing all those scatters we instead, idk, just store two vectors and sort. Because at this point I will try fucking anything. What if we just list out the column index and row index as we find them (aka build the sparse matrix in COO^[COO stands for _coordinate list_ and it's the least space-efficient of our options. It directly stores 3 length `n` vectors `(row, col, value)`. It's great for specifying matrices and it's pretty easy to convert from this format to any of the others.] format. The `jax.experimental.sparse` module has support for (blocked) COO objects but doesn't implement this transformation. `scipy.sparse` has a fast conversion routine so I'm going to use it. In the interest of being 100% JAX, I tried a version with `jnp.lexsort[index[1][jnp.lexsort((index[1], index[0]))]`], which basically does the same thing but it's a lot slower.\n"
      ],
      "id": "a8ffce9d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def symbolic_cholesky3(A_indices, A_indptr, L_indptr, parent, nnz):\n",
        "  @partial(jit, static_argnums = (4,))\n",
        "  def _inner(A_indices_, A_indptr_, L_indptr, parent, nnz):\n",
        "    ## Make everything a jnp array. Really should use jaxtyping\n",
        "    A_indices_ = jnp.array(A_indices_)\n",
        "    A_indptr_ = jnp.array(A_indptr_)\n",
        "    L_indptr = jnp.array(L_indptr)\n",
        "    parent = jnp.array(parent)\n",
        "\n",
        "    ## innermost while loop\n",
        "    def body_while(val):\n",
        "      index, i, counter,  node,  mark = val\n",
        "      mark = mark.at[node].set(i)\n",
        "      index[0] = index[0].at[counter].set(node) #column\n",
        "      index[1] = index[1].at[counter].set(i) # row\n",
        "      return (index, i, counter + 1, parent[node], mark)\n",
        "\n",
        "    def cond_while(val):\n",
        "      index, i, counter,  node,  mark = val\n",
        "      return lax.bitwise_and(lax.lt(node, i), lax.ne(mark[node], i))\n",
        "\n",
        "    ## Inner for loop\n",
        "    def body_inner_for(indptr, val):\n",
        "      index, i, counter, mark = val\n",
        "      node = A_indices_[indptr]\n",
        "      \n",
        "      index, i, counter,  node,  mark = lax.while_loop(cond_while, body_while, (index, i, counter,  node,  mark))\n",
        "      return (index, i, counter,  mark)\n",
        "    \n",
        "    ## Outer for loop\n",
        "    def body_out_for(i, val):\n",
        "      index, counter,  mark = val\n",
        "      mark = mark.at[i].set(i)\n",
        "      index[0] = index[0].at[counter].set(i)\n",
        "      index[1] = index[1].at[counter].set(i)\n",
        "      counter = counter + 1\n",
        "      index, i, counter, mark = lax.fori_loop(A_indptr_[i], A_indptr_[i+1], body_inner_for, (index, i, counter,  mark))\n",
        "\n",
        "      return (index, counter,  mark)\n",
        "\n",
        "    ## Body of code\n",
        "    n = len(A_indptr_) - 1\n",
        "    mark = jnp.repeat(-1, n)\n",
        "\n",
        "    index = [jnp.zeros(nnz, dtype=int), jnp.zeros(nnz, dtype=int)]\n",
        "    counter = 0\n",
        "\n",
        "    init = (index, counter, mark)\n",
        "    index, counter, mark = lax.fori_loop(0, n, body_out_for, init)\n",
        "    \n",
        "    return index\n",
        "  n = len(A_indptr) - 1\n",
        "  index = _inner(A_indices, A_indptr, L_indptr, parent, nnz)\n",
        "  ## return jnp.lexsort[index[1][jnp.lexsort((index[1], index[0]))\n",
        "  return sparse.coo_array((np.ones(nnz), (index[1], index[0])), shape = (n,n)).tocsc().indices"
      ],
      "id": "9b5c1823",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First things first, let's check how fast this is.\n"
      ],
      "id": "cd2af811"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A_indices, A_indptr, A_x, A = make_matrix(15)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "L_indices = symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1]))\n",
        "L_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n",
        "print(all(L_indices == L_indices_true))\n",
        "print(all(L_indptr == L_indptr_true))\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(31)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "L_indices = symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1]))\n",
        "L_indices_true, L_indptr_true = _symbolic_factor(A_indices, A_indptr)\n",
        "print(all(L_indices == L_indices_true))\n",
        "print(all(L_indptr == L_indptr_true))\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(50)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda: symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1])),number = 1, repeat = 5)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(200)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda: symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1])),number = 1, repeat = 5)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(300)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda: symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1])),number = 1, repeat = 5)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")\n",
        "\n",
        "A_indices, A_indptr, A_x, A = make_matrix(1000)\n",
        "parent, col_count, row_count = etree(A.indices, A.indptr)\n",
        "L_indptr = np.zeros(A.shape[0]+1, dtype=int)\n",
        "L_indptr[1:] = np.cumsum(col_count)\n",
        "times = timeit.repeat(lambda: symbolic_cholesky3(A.indices, A.indptr, L_indptr, parent, nnz = int(L_indptr[-1])),number = 1, repeat = 1)\n",
        "print(f\"n = {A.shape[0]}: {[round(t,2) for t in times]}\")"
      ],
      "id": "085525e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You know what? I'll take it. It's not perfect, in particular I would prefer a pure JAX solution. But everything I tried hit hard against the indirect memory access issue. The best I found was using `jnp.lexsort` but even it had noticeable performance degradation as `nnz` increased relative to the scipy solution. \n",
        "\n",
        "# Next time on Sparse Matrices with Dan\n",
        "\n",
        "So that's where I'm going to leave it. I am off my flight and I've slept very well and now I'm going to be on holidays for a little while.\n",
        "\n",
        "The next big thing to do is look at the numerical factorisation. We are going to run headlong into all of the problems we've hit today, so that should be fun. The reason why I'm separating it into a separate post^[other than holiday] is that I want to actually test all of those things out properly.\n",
        "\n",
        "So next time you can expect\n",
        "\n",
        "1. Classes! Because frankly this code is getting far too messy, especially now that certain things need to be passed as static arguments. The only reason I've avoided it up to now is that I think it hides too much of the algorithm in boilerplate. But now the boilerplate is ruining my life and causing far too many dumb typos^[`A_index` and `A.index` are different].\n",
        "\n",
        "1. Type hints! Because for a language where types aren't explicit, they sure are important. Also because I'm going to class it up I might as well do it properly.\n",
        "\n",
        "1. Some helper routines! I'm going to need a sparse-matrix scatter operation (aka the structured copy of $A$ to have the sparsity pattern of $L$)! And I'm certainly going to need some reorderings^[I'm probably going to bind Eigen's AMD decomposition. I'm certainly not writing it myself.]  \n",
        "\n",
        "1. A battle royale between padded and non-padded methods! \n",
        "\n",
        "It should be a fun time!"
      ],
      "id": "3d427e8c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}