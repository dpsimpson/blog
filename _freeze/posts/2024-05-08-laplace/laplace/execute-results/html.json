{
  "hash": "8138d057532ef926284e96fd7c98ac31",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won''t you buy me a Laplace approximation'\ndescription: |\n  Exploiting linearity and sparisty to speed up JAX Hessians and slowly ruin my life.\ndate: '2024-05-08'\nimage: hat.jpg\ncategories:\n  - JAX\n  - Laplace approximation\n  - Sparse matrices\n  - Autodiff\ntwitter-card:\n  title: Lord won't you buy me a Laplace approximation\n  creator: '@dan_p_simpson'\ncitation:\n  url: 'https://dansblog.netlify.app/posts/2024-05-08-laplace/laplace.html'\nformat:\n  html:\n    df-print: paged\ndraft: false\nexecute:\n  eval: true\n---\n\nI am, once again, in a bit of a mood. And the only thing that will fix my mood is \na good martini and a Laplace approximation. And I'm all out of martinis.\n\nTo be honest I started writing this post in February 2023, but then got distracted by visas and jobs and all that jazz. But I felt the desire to finish it, so here we are. I wonder how much I will want to re-write^[I will never reveal how much. But it was most of it.]\n\nThe post started as a pedagogical introduction to Laplace approximations (for reasons I don't fully remember), but it rapidly went off the rails. So strap yourself in^[or on] for a tour through the basics of sparse autodiff and a tour through manipulating the `jaxpr` intermediate representation in order to make one very simple logistic regression produce autodiff code that is almost as fast as a manually programmed gradient.\n\n## The Laplace approximation\n\nOne of the simplest approximations to a distribution is the Laplace approximation. It\nbe defined as the Gaussian distribution that matches the location\nand the curvature at the mode of the target distribution. It lives its best life when\nthe density is of the form\n$$\np(x) \\propto \\exp(-nf_n(x)),\n$$\nwhere $f_n$ is a sequence of functions^[that probably converges. Think of it like $f_n(\\theta) = n^{-1} \\left(\\sum_{i=1}^n p(y_i \\mid \\theta) + p(\\theta)\\right)$, where $p(y_i \\mid \\theta)$ is the likelihood and $p(\\theta)$ is the prior.]. Let's imagine that we want to approximate the normalized density $p(x)$\nnear the mode $x^*$. We can do this by taking the second order \nTaylor expansion of $f_n$ around $x=x_0$, which is \n$$\nf_n = f_n(x^*) + (x-x^*)^TH(x^*)(x-x^*)  + \\mathcal{O}((x-x^*)^3),\n$$\nwhere^[The first-order term disappears because at the mode $x^*$ $\\nabla f(x^*)=0$]\n$$\n[H(x^*)]_{ij} = \\frac{\\partial^2 f_n}{\\partial x_i \\partial x_j}\n$$\nis the Hessian matrix. \n\nIf we replace $f_n$ by its quadratic approximation we get \n$$\np(x) \\approx  C\\exp(-n(x- x^*)^TH(x^*)(x-x^*)),\n$$\nwhere $C$ is a constant.\n\nAfter normalizing the approximation to make sure that we get a proper density, we get the Laplace approximation\n$$\np(x) \\approx N(x^*, n^{-1}H(x^*)^{-1}).\n$$\n\nThe Laplace approximation can be justified rigorously and has\na well-studied error and it's known to work quite well when $p(x)$ is a) unimodal^[or has one dominant mode] and b) isn't tooooo non-Gaussian.\n\nIn practice, people have found that Laplace approximations \ndo a reasonable^[Something isn't always better than nothing but sometimes it is] job quantifying uncertainty [even in complex neural network models](https://arxiv.org/abs/2106.14806) and it is at the heart of any number of classical estimators in statistics.\n\nFrom an implementation perspective, the Laplace approximation is pretty simple. It's just a two step process:\n\n::: {.algorithm}\n1. Find the mode $x^* = \\arg \\max_x f_n(x)$ using your favorite optimizer\n\n2. Compute the Hessian $H(x^*)$.\n:::\n\nIn a Bayesian context, we typically take \n$$\nf_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\log p(y_i \\mid x) + \\frac{1}{n} \\log p(x),\n$$\nwhich will lead to a Gaussian approximation to the posterior distribution.\nBut this post really isn't about Bayes. It's about Laplace approximations.\n\n### Computing the Laplace approximation in JAX\n\nThis is a two step process and, to be honest, all of the steps\nare pretty standard. So (hopefully) this will not be too tricky\nto implement. For simplicity, I'm not going to bother with the\ndividing and multiplying by $n$, although for very large data\nit could be quite \n\n::: {#4cea8f03 .cell execution_count=1}\n``` {.python .cell-code}\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\nfrom jax.scipy.special import expit\nfrom jax import jacfwd, grad\nfrom jax import Array\nfrom typing import Callable, Tuple, List, Set, Dict\n\ndef laplace(f: Callable, x0: Array) -> Array:\n    nx = x0.shape[0]\n    mode, *details = minimize(lambda x: -f(x), x0, method = \"BFGS\")\n    H =  -1.0 * jacfwd(grad(f))(mode)\n    return mode, H\n```\n:::\n\n\nThere are a few things worth noting here. There's not really much in this code, except to note that `jax.scipy.optimize.minimize` finds the minimum of $f$, so I had to pass through the negative of the function. This change also propagates to the computation of the Hessian, which is computed as the Jacobian of the gradient of f. \n\nDepending on what needs to be done with the Laplace approximation, it might be more appropriate to output the log-density rather than\njust the mode and the Hessian, but for the moment we will keep this signature.\n\nLet's try it out. First of all, I'm going to generate some random \ndata from a logistic regression model. This is going to use [Jax's slightly odd random number system where you need to manually update the state of the pseudo-random number generator](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html). \nThis is beautifully repeatable^[You could say reproducible code but I won't because that word means something pretty specific. I mean, this is not the place for a rant, but it is _very_ difficult to write strictly reproducible code and I am frankly not even going to try to take a bite out of that particular onion.] unlike, say, R or standard numpy, where you've got to pay _a lot_ of attention to the state of the random number generator to avoid oddities.\n\n::: {#938fe91c .cell execution_count=2}\n``` {.python .cell-code}\nfrom jax import random as jrandom\n\ndef make_data(key, n: int, p: int) -> Tuple[Array, Array]:\n  key, sub = jrandom.split(key)\n  X = jrandom.normal(sub, shape = (n,p)) /jnp.sqrt(p)\n\n  key, sub = jrandom.split(key)\n  beta = 0.5 * jrandom.normal(sub, shape = (p,))\n  key, sub = jrandom.split(key)\n  beta0 = jrandom.normal(sub)\n\n\n  key, sub = jrandom.split(key)\n  y = jrandom.bernoulli(sub, expit(beta0 + X @ beta))\n\n  return (y, X)\n```\n:::\n\n\nAn interesting side-note here is that I've generated the design matrix $X$ to have standard Gaussian columns. This is _not_ a benign choice as $n$ gets big. With _very_ high probability, the columns of $X$ will be almost^[The maths under this is very interesting and surprisingly accessible (in a very advanced sort of way). I guess it depends on what you think of as accessible, but it's certainly much nicer than entropy and VC-classes. A lovely set of notes that cover everything you've ever wanted to know is [here](https://arxiv.org/abs/1011.3027)] orthonormal, which means that this is the best possible case for logistic regression. Generally speaking, design matrices from real^[Unless someone's been doing their design of experiments] data have a great deal of co-linearity in them and so algorithms that perform well on random design matrices may perform less well on real data.\n\nOk, so let's fit the model! I'm just going to use $N(0,1)$ priors on all of the $\\beta$s. \n\n::: {#58eec67f .cell execution_count=3}\n``` {.python .cell-code}\nfrom functools import partial\nn = 100\np = 5\n\nkey = jrandom.PRNGKey(30127)\ny, X = make_data(key, n, p)\n\ndef log_posterior(beta: Array, X: Array, y: Array) -> Array:\n    assert beta.shape[0] == X.shape[1] + 1\n\n    prob = expit(beta[0] + X @ beta[1:])\n    \n    return (\n      jnp.sum(y * jnp.log(prob) + \n      (1-y) * jnp.log1p(-prob)) - \n      0.5 * jnp.dot(beta, beta)\n    )\n\n\npost_mean, H = laplace(\n  partial(log_posterior, X = X, y = y),\n  x0 =jnp.zeros(X.shape[1] + 1)\n)\n\npost_cov = jnp.linalg.inv(H)\n```\n:::\n\n\nLet's see how this performs relative to MCMC. To do that, I'm going to build and equivalent PyMC model.\n\n::: {#28ec7812 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pymc as pm\nimport pandas as pd\n\nwith pm.Model() as logistic_reg:\n  beta = pm.Normal('beta', 0, 1, shape = (p+1,))\n  linpred = beta[0] + pm.math.dot(np.array(X), beta[1:])\n  \n  pm.Bernoulli(\n    \"y\", \n    p = pm.math.invlogit(linpred),\n    observed = np.array(y)\n  )\n  posterior = pm.sample(\n    tune=1000, \n    draws=1000, \n    chains=4, \n    cores = 1)\n\n# I would like to apologize for the following pandas code.\ntmp = pm.summary(posterior)\ntmp = tmp.assign(\n  laplace_mean = post_mean, \n  laplace_sd = np.sqrt(np.diag(post_cov)), \n  Variable = tmp.index\n)[[\"Variable\", \"mean\", \"laplace_mean\", \"sd\", \"laplace_sd\"]]\n\nwith pd.option_context('display.precision', 3):\n    print(tmp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (4 chains in 1 job)\nNUTS: [beta]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 5 seconds.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n    <div>\n      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 0, 0 divergences]\n    </div>\n    \n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n    <div>\n      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 1, 0 divergences]\n    </div>\n    \n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n    <div>\n      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 2, 0 divergences]\n    </div>\n    \n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n    <div>\n      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 3, 0 divergences]\n    </div>\n    \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n        Variable   mean  laplace_mean     sd  laplace_sd\nbeta[0]  beta[0]  0.249         0.234  0.235       0.229\nbeta[1]  beta[1] -0.964        -0.914  0.435       0.428\nbeta[2]  beta[2] -1.710        -1.616  0.490       0.470\nbeta[3]  beta[3] -0.975        -0.926  0.423       0.416\nbeta[4]  beta[4] -0.739        -0.716  0.470       0.457\nbeta[5]  beta[5]  0.637         0.609  0.481       0.475\n```\n:::\n:::\n\n\nWell that's just dandy! Everything is pretty^[With 100 observations, we expect our data-driven variation (aka the frequentist version) to be about one decimal place, so the Laplace approximation is accurate within that tolerance. In fact, clever maths types can analyse the error in the Laplace approximation and show that the error is about $\\mathcal{O}(n^{-1})$, which is asymptotically much smaller than the sampling variability of $\\mathcal{O}(n^{-1/2})$, which suggests that the error introduced by the Laplace approximation isn't catastrophic. At least with enough data.] close. With 1000 observations it's identical to within 3 decimal places.\n\n### Speeding up the computation\n\nSo that is all well and dandy. Let's see how long it takes. I am interested in big models, so for this demonstration, I'm going to take $p = 5000$. That said, I'm not enormously interested in seeing how this scales in $n$ (linearly), so I'm going to keep that at the fairly unrealistic value of $n=1000$.\n\n::: {#4fda5153 .cell execution_count=5}\n``` {.python .cell-code}\nimport timeit\ndef hess_test(key, n, p):\n  y, X = make_data(key, n , p)\n  inpu = jnp.ones(p+1)\n  def hess():\n    f = partial(log_posterior, X = X, y = y)\n    return -1.0 * jacfwd(grad(f))(inpu)\n  return hess\n\nn = 1000\np = 5000\nkey, sub = jrandom.split(key)\nhess = hess_test(sub, n , p)\ntimes = timeit.repeat(hess, number = 5, repeat = 5)\nprint(f\"Autodiff: The average time with p = {p} is {np.mean(times): .3f}(+/-{np.std(times): .3f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAutodiff: The average time with p = 5000 is  3.222(+/- 0.379)\n```\n:::\n:::\n\n\nThat doesn't seem too bad, but the thing is that I know quite a \nlot about logistic regression. It is, after all, logistic \nregression. In particular, I know that the Hessian has the form\n$$\nH = X^T D(\\beta) X,\n$$\nwhere $D(\\beta)$ is a _diagonal_ $n \\times n$ matrix that has a known form.\n\nThis means that the appropriate comparison is between the speed of the autodiff Hessian and how long it takes to compute $X^TDX$ for some diagonal matrix X. \n\nNow you might be worried here that I didn't explicitly save $X$ and $y$, so the comparison might not be fair. But my friends, I have good news! All of that awkward `key, sub = jrandom.split(key)` malarkey has the singular advantage that if I pass the same key into `make_data` that I used for `hess_test`, I will get _the exact same generated data_! So let's do that. For $D$ I'm just going to pick a random matrix. This will give a _minimum_ achievable time for computing the Hessian (as it doesn't do the extra derivatives to compute $D$ properly).\n\nIf you look at that code and say _but Daniel you used the wrong multiplication operator_, you can convince yourself that `X * d[:, None]` gives the same result as `jnp.diag(d) @ X`. But it will be faster. And it uses such beautiful^[Be still my beating heart.] broadcasting rules.\n\n::: {#252d7828 .cell execution_count=6}\n``` {.python .cell-code}\ny, X = make_data(key, n , p)\nkey, sub = jrandom.split(key)\nd = jrandom.normal(sub, shape = (n,))\nmm = lambda: X.T @ (X * d[:, None])\ntimes = timeit.repeat(mm, number = 5, repeat = 5)\nprint(f\"Symbolic (minimum possible): The average time with p = {p} is {np.mean(times): .3f}(+/-{np.std(times): .3f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSymbolic (minimum possible): The average time with p = 5000 is  0.766(+/- 0.014)\n```\n:::\n:::\n\n\nOh dear. The symbolic derivative^[Ok. You caught me. They're not technically the same model. The symbolic code doesn't include an intercept. I just honestly cannot be arsed to do the very minor matrix algebra to add it in. Nor can I be arsed to add a column of ones to `X`.] is _a lot_ faster.\n\nSpeeding this up is going to take a little work. The first thing we can try is to explicitly factor out the linear transformation.\nInstead of passing in the function $f$, we could pass in $g$ such that \n$$\nf(x) = g(Ax),\n$$\nfor some matrix $A$. In our case $g$ would have a diagonal Hessian. Let's convince ourselves of that with a small example. As well as dropping the intercept, I've also dropped the prior term.\n\n::: {#3a085e1e .cell execution_count=7}\n``` {.python .cell-code}\ng = lambda prob: jnp.sum(y * jnp.log(prob) + (1-y) * jnp.log1p(-prob))\nkey, sub2 = jrandom.split(key)\ny, X = make_data(sub2, 5, 3)\nb = X @ jnp.ones(3)\nD = -1.0 * jacfwd(grad(g))(b)\nprint(np.round(D, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0.7 0.  0.  0.  0. ]\n [0.  3.7 0.  0.  0. ]\n [0.  0.  7.8 0.  0. ]\n [0.  0.  0.  4.9 0. ]\n [0.  0.  0.  0.  0.3]]\n```\n:::\n:::\n\n\nWonderfully diagonal!\n\n::: {#9afd020c .cell execution_count=8}\n``` {.python .cell-code}\ndef hess2(g, A, x):\n  # \n  b = A @ x\n  D = -1.0 * jacfwd(grad(g))(b)\n  H = A.T @ (A * jnp.diag(D)[:, None])\n  return H\n\ny, X = make_data(sub, n, p)\ng = lambda prob: jnp.sum(y * jnp.log(prob) + (1-y) * jnp.log1p(-prob))\nx0 = jnp.ones(p)\nh2 = lambda: hess2(g, X, x0)\ntimes = timeit.repeat(h2, number = 5, repeat = 5)\nprint(f\"Separated Hessian: The average time with p = {p} is {np.mean(times): .3f}(+/-{np.std(times): .3f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeparated Hessian: The average time with p = 5000 is  0.975(+/- 0.163)\n```\n:::\n:::\n\n\nWell that's definitely better.\n\nNow, we might be able to do even better than that if we notice that if we _know_ that $D$ is diagonal, then we don't need to compute the entire Hessian, we can simply compute the Hessian-vector product \n$$\n\\operatorname{diag}(H) = H 1 \\qquad \\text{iff }H\\text{ is diagonal},\n$$\nwhere $1$ is the vector of ones. Just as we computed the Hessian by computing the Jacobian of the gradient, it turns out that we can compute a Hessian-vector product by computing a Jacobian-vector product `jvp` of the gradient. The syntax in JAX is, honestly, a little bit gross here^[So many tuples], but if you want to read up about how it works [the docs are really nice](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode)^[This is in pretty stark contrast to the pytorch docs, which are shit. Be more like JAX.].\n\nThis observation is going to be useful because `jacfwd` computes the Jacobian by computing $n$ Jacobian-vector products. So this observation is saving us _a lot_ of work.\n\n::: {#279128f8 .cell execution_count=9}\n``` {.python .cell-code}\nfrom jax import jvp\ndef hess3(g, A, x):\n  # \n  b = A @ x\n  D = -1.0 * jvp(grad(g), (b,), (jnp.ones(n),))[1]\n  H = A.T @ (A * D[:, None])\n  return H\n\nh3 = lambda: hess3(g, X, x0)\ntimes = timeit.repeat(h3, number = 5, repeat = 5)\nprint(f\"Compressed Hessian: The average time with p = {p} is {np.mean(times): .3f}(+/-{np.std(times): .3f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCompressed Hessian: The average time with p = 5000 is  0.879(+/- 0.082)\n```\n:::\n:::\n\n\nThis is very nearly as fast as the lower bound for the symbolic Hessian. There must be a way to use this.\n\n## Can we automate this? Parsing JAX expressions\n\nSo that was all lovely and shiny. But the problem is that it was very labor intensive. I had to recognize both that you could write $f(x) = g(Ax)$ _and_ that $g$ would have a diagonal Hessian. That is, frankly, hard to do in general.\n\nIf I was building a system like [`bambi`](https://bambinos.github.io/bambi/) or [`brms`](https://paul-buerkner.github.io/brms/) or [INLA](https://www.r-inla.org/)^[INLA does this. Very explicitly. And a lot of other cool stuff. It doesn't use autodiff though.], where the model classes are relatively constrained, it's possible to automate both of these steps by analyzing the formula. But all I get is a function. So I need to work out how I can automatically parse the code for $f$ to find $g$ and $A$ (if they exist) and to determine if $g$ would have a sparse Hessian.\n\nWe can't do this easily with a standard Python program, but we can do it with JAX because it traces through the code and provides an _intermediate representation_ (IR)of the code. This is, incidentally, the first step that any code compiler uses. The beauty of an IR is that it abstracts away all of the specific user choices and provides a clean, logical representation of the program that can then be executed or, in our case, manipulated. These manipulations are, for example, key to how JAX computes gradients, how it JIT-compiles code, and how it does `vmap` and `pmap` operations. \n\nBut we can do more types of manipulations. In particular, we can take the IR and transform it into another IR that produces the same output in a more efficient way. Anyone who's familiar with compiled programming languages should know that this happens under that hood. They also probably know that compiler writers are small gods and I'm definitely not going to approach anywhere near that level of complexity in a blog post.\n\nSo what are our tasks. First of all we need to trace our way through the JAX code. We can do this by using the intermediate representation that JAX uses when transforming functions: the `jaxpr`s.\n\n### Getting to know jaxprs\n\nA `jaxpr` is a transformation of the python code for evaluating a JAX function into a human-readable language that maps types primitives through the code. We can view it using the `jax.make_jaxpr` function.\n\nLet's look at the log-posterior function after partial evaluation to make it a single-input function.\n\n::: {#bd5d6df2 .cell execution_count=10}\n``` {.python .cell-code}\nfrom jax import make_jaxpr\n\nlp = partial(log_posterior, X=X, y=y)\nprint(make_jaxpr(lp)(jnp.ones(p+1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{ lambda a:f32[1000,5000] b:bool[1000]; c:f32[5001]. let\n    d:f32[1] = dynamic_slice[slice_sizes=(1,)] c 0\n    e:f32[] = squeeze[dimensions=(0,)] d\n    f:f32[5000] = dynamic_slice[slice_sizes=(5000,)] c 1\n    g:f32[1000] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a f\n    h:f32[1000] = add e g\n    i:f32[1000] = logistic h\n    j:f32[1000] = log i\n    k:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b\n    l:f32[1000] = mul k j\n    m:i32[1000] = convert_element_type[new_dtype=int32 weak_type=True] b\n    n:i32[1000] = sub 1 m\n    o:f32[1000] = neg i\n    p:f32[1000] = log1p o\n    q:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] n\n    r:f32[1000] = mul q p\n    s:f32[1000] = add l r\n    t:f32[] = reduce_sum[axes=(0,)] s\n    u:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] c c\n    v:f32[] = mul 0.5 u\n    w:f32[] = sub t v\n  in (w,) }\n```\n:::\n:::\n\n\nThis can be a bit tricky to read the first time you see it, but it's waaaay easier that X86-Assembly or the LLVM-IR. Basically it says that to compute `lp(jnp.ones(p+1))` you need to run through this program. The first line gives the inputs (with types and shapes). Then after the `let` statement, there are a the commands that need to be executed in order. A single execution looks like \n```\nd:f32[1] = dynamic_slice[slice_sizes=(1,)] c 0\n``` \n\nThis can be read as _take a slice of vector `c` starting at `0` of shape `(1,)` and store it in `d`, which is a 1-dimensional 32bit float array_. (The line after turns it into a scalar.)\n\nAll of the other lines can be read similarly. A good trick, if you don't recognize the primitive^[For example, there's no call to `logistic` in the code, but a quick look at `jax.lax.logistic` shows that it's the same thing as `expit`.], is to [look it up](https://jax.readthedocs.io/en/latest/jax.lax.html) in the `jax.lax` sub-module.\n\nEven a cursory read of this suggests that we could probably save a couple of tedious operations by passing in an integer `y`, rather than a Boolean `y`, but hey. That really shouldn't cost much.\n\nWhile the `jaxpr` is lovely, it's a whole lot easier to reason about if you see it graphically. We can plot the _expression graph_ using^[This basically _just works_ as long as you've got `graphviz` installed on your system. And once you find the right regex to strip out the _terrible_ auto-generated title.] the `haiku`^[You need to install the dev version, or else it renders a lot of `pjit`s where the `sum` and `sub`s are supposed to be.] package from DeepMind.\n\n::: {#bf3d6463 .cell execution_count=11}\n``` {.python .cell-code}\nfrom haiku.experimental import to_dot\nimport graphviz\nimport re\nf = partial(log_posterior, X = X, y = y)\ndot = to_dot(f)(jnp.ones(p+1))\n#Strip out an obnoxious autogen title\ndot = re.sub(\"<<.*>>;\",\"\\\" \\\"\", dot, count = 1, flags=re.DOTALL)\ngraphviz.Source(dot)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n![](laplace_files/figure-html/cell-12-output-1.svg){}\n:::\n:::\n\n\nTo understand this graph, the orange-y boxes represent the input for `lp`. In this case it's an array of floating point digits with $p+1 = 5001$. The purple boxes are constants that are used in the function. Some of these are signed integers (s32), there's a matrix (f32[1000, 5000]), and there is even a literal (0.5). The blue box is the output. That leaves the yellow boxes, which have all of the operations, with inward arrows indicating the inputs and outward arrows indicating the outputs. \n\n### Splitting the expression graph into linear and non-linear subgraphs\n\nLooking at the graph, we can split it into three sub-graphs. The first sub-graph can be found by tracing an input value through the graph until it hits either a non-linear operation or the end of the graph. The sub-graph is created by making the penultimate node in that sequence an output node. This sub-graph represents a linear transformations.\n\n::: {#a5a99eed .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n![](laplace_files/figure-html/cell-13-output-1.svg){}\n:::\n:::\n\n\nOnce we have reached the end of the linear portion, we can link the output from this operation to the input of the non-linear sub-graph.\n\n::: {#56993076 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display execution_count=13}\n![](laplace_files/figure-html/cell-14-output-1.svg){}\n:::\n:::\n\n\nFinally, we have one more trace of $\\beta$ through the graph that is non-linear. We could couple this into the non-linear graph at the cost of having to reason about a bivariate Hessian (which will become complex).\n\n::: {#96b1b043 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display execution_count=14}\n![](laplace_files/figure-html/cell-15-output-1.svg){}\n:::\n:::\n\n\nThe two non-linear portions of the graph are merged through a trivial linear combination.\n\n::: {#b473bcd8 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display execution_count=15}\n![](laplace_files/figure-html/cell-16-output-1.svg){}\n:::\n:::\n\n\n### Step right up to play the game of the year: Is it linear?\n\nSo we need to trace through these jaxprs and keep a record of which of the sub-graphs they are in (and we do not know how many sub-graphs there will be!). We also need to note if an operation is linear or not. This is not something that is automatically provided. We need to store this information ourselves.\n\nThe only way I can think to do this is to make a set\nof all of the JAX operations that I know to be linear. Many of them are just index or type stuff. Unfortunately, there is a more complex class of operation, which are only _sometimes_ linear.\n\nThe first example we see of this is \n\n```\ng:f32[1000] = dot_general[\n      dimension_numbers=(((1,), (0,)), ((), ()))\n      precision=None\n      preferred_element_type=None\n    ] a f\n```\n\nThis line represents the general tensor dot product between `a` and `f`. In this case, `a` is constant input (the matrix $X$) while `f` is a linear transformation of the input (`beta[1:]`), so the resulting step is linear. However, there is a second `dot_general` in the code, which occurs at \n\n```\nu:f32[] = dot_general[\n      dimension_numbers=(((0,), (0,)), ((), ()))\n      precision=None\n      preferred_element_type=None\n    ] c c\n````\n\nIn this case, `c` is a linear transformation of the input (it's just `beta`), but `dot(c,c)` is a quadratic function. Hence in this case, `dot_general` is not linear.\n\nWe are going to need to work out how to handle this case. In the folded code is a partial^[If I wasn't sure, I deleted them from the linear list. There were also `scatter_mul`, `reduce_window`, and `reduce_window_shape_tuple`, which are all sometimes linear but frankly I didn't want to work out the logic.] list of the `jax.lax` primitives that are linear or occasionally linear. All in all there are 69 linear or no-op primitives and 7 sometimes linear primitives.\n\n::: {#de3a709f .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"jax.lax linear and sometimes linear primitives\"}\njax_linear = {\n  'add',\n  'bitcast_convert_type',\n  'broadcast',\n  'broadcast_in_dim',\n  'broadcast_shapes',\n  'broadcast_to_rank',\n  'clz',\n  'collapse',\n  'complex',\n  'concatenate',\n  'conj',\n  'convert_element_type',\n  'dtype',\n  'dtypes',\n  'dynamic_slice',\n  'expand_dims',\n  'full',\n  'full_like',\n  'imag',\n  'neg',\n  'pad',\n  'padtype_to_pads',\n  'real',\n  'reduce',\n  'reshape',\n  'rev',\n  'rng_bit_generator',\n  'rng_uniform',\n  'select',\n  'select_n',\n  'squeeze',\n  'sub',\n  'transpose',\n  'zeros_like_array',\n  'GatherDimensionNumbers',\n  'GatherScatterMode',\n  'ScatterDimensionNumbers',\n  'dynamic_index_in_dim',\n  'dynamic_slice',\n  'dynamic_slice_in_dim',\n  'dynamic_update_index_in_dim',\n  'dynamic_update_slice',\n  'dynamic_update_slice_in_dim',\n  'gather',\n  'index_in_dim',\n  'index_take',\n  'reduce_sum',\n  'scatter',\n  'scatter_add',\n  'slice',\n  'slice_in_dim',\n  'conv',\n  'conv_dimension_numbers',\n  'conv_general_dilated',\n  'conv_general_permutations',\n  'conv_general_shape_tuple',\n  'conv_shape_tuple',\n  'conv_transpose',\n  'conv_transpose_shape_tuple',\n  'conv_with_general_padding',\n  'cumsum',\n  'fft',\n  'all_gather',\n  'all_to_all',\n  'axis_index',\n  'ppermute',\n  'pshuffle',\n  'psum',\n  'psum_scatter',\n  'pswapaxes',\n  'xeinsum'\n}\n\njax_sometimes_linear = { \n  'batch_matmul',\n  'dot',\n  'dot_general',\n  'mul'\n }\njax_first_linear = {\n  'div'\n }\njax_last_linear = {\n  'custom_linear_solve',\n  'triangular_solve',\n  'tridiagonal_solve'\n }\n```\n:::\n\n\nAll of the _sometimes linear_ operations are linear as long as only one of their arguments depends on the function inputs. For both `div` and the various linear solves, the position of the input-dependent argument is restricted to one of the two positions. \n\n::: {.callout-note appearance=\"simple\"}\n  A more JAX-native way to deal with this is to think of how the `transpose` operation works. Essentially, it has the same dimension as the function argument, but evaluates to `None` when the operation isn't linear in that variable. But I had already done all of this before I got there and at some point truly you've gotta stop making your blog post more complicated.\n:::\n\n\n### Tracing through the jaxprs \n\nIn order to split our graph into appropriate sub-graphs we need to trace through the `jaxpr` and keep track of every variable and if it depends on linear or non-linear parts.\n\nFor simplicity, consider the following expression graph for computing `lambda x, y: 0.5*(x+y)`.\n\n![An expression graph for computing `lambda x, y: 0.5*(x+y)`. The blue rectangles are input variables, the rectangle square is a literal constants, and the green oval is the output node. (Yes I know the haiku colours are different. Sue me.)](execution_graph.png)\n\nThis figure corresponds roughly to the jaxpr\n\n::: {#2d3dd5e6 .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\n{ lambda ; a:f32[] b:f32[]. let c:f32[] = add a b; d:f32[] = mul 0.5 c in (d,) }\n```\n:::\n:::\n\n\nFor each node, the graph tells us\n\n- its unique identifier (internally^[The letters are `__repl__` magic] JAX uses integers)\n- which equation generated the value\n- which nodes are its parents in the graph (the input(s) to the equation)\n- whether or not this node depends on the inputs.  This is useful for ignoring non-linearities that just apply to the constants bound to the jaxpr.\n\nWe can record this information in a dataclass.\n\n::: {#ebcee419 .cell execution_count=18}\n``` {.python .cell-code}\nimport dataclasses as dc\n@dc.dataclass\nclass Node:\n  number: int = None\n  eqn: int = None\n  parents: List[int] = dc.field(default_factory=list)\n  depends_on_input: bool = True\n```\n:::\n\n\nNow we can build up our graph with all of the side information we need. The format of a `jaxpr` places the constant inputs in the first node, followed by the non-constant inputs (which I'm calling the input variables). For simplicity, I am assuming that there is only one input variable. \n\n::: {.callout-note appearance=\"simple\"}\nYou're going to look at this code and say _girl why are you using a dictionary, this is clearly a list_. And you would be correct except for one little thing: I can't guarantee that the `count` variables begin at `0`. They usually do. But one time they didn't. What is _probably_ true is that we could subtract off the first count from `constvars` or `invars` and we would have an ordinary list with the `count` variable corresponding to the input. But I'm not spelunking in the source code to ensure that `Literal` `Var`s can't be reused etc. And anyway, this is not a performance-critical data structure.\n\nI'm also relying heavily on dictionaries remembering key entry order, as the nodes are topographically sorted.\n:::\n\n::: {#39e76b59 .cell execution_count=19}\n``` {.python .cell-code}\nimport jax.core as jcore\nfrom jax import make_jaxpr\n\njpr = make_jaxpr(lp)(jnp.ones(p+1))\n\nnode_list = {\n  const.count: Node(\n    number=const.count, \n    depends_on_input=False\n  ) for const in jpr.jaxpr.constvars\n}\n\nnode_list |= {\n  inval.count: Node(number=inval.count) \n  for inval in jpr.jaxpr.invars\n}\n\n## For later, we need to know the node numbers that correspond\n## to the constants and inputs\n\nconsts_and_inputs = {node.number for node in node_list.values()}\n\nnode_list |= {\n  node.count: Node(\n    number=node.count,\n    eqn=j,\n    parents=[\n      invar.count for invar in eqn.invars if not isinstance(invar, jcore.Literal)\n    ],\n  )\n  for j, eqn in enumerate(jpr.jaxpr.eqns)\n  for node in eqn.outvars\n}\n\nfor node in node_list.values():\n  if len(node.parents) > 0:\n    node.depends_on_input =  any(\n      node_list[i].depends_on_input for i in node.parents\n    )\n\nnode_list\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n{0: Node(number=0, eqn=None, parents=[], depends_on_input=False),\n 1: Node(number=1, eqn=None, parents=[], depends_on_input=False),\n 2: Node(number=2, eqn=None, parents=[], depends_on_input=True),\n 3: Node(number=3, eqn=0, parents=[2], depends_on_input=True),\n 4: Node(number=4, eqn=1, parents=[3], depends_on_input=True),\n 5: Node(number=5, eqn=2, parents=[2], depends_on_input=True),\n 6: Node(number=6, eqn=3, parents=[0, 5], depends_on_input=True),\n 7: Node(number=7, eqn=4, parents=[4, 6], depends_on_input=True),\n 8: Node(number=8, eqn=5, parents=[7], depends_on_input=True),\n 9: Node(number=9, eqn=6, parents=[8], depends_on_input=True),\n 10: Node(number=10, eqn=7, parents=[1], depends_on_input=False),\n 11: Node(number=11, eqn=8, parents=[10, 9], depends_on_input=True),\n 12: Node(number=12, eqn=9, parents=[1], depends_on_input=False),\n 13: Node(number=13, eqn=10, parents=[12], depends_on_input=False),\n 14: Node(number=14, eqn=11, parents=[8], depends_on_input=True),\n 15: Node(number=15, eqn=12, parents=[14], depends_on_input=True),\n 16: Node(number=16, eqn=13, parents=[13], depends_on_input=False),\n 17: Node(number=17, eqn=14, parents=[16, 15], depends_on_input=True),\n 18: Node(number=18, eqn=15, parents=[11, 17], depends_on_input=True),\n 19: Node(number=19, eqn=16, parents=[18], depends_on_input=True),\n 20: Node(number=20, eqn=17, parents=[2, 2], depends_on_input=True),\n 21: Node(number=21, eqn=18, parents=[20], depends_on_input=True),\n 22: Node(number=22, eqn=19, parents=[19, 21], depends_on_input=True)}\n```\n:::\n:::\n\n\nNow let's identify which equations are linear and which aren't.\n\n::: {#9cfeb635 .cell execution_count=20}\n``` {.python .cell-code}\nlinear_eqn =[False] * len(jpr.jaxpr.eqns)\n\nfor node in node_list.values():\n  if node.eqn is None:\n    continue\n\n  prim = jpr.jaxpr.eqns[node.eqn].primitive.name\n  \n  if prim in jax_linear:\n    linear_eqn[node.eqn] = True\n  elif prim in jax_sometimes_linear:\n    # this is a check for being called once\n    linear_eqn[node.eqn] = (\n      sum(\n        node_list[i].depends_on_input for i in node.parents\n      ) == 1\n    )\n  elif prim in jax_first_linear:\n    linear_eqn[node.eqn] = (\n      node_list[node.parents[0]].depends_on_input \n      and not any(node_list[pa].depends_on_input for pa in node.parents[1:])\n    )\n  elif prim in jax_last_linear:\n    linear_eqn[node.eqn] = (\n      node_list[node.parents[-1]].depends_on_input \n      and not any(node_list[pa].depends_on_input for pa in node.parents[:-1])\n    )\n  elif all(not node_list[i].depends_on_input for i in node.parents):\n    linear_eqn[node.eqn] = True # Constants are linear\n\n```\n:::\n\n\nThe only messy thing^[Lord I hate a big 'if'/'elif' block. Just terrible. I should refactor but this is a weekend blog post not a work thing] in here is dealing with the sometimes linear primitives. If I was sure that every JAX primitive was guaranteed to have only two inputs, this could be simplified, but sadly I don't know that.\n\n### Partitioning the graph \n\nNow it's time for the fun: partitioning the problem into sub-graphs. To do this, we need to think about what rules we want to encode. \n\nThe _first rule_ is that every input for an equation or sub-graph needs to be either a constant, the function input, or the output of some other sub-graph that has already been computed. This means that if we find an equation with an input that doesn't satisfy these conditions, we need to split the sub-graph that it's in into two sub-graphs.\n\nThe _second rule_ is the only exception to the first rule. A sub-graph can have inputs from non-linear sub-graphs if an only if it contains a sequence of `sum` or `sub` terms and it finishes with the terminal node. This covers the common case where the function we are taking the Hessian of is a linear combination of independent functions. For instance, `log_posterior(beta) = log_likelihood(beta) + log_prior(beta)`. In this case we can compute the Hessians for the non-linear sub-expressions and then combine them.\n\nThe _third rule_ is that every independent use of the function input is the opportunity to start a new tree. (It may merge with a known tree.)\n\nAnd that's it. Should be simple enough to implement.\n\nI'm feeling like running this bad boy backwards, so let's do that. One of the assumption we have made is that the function we are tracing has a single output and that is always in the last node and defined in the last equation. So first off, lets get our terminal combination expressions.\n\n::: {#bf20c90a .cell execution_count=21}\n``` {.python .cell-code}\n## Find the terminal combination expressions\nterminal_expressions = {\"sum\", \"sub\"}\ncomb_eqns = []\nfor eqn in jpr.jaxpr.eqns[::-1]:\n  if any(\n    node_list[a.count].depends_on_input \n    for a in eqn.invars \n    if not isinstance(a, jcore.Literal)\n  )  and (\n    eqn.primitive.name in terminal_expressions\n  ):\n    comb_eqns.append(eqn)\n  else:\n    break\n\nprint(comb_eqns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[a:f32[] = sub b c]\n```\n:::\n:::\n\n\nNow for each of the terminal combination expressions, we will trace their parent back until we run out of tree. While we are doing this, we can also keep track of runs of linear operations. We also have to visit each equation once, so we need to keep track of our visited equations. This is, whether we like it or not, a depth-first search. It's always a bloody depth-first search, isn't it. \n\nSo what we are going to do is go through each of the combiner nodes and trace the graph down from it and note the path and it's parent. If we run into a portion of the graph we have already traced, we will note that for later. These paths will either be merged or, if the ancestral path from that point is all linear, will be used as a linear sub-graph.\n\n::: {#d1c69405 .cell execution_count=22}\n``` {.python .cell-code}\ndef dfs(visited, graph, subgraph, to_check, node):\n  if node in visited:\n    to_check.add(node)\n  else:\n    visited.add(node)\n    subgraph.add(graph[node].eqn)\n    for neighbour in graph[node].parents:\n      dfs(visited, graph, subgraph, to_check, neighbour)\n  \n\nvisited = consts_and_inputs\nto_check = set()\nsubgraphs = []\nfor ce in comb_eqns:\n  for v in (a for a in ce.invars if not isinstance(a, jcore.Literal)):\n    if v.count not in visited:\n      subgraphs.append(set())\n      dfs(visited, node_list, subgraphs[-1], to_check, v.count)\n\nto_check = to_check.difference(consts_and_inputs)\nprint(f\"Subgraphs: {subgraphs}\")\nprint(f\"Danger nodes: {to_check}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSubgraphs: [{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {17, 18}]\nDanger nodes: set()\n```\n:::\n:::\n\n\nThe `to_check` nodes are only dangerous insofar as we need to make sure that if they are in one of the linear sub-graphs they are terminal nodes of a sub-graph. To that end, let's make the linear sub-graphs.\n\n::: {#594c17e8 .cell execution_count=23}\n``` {.python .cell-code}\nlinear_subgraph = []\nnonlin_subgraph = []\nn_eqns = len(jpr.jaxpr.eqns)\nfor subgraph in subgraphs:\n  print(subgraph)\n  split = next(\n    (\n      i for i, lin in enumerate(linear_eqn) \n      if not lin and i in subgraph\n    )\n  )\n  if any(chk in subgraph for chk in to_check):\n    split = min(\n      split, \n      min(chk for chk in to_check if chk in subgraph)\n    )\n\n  linear_subgraph.append(list(subgraph.intersection(set(range(split)))))\n  nonlin_subgraph.append(list(subgraph.intersection(set(range(split, n_eqns)))))\n\nprint(f\"Linear subgraphs: {linear_subgraph}\")\nprint(f\"Nonlinear subgraphs: {nonlin_subgraph}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n{17, 18}\nLinear subgraphs: [[0, 1, 2, 3, 4], []]\nNonlinear subgraphs: [[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [17, 18]]\n```\n:::\n:::\n\n\nThe only interesting thing here is  making sure that if there is a linear node in the graph that was visited twice, it is the terminal node of the linear graph. The better thing would be to actually split the linear graph, but I'm getting a little bit sick of this post and I don't really want to deal with multiple linear sub-graphs. So I shan't. But hopefully it's relatively clear how you would do that.\n\nIn this case it's pretty clear that we are ok.\n\n::: {#c986a424 .cell execution_count=24}\n``` {.python .cell-code}\nany(linear_eqn[node_list[j].eqn] for j in to_check)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\nFalse\n```\n:::\n:::\n\n\n### Putting it together\n\nWell that's a nice script that does what I want. Now let's put it together in a function. I'm going to give it the _very_ unspecific name `transform_jaxpr` because sometimes you've gotta annoy your future self.\n\n::: {#c0f4a607 .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\ndef transform_jaxpr(\n  jaxpr: jcore.ClosedJaxpr\n) -> Tuple[List[Set[int]], List[Set[int]], List[jcore.JaxprEqn]]:\n  assert len(jpr.in_avals) == 1\n  assert len(jpr.out_avals) == 1\n\n  from jax import core as jcore\n\n  ## 1. Extract the tree and its relevant behavior\n  node_list = {\n    const.count: Node(\n      number=const.count, \n      depends_on_input=False\n    ) for const in jpr.jaxpr.constvars\n  }\n\n  node_list |= {\n    inval.count: Node(number=inval.count) \n    for inval in jpr.jaxpr.invars\n  }\n\n  ## For later, we need to know the node numbers that correspond\n  ## to the constants and inputs\n\n  consts_and_inputs = {node.number for node in node_list.values()}\n\n  node_list |= {\n    node.count: Node(\n      number=node.count,\n      eqn=j,\n      parents=[\n        invar.count for invar in eqn.invars if not isinstance(invar, jcore.Literal)\n      ],\n    )\n    for j, eqn in enumerate(jpr.jaxpr.eqns)\n    for node in eqn.outvars\n  }\n\n  for node in node_list.values():\n    if len(node.parents) > 0:\n      node.depends_on_input =  any(\n        node_list[i].depends_on_input for i in node.parents\n      )\n\n  ## 2. Identify which equations are linear_eqn\n\n  linear_eqn =[False] * len(jpr.jaxpr.eqns)\n\n  for node in node_list.values():\n    if node.eqn is None:\n      continue\n\n    prim = jpr.jaxpr.eqns[node.eqn].primitive.name\n    \n    if prim in jax_linear:\n      linear_eqn[node.eqn] = True\n    elif prim in jax_sometimes_linear:\n      # this is a check for being called once\n      linear_eqn[node.eqn] = (\n        sum(\n          node_list[i].depends_on_input for i in node.parents\n        ) == 1\n      )\n    elif prim in jax_first_linear:\n      linear_eqn[node.eqn] = (\n        node_list[node.parents[0]].depends_on_input \n        and not any(node_list[pa].depends_on_input for pa in node.parents[1:])\n      )\n    elif prim in jax_last_linear:\n      linear_eqn[node.eqn] = (\n        node_list[node.parents[-1]].depends_on_input \n        and not any(node_list[pa].depends_on_input for pa in node.parents[:-1])\n      )\n    elif all(not node_list[i].depends_on_input for i in node.parents):\n      linear_eqn[node.eqn] = True # Constants are linear\n\n  ##3. Find all the terminal expressions\n  ## Find the terminal combination expressions\n  terminal_expressions = {\"sum\", \"sub\"}\n  comb_eqns = []\n  for eqn in jpr.jaxpr.eqns[::-1]:\n    if any(\n      node_list[a.count].depends_on_input \n      for a in eqn.invars \n      if not isinstance(a, jcore.Literal)\n    )  and (\n      eqn.primitive.name in terminal_expressions\n    ):\n      comb_eqns.append(eqn)\n    else:\n      break\n  \n  ## 4. Identify the sub-graphs \n  def dfs(visited, graph, subgraph, to_check, node):\n    if node in visited:\n      to_check.add(node)\n    else:\n      visited.add(node)\n      subgraph.add(graph[node].eqn)\n      for neighbour in graph[node].parents:\n        dfs(visited, graph, subgraph, to_check, neighbour)\n    \n\n  visited = consts_and_inputs\n  to_check = set()\n  subgraphs = []\n  for ce in comb_eqns:\n    for v in (a for a in ce.invars if not isinstance(a, jcore.Literal)):\n      if v.count not in visited:\n        subgraphs.append(set())\n        dfs(visited, node_list, subgraphs[-1], to_check, v.count)\n\n  to_check = to_check.difference(consts_and_inputs)\n\n  ## 5. Find the linear sub-graphs\n  linear_subgraph = []\n  nonlin_subgraph = []\n  n_eqns = len(jaxpr.eqns)\n  for subgraph in subgraphs:\n    split = next(\n      (\n        i for i, lin in enumerate(linear_eqn) \n        if not lin and i in subgraph\n      )\n    )\n    if any(chk in subgraph for chk in to_check):\n      split = min(\n        split, \n        min(chk for chk in to_check if chk in subgraph)\n      )\n\n    linear_subgraph.append(list(subgraph.intersection(set(range(split)))))\n    nonlin_subgraph.append(list(subgraph.intersection(set(range(split, n_eqns)))))\n  \n  return (linear_subgraph, nonlin_subgraph, comb_eqns)\n```\n:::\n\n\nFor one final sense check, let's compare these outputs to the original jaxpr.\n\n::: {#22f15cd4 .cell execution_count=26}\n``` {.python .cell-code}\nfor j, lin in enumerate(linear_subgraph):\n  print(f\"Linear: {j}\")\n  for i in lin:\n    print(jpr.eqns[i])\n\nfor j, nlin in enumerate(nonlin_subgraph):\n  print(f\"Nonlinear: {j}\")\n  for i in nlin:\n    print(jpr.eqns[i])\n\nprint(\"Combination equations\")\nfor eqn in comb_eqns:\n  print(eqn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear: 0\na:f32[1] = dynamic_slice[slice_sizes=(1,)] b 0\na:f32[] = squeeze[dimensions=(0,)] b\na:f32[5000] = dynamic_slice[slice_sizes=(5000,)] b 1\na:f32[1000] = dot_general[dimension_numbers=(([1], [0]), ([], []))] b c\na:f32[1000] = add b c\nLinear: 1\nNonlinear: 0\na:f32[1000] = logistic b\na:f32[1000] = log b\na:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b\na:f32[1000] = mul b c\na:i32[1000] = convert_element_type[new_dtype=int32 weak_type=True] b\na:i32[1000] = sub 1 b\na:f32[1000] = neg b\na:f32[1000] = log1p b\na:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b\na:f32[1000] = mul b c\na:f32[1000] = add b c\na:f32[] = reduce_sum[axes=(0,)] b\nNonlinear: 1\na:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] b b\na:f32[] = mul 0.5 b\nCombination equations\na:f32[] = sub b c\n```\n:::\n:::\n\n\nComparing to the original jaxpr, we see it has the same information (the formatting is a bit unfortunate, as the original `__repr__` keeps track of the links between things, but what can you do?).\n\n::: {#c85d5f8b .cell execution_count=27}\n``` {.python .cell-code}\nprint(jpr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{ lambda a:f32[1000,5000] b:bool[1000]; c:f32[5001]. let\n    d:f32[1] = dynamic_slice[slice_sizes=(1,)] c 0\n    e:f32[] = squeeze[dimensions=(0,)] d\n    f:f32[5000] = dynamic_slice[slice_sizes=(5000,)] c 1\n    g:f32[1000] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a f\n    h:f32[1000] = add e g\n    i:f32[1000] = logistic h\n    j:f32[1000] = log i\n    k:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] b\n    l:f32[1000] = mul k j\n    m:i32[1000] = convert_element_type[new_dtype=int32 weak_type=True] b\n    n:i32[1000] = sub 1 m\n    o:f32[1000] = neg i\n    p:f32[1000] = log1p o\n    q:f32[1000] = convert_element_type[new_dtype=float32 weak_type=False] n\n    r:f32[1000] = mul q p\n    s:f32[1000] = add l r\n    t:f32[] = reduce_sum[axes=(0,)] s\n    u:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] c c\n    v:f32[] = mul 0.5 u\n    w:f32[] = sub t v\n  in (w,) }\n```\n:::\n:::\n\n\n### Making sub-functions\n\nNow that we have the graph partitioned, let's make our sub-functions. We do this by manipulating the `jaxpr`\nand then _closing_ over the literals.\n\nThere are a few ways we can do this. We could build completely new [`JaxprEqn`](https://github.com/google/jax/blob/c3f5af7d46b803da346aa7644cbeea3cb73b4c10/jax/_src/core.py#L297) objects from the existing Jaxpr. But honestly, that is just annoying, so instead I'm just going to modify the [basic, but incomplete, parser](https://jax.readthedocs.io/en/latest/notebooks/Writing_custom_interpreters_in_Jax.html)^[It is very little extra work to deal with eg JIT'd primitives and that sort of stuff, but for the purpose of this post, let's keep things as simple as possible.].\n\nThe only modification from the standard `eval_jaxpr` is that we are explicitly specifying the `invars` in order to overwrite the standard ones. This relies on the lexicographic ordering of the jaxpr expression graph.\n\n::: {#20360fac .cell execution_count=28}\n``` {.python .cell-code}\nfrom typing import Callable\n\nfrom jax import core as jcore\nfrom jax import lax\nfrom jax._src.util import safe_map\n\ndef eval_subjaxpr(\n  *args,\n  jaxpr: jcore.Jaxpr, \n  consts: List[jcore.Literal], \n  subgraph: List[int], \n  invars: List[jcore.Var]\n):\n\n\n  assert len(invars) == len(args)\n  \n  # Mapping from variable -> value\n  env = {}\n  \n  def read(var):\n    # Literals are values baked into the Jaxpr\n    if type(var) is jcore.Literal:\n      return var.val\n\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # We need to bind the input to the sub-function\n  # to the environment.\n  # We only need to write the consts that appear\n  # in our sub-graph, but that's more bookkeeping\n  safe_map(write, invars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Loop through equations and evaluate primitives using `bind`\n  outvars = []\n  for j in subgraph:\n    eqn = jaxpr.eqns[j]\n    # Read inputs to equation from environment\n    invals = safe_map(read, eqn.invars)  \n    # `bind` is how a primitive is called\n    outvals = eqn.primitive.bind(*invals, **eqn.params)\n    # Primitives may return multiple outputs or not\n    if not eqn.primitive.multiple_results: \n      outvals = [outvals]\n    outvars += [eqn.outvars]\n    safe_map(write, eqn.outvars, outvals) \n  \n  return safe_map(read, outvars[-1])[0]\n```\n:::\n\n\nThe final thing we should do is combine our transformation with this evaluation\nmodule to convert a function into a sequence of callable sub-functions. I am \nmaking _liberal_ uses of `lambda`s to close over variables that the user should\nnever see (like the sub-graph!). Jesus loves closures and so do I.\n\n<!-- There is some cost here. Because I am too lazy to work out the minimal set of \ninputs for each sub-expression, I'm going to just make sure that all of the computed\nvalues are available to every function. This is obviously inefficient, but sometimes\nyou just need to write blog-quality code.\n\nThe other thing that comes out a bit tricky^[And relies _very_ heavily on the topological ordering of the equations!] here is that each returned here \nhas a different number of arguments. The first linear function takes `jpr.invars` as \nits input. The second takes those _and_ the output of the first linear function.\nFor each subsequent function, this list becomes longer. This is partly unavoidable,\nbut with some more clever bookkeeping it wouldn't be too hard to produce minimal\ninput sets. But once again: blog code. \n\nBut if you're going to write code that does weird shit like this, the least you can\ndo is remember to catch it and throw a useful error down the line. -->\n\n::: {#827d3dec .cell execution_count=29}\n``` {.python .cell-code}\ndef decompose(fun: Callable, *args) -> Tuple[List[Callable], List[Callable], List[jcore.Var]]:\n  from functools import partial\n  from jax import make_jaxpr\n\n  jpr = make_jaxpr(fun)(*args)\n  linear_subgraph, nonlin_subgraph, comb_eqns = transform_jaxpr(jpr)\n\n  assert len(linear_subgraph) == len(nonlin_subgraph)\n  assert len(jpr.jaxpr.invars) == 1, \"Functions must only have one input\"\n\n  def get_invars(sub: List[int]) -> List[jcore.Var]:\n    # There is an implicit assumption everywhere in this post \n    # that each sub-function only has one non-constant input\n    \n    min_count = jpr.jaxpr.eqns[sub[0]].outvars[0].count\n    literal_ceil = jpr.jaxpr.invars[0].count\n    for j in sub:\n      for v in jpr.jaxpr.eqns[j].invars:\n        if (\n          not isinstance(v, jcore.Literal) and\n          v.count >= literal_ceil and \n          v.count < min_count\n        ):\n          return [v]\n    raise Exception(\"Somehow you can't find any invars\")\n    \n\n  lin_funs = []\n  nlin_funs = []\n  nlin_inputs = []\n  lin_outputs = []\n  nlin_outputs = []\n\n  for lin in linear_subgraph:\n    if len(lin) == 0:\n      lin_funs += [None]\n      lin_outputs += [jpr.jaxpr.invars[0].count]\n    elif jpr.jaxpr.eqns[lin[-1]].primitive.multiple_results:\n      raise Exception(f\"This code doesn't deal with multiple outputs from subgraph {lin}\")\n    else:\n      # find \n      lin_outputs += [jpr.jaxpr.eqns[lin[-1]].outvars[0].count]\n      lin_funs += [\n        partial(eval_subjaxpr,\n          jaxpr = jpr.jaxpr, \n          consts = jpr.literals, \n          subgraph = lin, \n          invars = get_invars(lin)\n        )\n      ]\n      \n  for nlin in nonlin_subgraph:\n    if len(nlin) == 0:\n      nlin_funs += [None]\n      nlin_inputs += [-1]\n      nlin_outputs += [None]\n    elif jpr.jaxpr.eqns[nlin[-1]].primitive.multiple_results:\n      raise Exception(f\"This code doesn't deal with multiple outputs from subgraph {nlin}\")\n    else:\n      invar = get_invars(nlin)\n      nlin_inputs += [lin_outputs.index(invar.count) if invar.count in lin_outputs else -1]\n      nlin_outputs += [jpr.jaxpr.eqns[nlin[-1]].outvars[0].count]\n      nlin_funs += [\n        partial(eval_subjaxpr,\n          jaxpr = jpr.jaxpr, \n          consts = jpr.literals, \n          subgraph = nlin, \n          invars = get_invars(nlin)\n        )\n      ]\n\n  combine = [0.0] * len(linear_subgraph)\n  # print(combine)\n  for eqn in comb_eqns:\n    combine[nlin_outputs.index(eqn.invars[0].count)] += 1.0\n    if eqn.primitive.name == \"sub\":\n      combine[nlin_outputs.index(eqn.invars[1].count)] += -1.0\n    else:\n      combine[nlin_outputs.index(eqn.invars[1].count)] += 1.0\n\n\n  return lin_funs, nlin_funs, nlin_inputs, combine\n\n```\n:::\n\n\n## Making the Hessian\n\nAfter _all_ of this work, we can finally make a function that builds a Hessian!\n\nWe remember that if $f(x) = g(h(x))$, where $h(x)$ is linear and $g(x)$ is nonlinear,\nthen the hessian of $f$ is \n\n$$\nH_f(x) = J_h^T H_g J_h,\n$$\nwhere $J_h$ is the Jacobian of $h$.\n\n::: {#d7f21525 .cell execution_count=30}\n``` {.python .cell-code}\ndef smarter_hessian(fun: Callable) -> Callable:\n  from jax import jacfwd\n  from jax import hessian\n  from jax import numpy as jnp\n  def hess(*args):\n    assert len(args) == 1, \"This only works for functions with one input\"\n    \n    lin_funs, nlin_funs, nlin_inputs, combine = decompose(fun, *args)\n    n_in = args[0].shape[0]\n    part = jnp.zeros((n_in, n_in))\n\n    for lin, nlin, nimp, comb in zip(lin_funs, nlin_funs, nlin_inputs, combine):\n      \n      if lin is not None:\n        lin_val = lin(*args)\n        jac = jacfwd(lin)(*args)\n      \n\n      h_args = (lin_val,) if lin is not None else args\n      hess = hessian(nlin)(*h_args) if nlin is not None else None\n\n      if lin is not None and nlin is not None:\n        part += comb * (jac.T @ (hess @ jac))\n      elif lin is not None:\n        part += comb * jac.T @ jac\n      elif nlin is not None:\n        part += comb * hess\n      \n    return part\n  return hess\n```\n:::\n\n\nAfter all of that, let's see if this works!\n\n::: {#431cf1b2 .cell execution_count=31}\n``` {.python .cell-code}\nmode_jax, H_jax = laplace(\n  partial(log_posterior, X = X, y = y),\n  x0 =jnp.zeros(X.shape[1] + 1)\n)\n\nH_smarter = -1.0 * smarter_hessian(partial(log_posterior, X = X, y = y))(mode_jax)\n\nprint(f\"The error is {jnp.linalg.norm(H_jax - H_smarter).tolist()}!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe error is 2.3684690404479625e-06!\n```\n:::\n:::\n\n\nIn single precision, that is good enough for government work.\n\n### But is it faster?\n\nNow let's take a look at whether we have actually saved any time.\n\n::: {#7907d5b4 .cell execution_count=32}\n``` {.python .cell-code}\nimport jax\ntimes_hess = timeit.repeat(lambda: jax.hessian(partial(log_posterior, X = X, y = y))(mode_jax), number = 5, repeat = 5)\nprint(f\"Full Hessian: The average time with p = {p} is {np.mean(times_hess): .3f}(+/-{np.std(times_hess): .3f})\")\n\ntimes_smarter = timeit.repeat(lambda: smarter_hessian(partial(log_posterior, X = X, y = y))(mode_jax), number = 5, repeat = 5)\nprint(f\"Smarter Hessian: The average time with p = {p} is {np.mean(times_smarter): .3f}(+/-{np.std(times_smarter): .3f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFull Hessian: The average time with p = 5000 is  3.444(+/- 0.201)\nSmarter Hessian: The average time with p = 5000 is  3.569(+/- 0.024)\n```\n:::\n:::\n\n\nWell that didn't make much of a difference. If anything, it's a little bit slower. This is likely due to compiler operations that can be improved if you just lower the whole thing.\n\n### But you forgot the diagonal trick\n\nThat said, the decomposition into linear and non-linear parts was _not_ the real source of the savings. If we assume the Hessian of the likelihood is diagonal, then we can indeed do a lot better!\n\nThe problem here is that while `smarter_hessian` worked for any^[With input/output restrictions] JAX-traceable function, we are now making a structural assumption. In theory, we could go through the JAX primitives and mark all of the ones that would (conditionally) lead to diagonal Hessians, but honestly I kinda want this bit of the post to be done. So I will leave that as an _exercise to the interested reader_.\n\n::: {#f8675103 .cell execution_count=33}\n``` {.python .cell-code}\ndef smart_hessian(fun: Callable) -> Callable:\n  from jax import jacfwd\n  from jax import hessian\n  from jax import numpy as jnp\n  def hess(*args):\n    assert len(args) == 1, \"This only works for functions with one input\"\n    \n    lin_funs, nlin_funs, nlin_inputs, combine = decompose(fun, *args)\n    n_in = args[0].shape[0]\n    part = jnp.zeros((n_in, n_in))\n\n    for lin, nlin, nimp, comb in zip(lin_funs, nlin_funs, nlin_inputs, combine):\n      \n      if lin is not None:\n        lin_val = lin(*args)\n        jac = jacfwd(lin)(*args)\n      \n\n      h_args = (lin_val,) if lin is not None else args\n      D = jvp(grad(nlin), h_args, (jnp.ones_like(h_args[0]),))[1] if nlin is not None else None\n\n      if lin is not None and nlin is not None:\n\n        part += comb * (jac.T @ (jac * D[:,None]))\n      elif lin is not None:\n        part += comb * jac.T @ jac\n      elif nlin is not None:\n        part += comb * jnp.diag(D)\n      \n    return part\n  return hess\n\n\n\nH_smart = -1.0 * smart_hessian(partial(log_posterior, X = X, y = y))(mode_jax)\n\nprint(f\"The error is {jnp.linalg.norm(H_jax - H_smart).tolist()}!\")\n\ntimes_smart = timeit.repeat(lambda: smart_hessian(partial(log_posterior, X = X, y = y))(mode_jax), number = 5, repeat = 5)\nprint(f\"Smart (diagonal-aware) Hessian: The average time with p = {p} is {np.mean(times_smart): .3f}(+/-{np.std(times_smart): .3f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe error is 2.3684690404479625e-06!\nSmart (diagonal-aware) Hessian: The average time with p = 5000 is  2.269(+/- 0.031)\n```\n:::\n:::\n\n\nThat is a proper saving!\n\n## Some concluding thoughts\n\nWell, this post got out of control. I swear when I sat down I was just going to write a quick post about Laplace approximations. Ooops.\n\n### The power of compiler optimizations\nI think what I've shown here is that one of the really powerful things about _compiled_ languages like JAX is that you can perform a pile of code optimizations that can greatly improve their performance. \n\n\nIn the ideal world, this type of optimization should be _invisible_ to the end user. Were I to do this seriously^[I am currently dressed like a sexy clown.], I would make sure that if the assumptions of the optimized code weren't met, the behaviour would revert back to the standard `jax.hessian`. \n\nRecognizing when to perform an optimization is, in reality, the whole art of this type of process. And it's very hard. For this post, I was able to do that to automatically recognize the linear operation, but I didn't try to find conditions that ensured the Hessian would be diagonal.\n\n### Sparsity detection and sparse autodiff\n\nWould you believe that people have spent a lot of time studying the efficiency gains when you have things like sparse Hessians? There is, in fact, a massive literature on _sparse autodiff_ and it is implemented in several autodiff libraries, including [in Julia](https://github.com/JuliaDiff/SparseDiffTools.jl).\n\nSparsity exploiting autodiff uses symbolic analysis of the expression tree for a function to identify when certain derivatives are going to be zero. For Hessians, it needs to identify when two variables have at most linear dependencies.\n\nOnce you have worked out the sparsity pattern, you need to do something with it. In the logistic case, it is diagonal, but in a lot of cases it will depend on more than one element of the latent representation. That is the Hessian will be sparse^[Most of the entries will be zero], but it won't be diagonal.\n\nI guess the question is _can we generalist the observation if the Hessian is diagonal we only need to compute a single Hessian-vector product_ to general sparsity structures.\n\nIn general, we won't be able to get away with a single product and will instead need a specially constructed set of $k$ _probing vectors_, where $k$ is a number to be determined (that is hopefully _much_ smaller than $n$). This set of vectors $s_k$ will have the special property that \n$$\n\\sum_{j=1}^k s_j = 1.\n$$\nThis means that the non-zero elements of each probing vector corresponds to a disjoint grouping of the variables.\n\nTo do this, we need to construct our set of probing vectors in a very special way. Each $s_k$ will be a vector containing zeros and ones. The set of indices with $[s_k]_j = 1$ have color $k$. The aim is to associate each index with a unique color in such a way that we can recover the algorithm. We can do this with a structurally symmetric orthogonal partition, which is detailed in [Section 4 of this great review article](http://www.ii.uib.no/~fredrikm/fredrik/papers/sirev2005.pdf).\n\nImplementing^[The previous article goes for ease of implementation over speed. A faster and better algorithm, and a _very_ detailed comparison of all of the available options can be found [here](http://www.ii.uib.no/~fredrikm/fredrik/papers/SISC2007.pdf). And I am not implementing that for a fucking blog.]  sparsity-aware autodiff Hessians does require some graph algorithms, and is frankly beyond the scope of my patience here. But it certainly is possible and you would get quite general performance from it.\n\nCritically, because it reduces the computation of a $p \\times p$ dense Hessian matrix with $k$ Hessian-vector products, it is extremely well suited to modern GPU acceleration techniques!\n\n### Could we do more?\n\nThere are so many many many ways to improve the very simple symbolic reduction of the \nautodiff beyond the simple \"identify $f(Ax)\" strategy. For more complex cases, it \nmight be necessary to relax the _only one input and only one output_ assumption.\n\n\nIt also might be possible to chain multiple instances of this, although this would \nrequire a more complex Hessian chain rule. Nevertheless, the extra complexity\nmight be balanced by savings form the applicable instances of sparse autodiff.\n\nBut probably the thing that _actually_ annoys me in all of this is that we are\nconstantly recomputing the Jacobian for the linear equation, which is fixed. A\nbetter implementation would consider implementing symbolic differentiation for \nlinear sub-graphs, which should lead to even more savings.\n\n\n\n### But is JAX the right framework for this?\n\nAll of this was a fair bit of work so I'm tempted to throw myself at the sunk-cost fallacy and just declare it to be good. But there is a problem. Because JAX doesn't do a symbolic transformation of the program (only a trace through paths associated with specific values), there is no guarantee that the sparsity pattern for $H$ remains the same at each step. And there is nothing wrong with that. It's an expressive, exciting language.\n\nBut all of the code transformation to make a sparsity-exploiting Hessian doesn't come for free. And the idea of having to do it again every time a Hessian is needed is ... troubling. If we could guarantee that the sparsity pattern was static, then we could factor all of this complex parsing and coloring code away and just run it once for each problem.\n\nTheoretically, we could do something like hashing on the jaxpr, but I'm not sure how much that would help.\n\nIdeally, we could do this in a library that performs _symbolic_ manipulations and can compile them into an expression graph. JAX is not quite^[And it's not trying to. Their bread and butter is autodiff and what they're doing is absolutely natural for that.] that language.\nAn option for this type of symbolic manipulation would be [Aesara](https://aesara.readthedocs.io/en/latest/). It may even be possible to do it in [Stan](https://github.com/stan-dev/stanc3), but even my wandering mind doesn't want to work out how to do this in OCaml.\n\n",
    "supporting": [
      "laplace_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}